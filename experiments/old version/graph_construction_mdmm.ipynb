{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7a5f53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if import is working\n",
    "import gnn_tracking\n",
    "# check that we have a GPU\n",
    "import torch\n",
    "# should show True\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b617625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jun 29 03:51:07 2023       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:41:00.0 Off |                    0 |\r\n",
      "| N/A   30C    P0    57W / 500W |      3MiB / 81920MiB |      0%      Default |\r\n",
      "|                               |                      |             Disabled |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f690d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from tcn_trainer_mdmm import TCNTrainer\n",
    "\n",
    "from gnn_tracking.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from gnn_tracking.utils.loading import get_loaders, TrackingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2f8b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = Path(\"/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1\")\n",
    "assert graph_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a733c1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = []\n",
    "for file in os.listdir(graph_dir):\n",
    "    d = os.path.join(graph_dir, file)\n",
    "    if os.path.isdir(d):\n",
    "        dirs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "502f8213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_6',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_2',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_8',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_4',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_5',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_1',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_7',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_3',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_0']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d5b3e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04:15:50] INFO: DataLoader will load 810 graphs (out of 900 available).\u001b[0m\n",
      "\u001b[36m[04:15:50] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_0/data21025_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_8/data21999_s0.pt\u001b[0m\n",
      "\u001b[32m[04:15:50] INFO: DataLoader will load 90 graphs (out of 900 available).\u001b[0m\n",
      "\u001b[36m[04:15:50] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9/data21009_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9/data21986_s0.pt\u001b[0m\n",
      "\u001b[36m[04:15:50] DEBUG: Parameters for data loader 'train': {'batch_size': 4, 'num_workers': 1, 'sampler': <torch.utils.data.sampler.RandomSampler object at 0x1529e6474310>, 'pin_memory': True, 'shuffle': None}\u001b[0m\n",
      "\u001b[36m[04:15:50] DEBUG: Parameters for data loader 'val': {'batch_size': 1, 'num_workers': 1, 'sampler': None, 'pin_memory': True, 'shuffle': False}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"train\": TrackingDataset(dirs, stop=810),\n",
    "    \"val\": TrackingDataset(dirs, start=810, stop=900),\n",
    "}\n",
    "loaders = get_loaders(datasets, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c9faed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = {}\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key, value in dictionary.items():\n",
    "            dict_of_lists.setdefault(key, []).append(value)\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "303de3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mdmm(epsilon,\n",
    "                     damping,\n",
    "                     main_weight,\n",
    "                     constraint_weight,\n",
    "                     loaders,\n",
    "                     main_loss=\"attractive\",\n",
    "                     constraint_loss=\"repulsive\",\n",
    "                     num_epochs=50\n",
    "                    ):\n",
    "    \n",
    "    main_loss_functions = {\n",
    "        \"embedding_loss\": (GraphConstructionHingeEmbeddingLoss(), {main_loss: main_weight}),\n",
    "    }\n",
    "    constraint_loss_functions = {\n",
    "        \"embedding_loss\": (GraphConstructionHingeEmbeddingLoss(), {constraint_loss: (constraint_weight, epsilon, damping)}),\n",
    "    }\n",
    "\n",
    "    model = GraphConstructionFCNN(\n",
    "        in_dim = 14,\n",
    "        hidden_dim = 64,\n",
    "        out_dim = 10,\n",
    "        depth = 4,\n",
    "        beta = 0.4\n",
    "    )\n",
    "\n",
    "    trainer = TCNTrainer(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        main_loss_functions=main_loss_functions,\n",
    "        constraint_loss_functions=constraint_loss_functions,\n",
    "        lr=0.005,\n",
    "    )\n",
    "\n",
    "    loss_history = trainer.train(epochs=num_epochs)\n",
    "    return list_of_dicts_to_dict_of_lists(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9edb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_history(constraints, path, main_loss=\"attractive\", constraint_loss=\"repulsive\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for constraint in constraints:\n",
    "        print(f'Training for = {constraint}')\n",
    "        loss_history = train_model_mdmm(epsilon=constraint[0],\n",
    "                                damping=constraint[1],\n",
    "                                main_weight=constraint[2],\n",
    "                                constraint_weight=constraint[3],\n",
    "                                loaders=loaders,\n",
    "                                main_loss=main_loss,\n",
    "                                constraint_loss=constraint_loss,\n",
    "                                num_epochs=10)\n",
    "        model_dict = {'loss_history':loss_history,\n",
    "                      'epsilon':constraint[0],\n",
    "                      'damping':constraint[1],\n",
    "                      'weight':constraint[2]}\n",
    "        \n",
    "        file_path = os.path.join(path, f'{constraint[0]}_{constraint[1]}_{constraint[2]}_{constraint[3]}.pkl')\n",
    "\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(model_dict,f)\n",
    "            f.close()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4b1065d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_1 = [(0.0009653154573041118, 10.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 10.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 10.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 10.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 10.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b42b906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dc3896/notebooks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9d097698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04:18:17 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for scaling coefficients = (0.0009653154573041118, 10.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:21 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03089, embedding_loss_attractive=   0.03089, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "\u001b[36m[04:18:23 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00429, embedding_loss_attractive=   0.00404, embedding_loss_repulsive=   0.00471\u001b[0m\n",
      "\u001b[36m[04:18:23 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00317, embedding_loss_attractive=   0.00287, embedding_loss_repulsive=   0.00372\u001b[0m\n",
      "\u001b[36m[04:18:23 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00288, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00481\u001b[0m\n",
      "\u001b[36m[04:18:23 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00235, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00310\u001b[0m\n",
      "\u001b[36m[04:18:24 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00232, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00294\u001b[0m\n",
      "\u001b[36m[04:18:24 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00255, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00316\u001b[0m\n",
      "\u001b[36m[04:18:24 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00232, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00215\u001b[0m\n",
      "\u001b[36m[04:18:24 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00234, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00264\u001b[0m\n",
      "\u001b[36m[04:18:24 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00232, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00215\u001b[0m\n",
      "\u001b[36m[04:18:25 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00238, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00207\u001b[0m\n",
      "\u001b[36m[04:18:25 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00248, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00188\u001b[0m\n",
      "\u001b[36m[04:18:25 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00229, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00153\u001b[0m\n",
      "\u001b[36m[04:18:25 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00235, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00188\u001b[0m\n",
      "\u001b[36m[04:18:25 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00241, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00169\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00236, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00182\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00219, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00132\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00208, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00199, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00197, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:18:26 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00195, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00132\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:28 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 1.1342850540022482                                │ nan       │\n",
      "│    │ _time_train                              │ 7.748658911994426                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017499742744904426                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0025220373335748617                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017499742744904426                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0025220373335748617                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0014146932258477642                             │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0024145008942222435                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0014146932258477642                             │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0024145008942222435                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001994919846765697                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002872975386582829                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:28 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00203, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00133\u001b[0m\n",
      "\u001b[36m[04:18:28 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00189, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:18:28 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00193, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00130\u001b[0m\n",
      "\u001b[36m[04:18:28 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00193, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "\u001b[36m[04:18:28 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00179, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00116\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00177, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00181, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00171, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00182, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00179, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00182, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:18:29 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00192, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00093\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00189, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00186, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00174, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00177, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00168, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00173, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:30 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00177, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:31 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00164, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:18:31 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00161, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:32 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9213934370054631                                │ nan       │\n",
      "│    │ _time_train                              │ 3.178480208996916                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001633900535913805                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001729054795692817                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001633900535913805                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001729054795692817                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009924185113050043                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0011091453173505189                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009924185113050043                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0011091453173505189                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0016515421668171055                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0018141169589600099                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:32 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00165, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:18:32 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00175, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:32 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00166, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:32 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00173, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:18:32 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00165, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00161, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00176, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00160, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00163, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00151, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00161, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:18:33 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00167, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00160, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00153, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00156, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00149, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00154, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:34 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00153, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:35 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00151, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:18:35 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00151, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:18:35 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00150, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:36 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.959822611999698                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.1495516869981657                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016016449856882293                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016026794950574122                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016016449856882293                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016026794950574122                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008902570620800058                             │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000961494002103567                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008902570620800058                             │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000961494002103567                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0015534770636198422                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.0016002336922996297                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:36 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00153, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:36 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00155, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:18:36 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00148, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:18:36 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00161, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00151, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00154, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00155, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00147, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00151, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00155, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:37 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00159, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00150, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00150, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00139, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00142, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00147, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00148, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:18:38 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00157, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:18:39 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00141, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:18:39 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00142, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:18:39 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00148, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:40 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.924559742998099                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.1048935240032733                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014781338977627456                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0015216961949192024                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014781338977627456                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0015216961949192024                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009385084319445822                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009438214379734373                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009385084319445822                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009438214379734373                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014616860684731768                             │   0.00011 │\n",
      "│    │ total_train                              │ 0.0015081523056256728                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:40 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00146, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:40 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00147, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:18:40 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00155, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:18:40 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00145, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00148, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00148, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00150, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00154, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00147, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00141, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:41 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00134, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00140, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00095\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00138, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00138, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00142, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00140, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:18:42 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00150, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:43 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00136, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:43 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00137, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:18:43 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00140, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:43 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00144, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:44 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9671875400017598                                │ nan       │\n",
      "│    │ _time_train                              │ 3.1208401729963953                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001389011555713498                              │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014474605608116745                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001389011555713498                              │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014474605608116745                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009427138261445281                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009470139515443976                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009427138261445281                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009470139515443976                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013757530651572678                             │   0.00011 │\n",
      "│    │ total_train                              │ 0.00143640587310504                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:44 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00136, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:44 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00144, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:18:44 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00146, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00152, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00144, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00138, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00132, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00139, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00141, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:45 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00137, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00134, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00143, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00134, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00136, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00141, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:46 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00132, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:47 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00144, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:18:47 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00134, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:18:47 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00139, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:18:47 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00139, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:47 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00146, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:48 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8326503009957378                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2432235200030846                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013856987986299726                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013963985595200669                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013856987986299726                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013963985595200669                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008943408675905731                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000955596083047997                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008943408675905731                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000955596083047997                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013450078200548886                             │   0.00011 │\n",
      "│    │ total_train                              │ 0.0013907589459920238                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:48 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00138, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:18:48 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00143, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:18:48 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00131, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00130, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00131, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00126, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00130, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00131, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:18:49 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00149, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00137, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00137, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00135, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00129, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00143, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:18:50 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00122, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:51 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00139, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:18:51 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00135, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:18:51 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00137, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:51 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00131, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:51 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00138, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:52 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9380534809970413                                │ nan       │\n",
      "│    │ _time_train                              │ 3.1956899389988394                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013352738434655798                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013456402141187945                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013352738434655798                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013456402141187945                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008862561009462095                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009567007253845704                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008862561009462095                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009567007253845704                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0012911599116503364                             │   0.00010 │\n",
      "│    │ total_train                              │ 0.0013407678721848821                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:52 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00124, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:18:52 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00133, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00133, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00137, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00127, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00141, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00132, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00130, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:18:53 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00132, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00133, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00135, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00128, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00096\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00125, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00131, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:54 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00132, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00127, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00138, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00135, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00129, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00135, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:55 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00133, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:18:56 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8514244979960495                                │ nan       │\n",
      "│    │ _time_train                              │ 3.312183107002056                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013359757424849603                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013116339756074945                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013359757424849603                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013116339756074945                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009532068370996664                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009587057186588829                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009532068370996664                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009587057186588829                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013295150271409916                             │   0.00010 │\n",
      "│    │ total_train                              │ 0.001308012339676469                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:18:56 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00133, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00127, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00140, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00129, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00132, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00131, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:18:57 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00132, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00128, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00125, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00128, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00125, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00130, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:18:58 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00123, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00126, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00127, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00126, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00123, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00142, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:18:59 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00126, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:19:00 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00126, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:19:00 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00125, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:01 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9168166620002012                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4115464049973525                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013517983244835502                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001279844111061008                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013517983244835502                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001279844111061008                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008622031717095524                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000961929859921538                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008622031717095524                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000961929859921538                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0012966682384204533                             │   0.00010 │\n",
      "│    │ total_train                              │ 0.0012780346074990304                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:01 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00126, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:19:01 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00131, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:19:01 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00136, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:19:01 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00129, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:19:01 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00127, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00121, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00127, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00134, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00127, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00118, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:19:02 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00132, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00122, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00123, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00125, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00126, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:19:03 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00132, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:19:04 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00132, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:19:04 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00128, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:19:04 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00130, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:19:04 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00126, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:05 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9009570769994752                                │ nan       │\n",
      "│    │ _time_train                              │ 3.327595980001206                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0012160419900384214                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0012661456772736434                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0012160419900384214                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0012661456772736434                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009644885282290893                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009629696141928434                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009644885282290893                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009629696141928434                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0012157157425665194                             │   0.00010 │\n",
      "│    │ total_train                              │ 0.0012648855301601986                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:19:05 TCNTrainer] INFO: Saving checkpoint to 230629_041905_model.pt\u001b[0m\n",
      "\u001b[32m[04:19:05 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for scaling coefficients = (0.0004771597001106582, 10.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:05 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.04550, embedding_loss_attractive=   0.04550, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:05 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00566, embedding_loss_attractive=   0.00531, embedding_loss_repulsive=   0.00546\u001b[0m\n",
      "\u001b[36m[04:19:05 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00359, embedding_loss_attractive=   0.00309, embedding_loss_repulsive=   0.00471\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00290, embedding_loss_attractive=   0.00247, embedding_loss_repulsive=   0.00317\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00267, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00287\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00272, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00345\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00267, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00279\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00270, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00242\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00296, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00252\u001b[0m\n",
      "\u001b[36m[04:19:06 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00278, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00195\u001b[0m\n",
      "\u001b[36m[04:19:07 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00260, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00205\u001b[0m\n",
      "\u001b[36m[04:19:07 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00269, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00196\u001b[0m\n",
      "\u001b[36m[04:19:07 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00290, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00190\u001b[0m\n",
      "\u001b[36m[04:19:07 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00281, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00157\u001b[0m\n",
      "\u001b[36m[04:19:07 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00261, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00155\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00257, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00145\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00254, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00144\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00269, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00145\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00282, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00137\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00263, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "\u001b[36m[04:19:08 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00261, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00128\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:09 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8788244200040936                                │ nan       │\n",
      "│    │ _time_train                              │ 3.445349832996726                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021060061304726536                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0028434552836089665                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021060061304726536                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0028434552836089665                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00116268521006633                               │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0023615616281961868                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00116268521006633                               │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0023615616281961868                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025369990022025175                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.003384718329650281                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:09 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00260, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00268, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00249, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00268, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00250, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00252, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:19:10 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00282, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00244, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00263, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00251, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00249, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00250, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00106\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00248, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:19:11 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00243, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00264, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00256, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00267, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00244, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00223, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:19:12 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00266, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:19:13 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00246, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:14 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9054837270014104                                │ nan       │\n",
      "│    │ _time_train                              │ 3.353963515997748                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021615183407751224                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021690265556540468                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021615183407751224                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021690265556540468                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0007846416077680058                             │   0.00012 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009842394450289458                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0007846416077680058                             │   0.00012 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009842394450289458                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024448190686396426                             │   0.00016 │\n",
      "│    │ total_train                              │ 0.002557440472794284                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00237, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00249, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00250, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00246, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00251, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00236, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:14 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00246, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:19:15 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00243, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:19:15 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00234, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:15 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00231, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:19:15 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00230, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:15 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00235, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00234, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00245, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00242, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00219, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00227, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00219, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:19:16 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00214, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:19:17 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00235, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:17 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00210, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:18 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9566650019987719                                │ nan       │\n",
      "│    │ _time_train                              │ 3.311453842004994                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002045359622894062                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002105950114656735                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002045359622894062                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002105950114656735                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000571918246957163                              │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000726691112576025                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000571918246957163                              │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000726691112576025                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002151041899600791                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0023588610335890884                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:18 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00219, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:19:18 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00228, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:19:18 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00233, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:19:18 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00223, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00218, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00209, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00211, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00224, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00222, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00208, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:19:19 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00212, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00233, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00229, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00226, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00208, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00211, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:19:20 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00196, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:19:21 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00199, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:19:21 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00188, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:21 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00201, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:19:21 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00189, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:22 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8435442089976277                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3988725829985924                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018855627406285042                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00196691489780753                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018855627406285042                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00196691489780753                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00047025743891329814                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005940774361329988                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00047025743891329814                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005940774361329988                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018771114021850128                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002103680874945899                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:22 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00190, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:22 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00210, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:19:22 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00182, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00198, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00189, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00189, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00184, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00189, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:23 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00182, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00191, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00173, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00175, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00049\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00178, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00180, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:19:24 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00192, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00188, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00178, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00181, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00187, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00177, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:25 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00176, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:26 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9491481470031431                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3249554349968093                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017368539766822425                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018186628929766015                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017368539766822425                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018186628929766015                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004659128048741776                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005201288257218585                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004659128048741776                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005201288257218585                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017224901873204442                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0018725330044464966                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:26 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00171, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00183, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00178, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00171, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00184, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00186, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00179, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:27 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00182, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00177, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00174, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00182, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00171, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00184, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:28 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00175, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00176, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00165, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00181, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00171, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00189, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:29 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00169, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:30 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00172, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:30 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8543012990048737                                │ nan       │\n",
      "│    │ _time_train                              │ 3.314729453995824                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001746898264779399                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017450035760738873                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001746898264779399                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017450035760738873                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004464393054756025                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000496724648561714                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004464393054756025                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000496724648561714                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001706806994560692                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0017702976589233344                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00181, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00176, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00161, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00171, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00178, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:31 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00164, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00167, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00169, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00165, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00182, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00175, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:32 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00162, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00162, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00162, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00167, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00175, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00153, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00173, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:33 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00167, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:34 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00164, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:34 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00175, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:35 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9292759529998875                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3438045299990335                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017004494995085729                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016820153624067093                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017004494995085729                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016820153624067093                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00047388106985535056                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00048097430325796817                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00047388106985535056                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00048097430325796817                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001696173493595173                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0016870155031311102                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:35 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00175, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:35 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00174, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:35 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00164, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:35 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00168, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:35 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00161, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00161, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00164, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00155, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00166, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00174, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:36 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00176, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00155, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00041\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00161, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00162, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00170, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00155, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:19:37 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00161, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:38 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00156, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:38 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00147, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:19:38 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00180, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:38 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00166, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:39 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9316894809962832                                │ nan       │\n",
      "│    │ _time_train                              │ 3.332015443003911                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001614538026559684                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016426901393822334                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001614538026559684                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016426901393822334                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004405292249025984                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00047326906185065                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004405292249025984                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047326906185065                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001566727945788039                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0016375990748506387                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:39 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00163, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:39 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00146, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:39 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00156, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00163, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00155, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00167, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00169, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00167, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:40 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00154, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00161, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00156, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00160, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00144, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00160, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:41 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00151, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00156, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00152, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00176, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00149, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00153, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:42 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00143, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:43 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8597903150002821                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4311565539974254                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014925859868526459                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00158498622018275                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014925859868526459                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00158498622018275                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00045827455728107854                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004682700537383703                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00045827455728107854                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004682700537383703                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014682333153258596                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.0015734477281763122                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:43 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00164, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00147, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00150, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00146, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00138, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00154, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:44 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00143, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00146, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00160, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00150, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00154, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00158, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:19:45 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00153, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00147, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00144, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00149, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00150, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00151, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00142, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:19:46 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00159, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:19:47 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00157, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:48 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9119193660008023                                │ nan       │\n",
      "│    │ _time_train                              │ 3.364813187996333                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0015449583918477098                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0015326599232632246                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0015449583918477098                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0015326599232632246                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004334580547745443                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004671651917610666                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004334580547745443                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004671651917610666                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014897022124690314                             │   0.00011 │\n",
      "│    │ total_train                              │ 0.001519893214056094                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:19:48 TCNTrainer] INFO: Saving checkpoint to 230629_041948_model.pt\u001b[0m\n",
      "\u001b[32m[04:19:48 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for scaling coefficients = (0.00028893887271952276, 10.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:48 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02385, embedding_loss_attractive=   0.02383, embedding_loss_repulsive=   0.00184\u001b[0m\n",
      "\u001b[36m[04:19:48 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00366, embedding_loss_attractive=   0.00331, embedding_loss_repulsive=   0.00499\u001b[0m\n",
      "\u001b[36m[04:19:48 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00260, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00430\u001b[0m\n",
      "\u001b[36m[04:19:48 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00242, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00405\u001b[0m\n",
      "\u001b[36m[04:19:48 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00243, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00361\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00246, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00305\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00243, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00258\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00246, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00231\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00258, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00211\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00265, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00212\u001b[0m\n",
      "\u001b[36m[04:19:49 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00273, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00197\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00270, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00199\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00270, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00178\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00259, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00150\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00256, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00151\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00275, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00159\u001b[0m\n",
      "\u001b[36m[04:19:50 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00296, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00171\u001b[0m\n",
      "\u001b[36m[04:19:51 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00286, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00141\u001b[0m\n",
      "\u001b[36m[04:19:51 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00269, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00137\u001b[0m\n",
      "\u001b[36m[04:19:51 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00270, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:19:51 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00267, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00128\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:52 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9241277920009452                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.4602469120000023                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.00213719909855475                                │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002351713833041269                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.00213719909855475                                │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002351713833041269                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001121633492746494                               │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0024805935134129425                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.001121633492746494                               │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0024805935134129425                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026607360613221925                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002970626195800936                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:52 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00278, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00122\u001b[0m\n",
      "\u001b[36m[04:19:52 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00266, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00126\u001b[0m\n",
      "\u001b[36m[04:19:52 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00268, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00265, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00265, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00278, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00279, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00278, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:19:53 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00274, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00268, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00280, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00275, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00090\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00260, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00282, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:19:54 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00259, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00260, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00259, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00269, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00277, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00267, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:19:55 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00257, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:19:56 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8743109360002563                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.398532166997029                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002163865944991509                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002185608469078253                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002163865944991509                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002185608469078253                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006785675246242641                              │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000979666104379656                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006785675246242641                              │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000979666104379656                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002543237479403615                               │   0.00015 │\n",
      "│    │ total_train                              │ 0.002731785467215653                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:19:56 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00254, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:19:56 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00276, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:19:57 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00255, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:19:57 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00257, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:19:57 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00250, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:19:57 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00253, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:57 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00261, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00252, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00260, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00262, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00241, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00237, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00254, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:19:58 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00231, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00242, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00237, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00232, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00253, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00231, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:19:59 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00222, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:20:00 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00229, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:01 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9465177730016876                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3321769969988964                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018607761361636222                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020534031871898934                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018607761361636222                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020534031871898934                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000525185203878209                               │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006705475460710416                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000525185203878209                               │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006705475460710416                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002150923586709218                               │   0.00013 │\n",
      "│    │ total_train                              │ 0.0024713694973587256                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00228, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00211, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00225, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00235, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00222, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:20:01 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00207, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00219, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00217, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00206, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00216, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00222, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:20:02 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00209, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00222, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00225, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00224, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00218, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00203, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00210, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:03 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00213, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:20:04 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00208, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:04 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00208, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:05 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8411528950018692                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.316194278995681                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018929549997361997                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018664650398815795                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018929549997361997                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018664650398815795                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00044077144282507816                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005088807380439375                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00044077144282507816                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005088807380439375                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021070322280542716                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0021559265608500156                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:05 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00225, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:20:05 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00212, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:20:05 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00202, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:05 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00202, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:20:05 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00212, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00195, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00216, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00204, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00189, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00197, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:06 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00196, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00192, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00037\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00192, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00218, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00202, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00207, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:07 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00205, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:08 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00215, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:20:08 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00206, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:08 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00189, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:08 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00200, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:09 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9354416019996279                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.420037431002129                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017806933416674535                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018341649573736737                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017806933416674535                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018341649573736737                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00039499634327108245                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00044205587837854265                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00039499634327108245                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00044205587837854265                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0019463046065842113                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0020612937540476547                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:09 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00196, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:09 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00201, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:09 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00195, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00210, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00188, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00191, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00208, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00200, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:10 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00190, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00202, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00195, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00204, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00210, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00184, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:11 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00207, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00201, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00202, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00196, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00204, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:12 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00188, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:13 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 1.0906379529988044                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.2759236830024747                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018210897066940864                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018057893609554677                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018210897066940864                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018057893609554677                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00033419114172122337                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00039784984864991876                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00033419114172122337                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00039784984864991876                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018974204379547802                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.00198249631559045                                │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00199, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00197, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00200, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00186, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00195, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00204, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:14 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00194, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00186, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00189, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00195, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00204, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00181, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:15 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00187, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00205, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00183, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00184, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00195, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00189, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:16 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00207, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:17 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00198, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:17 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00176, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:18 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8411920979997376                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.370398622006178                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019254775213388105                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017824340588757264                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019254775213388105                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017824340588757264                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003064273111198822                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00036552993381703316                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003064273111198822                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036552993381703316                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0019567794852062234                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0019154630692015068                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:18 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00198, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:18 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00212, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:18 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00202, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:18 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00190, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:18 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00188, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00188, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00176, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00174, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00219, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00204, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:19 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00178, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00181, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00034\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00191, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00201, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00178, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00192, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00182, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:20 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00186, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:21 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00196, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:21 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00213, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:21 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00203, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:22 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9232351599930553                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.351824832003331                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018438427786653241                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017983838674965604                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018438427786653241                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017983838674965604                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003006907648846714                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003479793188942073                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003006907648846714                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003479793188942073                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018659237241889868                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.001906591167058704                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:22 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00187, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:22 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00179, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:22 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00178, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:20:22 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00187, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00194, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00189, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00183, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00181, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00179, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:23 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00190, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00181, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00182, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00184, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00172, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00175, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:24 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00181, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:25 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00175, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:20:25 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00195, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:25 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00180, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:25 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00173, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:25 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00179, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:26 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.878551868001523                                  │ nan       │\n",
      "│    │ _time_train                              │ 3.2714854609948816                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017783253078555894                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017696752034017606                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017783253078555894                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017696752034017606                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002820339771763732                              │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00032885758199940066                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002820339771763732                              │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032885758199940066                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017648925151055059                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0018460192457254371                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:26 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00190, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:26 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00176, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:26 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00179, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00180, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00186, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00197, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00188, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00172, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:27 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00177, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00184, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00185, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00188, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00189, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00180, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:20:28 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00176, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00191, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00180, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00177, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00190, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00175, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:29 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00178, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:30 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9165903780012741                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3861534300012863                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016955764796067444                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017597852710616118                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016955764796067444                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017597852710616118                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000291922258070877                               │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003177755143266318                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000291922258070877                               │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003177755143266318                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017015655654379063                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0018167437542817128                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:20:30 TCNTrainer] INFO: Saving checkpoint to 230629_042030_model.pt\u001b[0m\n",
      "\u001b[32m[04:20:30 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for scaling coefficients = (0.00016839923191582784, 10.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:30 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02306, embedding_loss_attractive=   0.02304, embedding_loss_repulsive=   0.00186\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00421, embedding_loss_attractive=   0.00384, embedding_loss_repulsive=   0.00520\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00311, embedding_loss_attractive=   0.00268, embedding_loss_repulsive=   0.00382\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00251, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00442\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00244, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00286\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00270, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00273\u001b[0m\n",
      "\u001b[36m[04:20:31 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00265, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00242\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00284, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00225\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00267, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00206\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00283, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00227\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00271, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00164\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00288, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00161\u001b[0m\n",
      "\u001b[36m[04:20:32 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00275, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00143\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00289, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00152\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00294, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00279, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00149\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00291, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00279, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00277, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:20:33 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00297, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00153\u001b[0m\n",
      "\u001b[36m[04:20:34 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00298, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:35 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9339139229996363                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.33687879700301                                   │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002250799451333781                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0024336973919757993                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002250799451333781                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0024336973919757993                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0010801926319901315                              │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002318506001201206                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0010801926319901315                              │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002318506001201206                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002860007085837424                               │   0.00016 │\n",
      "│    │ total_train                              │ 0.0030843575193725634                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:35 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00297, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:20:35 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00303, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:20:35 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00302, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:20:35 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00310, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:20:35 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00307, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00295, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00300, embedding_loss_attractive=   0.00241, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00300, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00291, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00296, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00309, embedding_loss_attractive=   0.00246, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:20:36 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00290, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00084\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:20:37 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00300, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:20:37 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00293, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:20:37 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00289, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:20:37 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00300, embedding_loss_attractive=   0.00243, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:20:37 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00282, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:20:38 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00284, embedding_loss_attractive=   0.00237, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:20:38 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00287, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:20:38 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00316, embedding_loss_attractive=   0.00256, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:20:38 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00302, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:39 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8606621840008302                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3959270820050733                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002341720071207318                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002302708194861785                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002341720071207318                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002302708194861785                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0007165013683132                                 │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009410481474458657                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0007165013683132                                 │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009410481474458657                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0029276581274138556                              │   0.00016 │\n",
      "│    │ total_train                              │ 0.002965273437654517                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:39 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00301, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:20:39 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00271, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:20:39 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00284, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:20:39 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00300, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00296, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00292, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00303, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00280, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00303, embedding_loss_attractive=   0.00245, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:20:40 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00301, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00302, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00303, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00295, embedding_loss_attractive=   0.00256, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00294, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00305, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:20:41 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00307, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:20:42 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00294, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:20:42 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00320, embedding_loss_attractive=   0.00267, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:20:42 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00293, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:20:42 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00312, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:20:42 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00285, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:43 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9253985329996794                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.4030427649995545                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022325608594756987                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002343225345621115                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022325608594756987                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002343225345621115                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0005612148919479094                              │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006646116868033087                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005612148919479094                              │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006646116868033087                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002785647903672523                               │   0.00015 │\n",
      "│    │ total_train                              │ 0.0029543881348146153                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:43 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00289, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:20:43 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00290, embedding_loss_attractive=   0.00242, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00284, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00289, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00291, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00292, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00297, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:20:44 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00274, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00276, embedding_loss_attractive=   0.00245, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00286, embedding_loss_attractive=   0.00248, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00281, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00270, embedding_loss_attractive=   0.00236, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00276, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:20:45 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00265, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00255, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00273, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00263, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00247, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00267, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00260, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:46 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00260, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:47 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9257878069984145                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.186053271005221                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002164388598046369                               │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022490184944526756                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002164388598046369                               │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022490184944526756                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004166711681237858                              │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005024300883592937                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004166711681237858                              │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005024300883592937                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025834808384792673                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.002764467736063846                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:47 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00263, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00268, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00256, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00250, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00273, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00249, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:48 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00252, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00244, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00264, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00269, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00263, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00256, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00040\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00241, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:49 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00241, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00258, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00252, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00239, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00243, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00245, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:50 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00246, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:20:51 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00255, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:52 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8753396819956833                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3425732660034555                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002032744237739179                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020958523844079725                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002032744237739179                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020958523844079725                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00036033871470459014                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004021416999184036                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00036033871470459014                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004021416999184036                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024013275746256114                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.002516678166277523                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00246, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00227, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00253, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00243, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00225, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:52 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00245, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00263, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00229, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00227, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00226, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00272, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:53 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00239, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00232, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00231, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00235, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00222, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00269, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:20:54 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00221, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:20:55 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00225, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:55 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00237, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:20:55 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00220, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:20:56 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9366274630010594                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.348538717000338                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002026398351881653                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020241513590241226                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002026398351881653                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020241513590241226                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002891983486026422                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003452337754998136                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002891983486026422                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003452337754998136                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0022829829728127356                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.002381230220473899                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:20:56 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00239, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:56 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00226, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:56 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00237, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:56 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00235, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00233, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00229, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00213, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00220, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00227, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:57 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00216, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00221, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00232, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00232, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00216, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00225, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:20:58 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00224, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:20:59 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00222, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:59 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00231, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:20:59 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00243, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:20:59 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00212, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:20:59 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00209, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:00 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8894112489942927                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.385031504003564                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018316000505971411                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001958745868367331                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018316000505971411                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001958745868367331                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003221321453262741                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00030443826653040357                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003221321453262741                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00030443826653040357                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021860372132828667                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0022597101211933226                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:00 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00217, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:00 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00216, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00223, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00233, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00203, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00211, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00215, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:01 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00231, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00230, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00211, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00214, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00212, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00024\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00223, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00230, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:02 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00219, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00211, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00233, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00204, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00230, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00207, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:03 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00216, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:04 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8869250429997919                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.409203023002192                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.00191390571417287                                │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019196773348192644                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.00191390571417287                                │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019196773348192644                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00024089438326579208                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002790061116684228                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00024089438326579208                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002790061116684228                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020931322062905464                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0021836278401544604                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00216, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00202, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00216, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00208, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00216, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:05 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00227, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00208, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00215, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00204, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00200, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00215, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:06 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00244, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00224, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00197, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00209, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00217, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00215, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00197, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:07 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00206, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:08 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00203, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:08 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00203, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:09 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9309996480005793                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3835554929974023                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001874885669288536                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019132218276623069                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001874885669288536                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019132218276623069                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00022187789695130453                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000260818548520036                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00022187789695130453                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000260818548520036                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002015534347285413                               │   0.00013 │\n",
      "│    │ total_train                              │ 0.002148791468069141                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:09 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00224, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:09 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00195, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:09 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00208, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:09 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00203, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:09 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00202, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00197, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00214, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00204, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00214, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00195, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:10 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00217, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00212, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00206, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00228, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00201, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00195, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:11 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00208, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:12 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00240, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:12 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00216, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:12 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00209, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:12 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00198, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:13 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8530597790013417                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3619941430006293                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020967403400896324                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018897207833487046                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020967403400896324                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018897207833487046                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00021561778315420572                             │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00024380666291825198                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00021561778315420572                             │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00024380666291825198                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0022277718151195182                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.002093376028076218                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:21:13 TCNTrainer] INFO: Saving checkpoint to 230629_042113_model.pt\u001b[0m\n",
      "\u001b[32m[04:21:13 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for scaling coefficients = (0.00010062895363475553, 10.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:13 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02799, embedding_loss_attractive=   0.02799, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:21:13 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00448, embedding_loss_attractive=   0.00406, embedding_loss_repulsive=   0.00562\u001b[0m\n",
      "\u001b[36m[04:21:13 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00311, embedding_loss_attractive=   0.00262, embedding_loss_repulsive=   0.00419\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00289, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00386\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00273, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00368\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00276, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00300\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00282, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00257\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00275, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00235\u001b[0m\n",
      "\u001b[36m[04:21:14 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00214\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00272, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00218\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00301, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00236\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00309, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00187\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00309, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00204\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00310, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00161\u001b[0m\n",
      "\u001b[36m[04:21:15 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00309, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00159\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00301, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00151\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00329, embedding_loss_attractive=   0.00244, embedding_loss_repulsive=   0.00154\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00302, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00313, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00125\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00288, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:21:16 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00314, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00133\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:17 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9434542669987422                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.4734939120025956                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002333092943040861                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0026075539329396607                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002333092943040861                               │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0026075539329396607                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001076689371580465                               │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00231199697195667                                │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.001076689371580465                               │   0.00017 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00231199697195667                                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0030131101970457368                              │   0.00017 │\n",
      "│    │ total_train                              │ 0.00331650190445333                                │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:17 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00304, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00291, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00308, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00290, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00305, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00315, embedding_loss_attractive=   0.00248, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:21:18 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00296, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00281, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00281, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00277, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00273, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00278, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00092\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:21:19 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00289, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00272, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00275, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00276, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00277, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00286, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00287, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:21:20 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00267, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:21:21 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00280, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:22 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9201133440001286                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.2597075950034196                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021165299863140615                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021640894386293413                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021165299863140615                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021640894386293413                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006980722795964943                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0008602760536619118                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006980722795964943                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0008602760536619118                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0027650104395838247                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002832603484869297                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00292, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00254, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00269, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00269, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00268, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:21:22 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00269, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00260, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00256, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00269, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00265, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00265, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:21:23 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00269, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00255, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00259, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00258, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00266, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00256, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:21:24 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00270, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:21:25 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00257, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:21:25 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00272, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:21:25 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00255, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:26 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8816855100012617                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3575964069968904                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002034042531158775                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020461441975389723                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002034042531158775                               │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020461441975389723                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000491622247823721                               │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005817834171466529                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000491622247823721                               │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005817834171466529                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025850479749755725                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0026429969223280407                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:26 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00269, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:21:26 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00275, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:21:26 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00242, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:21:26 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00260, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00255, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00270, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00260, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00260, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00255, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:21:27 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00254, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00277, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00270, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00250, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00255, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00260, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00271, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:28 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00264, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:29 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00278, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:21:29 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00261, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:21:29 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00262, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:21:29 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00272, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:30 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9152314940001816                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3099496610011556                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020527098702991173                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020755122024611813                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020527098702991173                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020755122024611813                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004158671414997015                              │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004621908589658497                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004158671414997015                              │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004621908589658497                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025909693989281854                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0026371086754932486                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:30 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00278, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:21:30 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00269, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:21:30 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00267, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00259, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00270, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00270, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00263, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00273, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:21:31 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00262, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00269, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00256, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00261, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00029\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00269, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00269, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00270, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:32 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00259, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:21:33 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00270, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:33 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00269, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:21:33 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00269, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:21:33 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00274, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:21:33 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00268, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:34 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8899213599943323                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3067284999997355                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020570067792303032                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002122813386136088                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020570067792303032                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002122813386136088                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00039587148672176733                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003926571351911854                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00039587148672176733                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003926571351911854                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026453714533191588                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002661767764374834                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:34 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00268, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:21:34 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00282, embedding_loss_attractive=   0.00237, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:35 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00272, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:35 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00262, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:35 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00262, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:35 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00275, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:21:35 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00281, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00283, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00275, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00254, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00276, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00267, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00264, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:36 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00266, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00266, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00269, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00271, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00271, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00263, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:37 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00266, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:38 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00245, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:39 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.885627158000716                                  │ nan       │\n",
      "│    │ _time_train                              │ 3.4268823680031346                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021478167502209542                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021668117530093405                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021478167502209542                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021668117530093405                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00031008373658146916                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003440809458221213                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031008373658146916                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003440809458221213                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026227951541336046                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002684384664713309                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00265, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00265, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00256, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00268, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00271, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:21:39 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00273, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00269, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00277, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00287, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00276, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00257, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:40 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00262, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00265, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00276, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00282, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00270, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00268, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00262, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:41 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00269, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:42 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00257, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:42 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00272, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:43 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9211258920040564                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.2768035709959804                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020891734023785425                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002210839962348195                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020891734023785425                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002210839962348195                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00031105253405662046                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00030763787964388744                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031105253405662046                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00030763787964388744                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026226936917131145                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0027071714006694668                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:43 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00259, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:43 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00285, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:21:43 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00277, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:43 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00274, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00269, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00270, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00261, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00271, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00266, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00270, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:44 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00273, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00270, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00031\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00278, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00273, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00268, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00277, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:45 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00267, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:21:46 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00257, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:46 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00263, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:46 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00279, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:46 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00274, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:47 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8109930490027182                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3703944419976324                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021785193329883945                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002231961256122501                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021785193329883945                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002231961256122501                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00026511166522848526                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002806300740101801                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00026511166522848526                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002806300740101801                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026387407030496334                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0027113442384899397                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:47 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00271, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:47 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00282, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:47 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00276, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00265, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00277, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00286, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00279, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00266, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00262, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:48 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00259, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00267, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00269, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00263, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00295, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00274, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:49 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00267, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:50 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00258, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:50 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00265, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:50 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00264, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:50 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00272, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:50 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00281, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:51 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9371185380005045                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.335420157003682                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022651257924735544                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002277708471307687                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022651257924735544                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002277708471307687                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002327288416886909                              │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002604281547561912                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002327288416886909                              │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002604281547561912                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026694738745896354                              │   0.00016 │\n",
      "│    │ total_train                              │ 0.002745325017601178                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:21:51 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00271, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:51 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00261, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00265, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00281, embedding_loss_attractive=   0.00244, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00267, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00263, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00284, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:52 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00290, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00250, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00019\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00260, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00276, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00258, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00019\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00265, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:53 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00263, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00276, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00262, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00277, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00274, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00269, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00258, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:21:54 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00265, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:21:55 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬────────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                              │       Std │\n",
      "├────┼──────────────────────────────────────────┼────────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9024615610032924                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.323739856998145                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002410599051250352                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022712438963720656                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002410599051250352                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022712438963720656                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0001953915874764789                              │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00023967418243324044                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0001953915874764789                              │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00023967418243324044                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 10.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00272486324991203                                │   0.00015 │\n",
      "│    │ total_train                              │ 0.0027142590969305466                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴────────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:21:55 TCNTrainer] INFO: Saving checkpoint to 230629_042155_model.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_loss_history(constraints_1, \"loss_histories/damping_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8576bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_2 = [(0.0009653154573041118, 5.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 5.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 5.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 5.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 5.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2659f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_3 = [(0.0009653154573041118, 1.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 1.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 1.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 1.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 1.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf9e612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04:26:23 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for = (0.0009653154573041118, 5.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:23 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03381, embedding_loss_attractive=   0.03381, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:23 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00458, embedding_loss_attractive=   0.00425, embedding_loss_repulsive=   0.00656\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00302, embedding_loss_attractive=   0.00256, embedding_loss_repulsive=   0.00527\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00263, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00438\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00229, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00359\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00238, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00345\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00233, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00272\u001b[0m\n",
      "\u001b[36m[04:26:24 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00237, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00239\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00235, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00219\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00240, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00245\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00240, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00212\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00238, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00214\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00235, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00165\u001b[0m\n",
      "\u001b[36m[04:26:25 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00242, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00186\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00237, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00174\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00239, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00154\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00248, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00170\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00239, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00160\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00243, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00177\u001b[0m\n",
      "\u001b[36m[04:26:26 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00228, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00135\u001b[0m\n",
      "\u001b[36m[04:26:27 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00224, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00141\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:28 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9106973769958131                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4629745870042825                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019329569289564259                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002541217607735046                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019329569289564259                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002541217607735046                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0014756912714801729                            │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002521951948384309                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0014756912714801729                            │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002521951948384309                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002208992594387382                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.002909303018869144                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00226, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00162\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00221, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00152\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00193, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00216, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00136\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00201, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00207, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00142\u001b[0m\n",
      "\u001b[36m[04:26:28 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00199, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:26:29 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00188, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:26:29 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00185, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:29 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00189, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:29 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00191, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:26:29 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00188, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00104\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00191, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00180, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00182, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00175, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00173, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00166, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:30 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00162, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:26:31 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00163, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:31 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00155, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:32 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9029143109946745                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3380353140018997                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016391869479169447                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001826306225231899                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016391869479169447                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001826306225231899                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009281773922137089                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0011123985455261333                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009281773922137089                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0011123985455261333                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00161550834050609                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.001912499896681918                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:32 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00163, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:32 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00161, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:32 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00171, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:26:32 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00170, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00166, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00116\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00157, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00164, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00173, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00164, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00152, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:26:33 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00158, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00152, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00155, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00150, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00159, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00156, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:26:34 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00155, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:35 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00160, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:35 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00151, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:26:35 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00152, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:35 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00155, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:36 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8522377629997209                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4059555569983786                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014845498979816007                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001599603943902944                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014845498979816007                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001599603943902944                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009372354924885763                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009549224096593525                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009372354924885763                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009549224096593525                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014668843411426576                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.001593005274573864                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:36 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00152, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:36 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00144, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:36 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00140, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00140, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00149, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00143, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00150, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00139, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:37 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00144, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00140, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00145, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00155, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00146, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00136, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:26:38 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00168, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00174, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00154, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00140, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00145, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00141, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:39 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00147, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:40 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9410958220032626                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3399558329983847                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014773182014727758                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014853276205684972                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014773182014727758                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014853276205684972                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000876621506176889                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009392875672795172                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000876621506176889                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009392875672795172                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014240503233547011                            │   0.00011 │\n",
      "│    │ total_train                              │ 0.0014692916732116285                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:40 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00149, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00132, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00147, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00158, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00137, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00147, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00141, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:41 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00138, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00128, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00137, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00158, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00128, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00073\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00144, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:26:42 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00124, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00145, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00157, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00122, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00127, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00148, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:43 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00149, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:44 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00134, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:44 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8626918329973705                               │ nan       │\n",
      "│    │ _time_train                              │ 3.34557697600394                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013405583061588306                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014085726120871642                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013405583061588306                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014085726120871642                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009172955547304203                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009398486707795393                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009172955547304203                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009398486707795393                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013134062212581436                            │   0.00011 │\n",
      "│    │ total_train                              │ 0.0013936730015683425                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00130, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00137, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00122, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00121, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00126, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:26:45 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00134, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00122, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00141, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00135, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00137, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00122, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:26:46 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00128, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00134, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00135, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00137, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00124, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00132, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:47 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00121, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:48 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00125, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:48 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:26:48 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00126, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:49 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9098268669986282                               │ nan       │\n",
      "│    │ _time_train                              │ 3.44792009700177                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0012875535305485958                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013099404607023219                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0012875535305485958                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013099404607023219                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008730200287472042                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009446362632193736                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008730200287472042                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009446362632193736                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0012383663250754276                            │   0.00010 │\n",
      "│    │ total_train                              │ 0.0012985297720741681                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:49 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00137, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:26:49 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00132, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:49 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00129, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:26:49 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00136, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00122, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00125, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00124, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00116, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:50 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00131, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00122, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00118, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00125, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00117, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00120, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:51 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00130, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:52 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00119, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:52 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00132, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:52 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00122, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:52 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00129, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:26:52 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00116, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:53 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9103072929938207                               │ nan       │\n",
      "│    │ _time_train                              │ 3.304409668002336                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0012152675720345644                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0012666438201561644                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0012152675720345644                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0012666438201561644                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009005434953400658                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009567520066205724                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009005434953400658                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009567520066205724                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001181786720149426                             │   0.00009 │\n",
      "│    │ total_train                              │ 0.001262116688647734                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:53 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00126, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:26:53 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:53 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00123, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00130, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00124, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00119, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00123, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00126, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:26:54 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00122, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00117, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00117, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00119, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00101\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00117, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:26:55 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00119, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00120, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00119, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00122, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00122, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:56 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00112, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:26:57 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8852389519961434                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3080114160038647                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0011684142984449864                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0012021726880479477                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0011684142984449864                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0012021726880479477                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008744757409052303                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009524431630161596                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008744757409052303                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009524431630161596                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0011237574430803458                            │   0.00009 │\n",
      "│    │ total_train                              │ 0.0011956487917639499                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:26:57 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00124, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00114, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00119, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00118, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00118, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00129, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:26:58 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00114, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00121, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00124, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00114, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00106, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:26:59 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00110, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00116, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00123, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00111, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00109, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00114, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:27:00 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00108, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:27:01 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:02 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9199241250025807                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3968266140000196                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0010893167740303196                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0011476148430285562                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0010893167740303196                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0011476148430285562                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009566989694980698                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009575589041551346                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009566989694980698                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009575589041551346                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001085276073232914                             │   0.00009 │\n",
      "│    │ total_train                              │ 0.0011438490086570015                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00113, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00116, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00102, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00111, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:27:02 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00102, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00114, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00095, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00110, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:27:03 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00123, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00115, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00111, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00119, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:27:04 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:27:05 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00102, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:27:05 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00105, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:27:05 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00114, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:06 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8945933369977865                               │ nan       │\n",
      "│    │ _time_train                              │ 3.31867221200082                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0010829006871467249                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0011250817938363684                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0010829006871467249                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0011250817938363684                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008994837905952913                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009646073865172054                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008994837905952913                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009646073865172054                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0010518497160066746                            │   0.00009 │\n",
      "│    │ total_train                              │ 0.0011247109210008545                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:27:06 TCNTrainer] INFO: Saving checkpoint to 230629_042706_model.pt\u001b[0m\n",
      "\u001b[32m[04:27:06 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.0004771597001106582, 5.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:06 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03202, embedding_loss_attractive=   0.03201, embedding_loss_repulsive=   0.00184\u001b[0m\n",
      "\u001b[36m[04:27:06 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00607, embedding_loss_attractive=   0.00555, embedding_loss_repulsive=   0.00797\u001b[0m\n",
      "\u001b[36m[04:27:06 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00350, embedding_loss_attractive=   0.00306, embedding_loss_repulsive=   0.00446\u001b[0m\n",
      "\u001b[36m[04:27:06 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00276, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00398\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00269, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00345\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00250, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00285\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00273, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00237\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00259, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00253\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00253, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00221\u001b[0m\n",
      "\u001b[36m[04:27:07 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00245, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00161\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00253, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00255, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00188\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00250, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00193\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00252, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00155\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00247, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00161\u001b[0m\n",
      "\u001b[36m[04:27:08 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00243, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00146\u001b[0m\n",
      "\u001b[36m[04:27:09 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00263, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00144\u001b[0m\n",
      "\u001b[36m[04:27:09 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00253, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00138\u001b[0m\n",
      "\u001b[36m[04:27:09 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00246, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:27:09 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00241, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00122\u001b[0m\n",
      "\u001b[36m[04:27:09 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00264, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00140\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:10 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.86540915700607                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.5082355759950588                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001988305353249113                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0026436967235012668                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001988305353249113                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0026436967235012668                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0012482492498949998                            │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002506507173515489                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0012482492498949998                            │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002506507173515489                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024361608782783152                            │   0.00014 │\n",
      "│    │ total_train                              │ 0.003164054084972942                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:10 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00253, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00143\u001b[0m\n",
      "\u001b[36m[04:27:10 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00254, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00138\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00257, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00241, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00257, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00270, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00272, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:27:11 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00248, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00251, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00262, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00247, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00236, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00097\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00238, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:27:12 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00258, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00243, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00222, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00235, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00246, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00237, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00222, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:27:13 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00238, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:14 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9450408060001791                               │ nan       │\n",
      "│    │ _time_train                              │ 3.346196616002999                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002069817549393823                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020920288430187357                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002069817549393823                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020920288430187357                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0007756850320018                               │   0.00012 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0010195649561605283                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0007756850320018                               │   0.00012 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0010195649561605283                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002327327995509323                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0024784476395347727                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00236, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00241, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00246, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00242, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00225, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00243, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:27:15 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00229, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00234, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00256, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00230, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00244, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00220, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:27:16 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00224, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00236, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00232, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00208, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00214, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00224, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:27:17 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00235, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:27:18 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00231, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:27:18 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00211, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:19 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.875601995998295                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3540796720044455                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019305619411170484                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00203490679913884                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019305619411170484                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00203490679913884                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006367581676588291                            │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0007419211163731484                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006367581676588291                            │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0007419211163731484                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020978298051179283                            │   0.00014 │\n",
      "│    │ total_train                              │ 0.0022866859770577235                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:19 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00215, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:27:19 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00209, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:27:19 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00199, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:27:19 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00207, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:27:19 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00216, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00233, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00225, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00214, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00212, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00206, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:27:20 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00199, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00199, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00207, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00213, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00178, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00195, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:21 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00212, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:27:22 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00198, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:27:22 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00219, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:22 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00210, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:22 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00202, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:23 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9581324690007023                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3919903860005434                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019471367251955802                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019387021543815952                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019471367251955802                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019387021543815952                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000511328913206752                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006167107646518704                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000511328913206752                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006167107646518704                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001987126401056432                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002093150329161344                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:23 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00207, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:27:23 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00204, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:27:23 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00197, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00196, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00203, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00214, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00190, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00207, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:24 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00179, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00182, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00197, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00186, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00050\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00181, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00186, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:25 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00201, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00197, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00182, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00198, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00199, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:26 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00185, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:27 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.919758581003407                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3209250009967946                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001795047590146876                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018689812962745724                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001795047590146876                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018689812962745724                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004912622819473553                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005479253108241582                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004912622819473553                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005479253108241582                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018126104466824069                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.001954340706854655                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:27 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00198, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00194, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00179, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00197, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00196, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00187, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:28 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00198, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00192, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00186, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00190, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00191, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00199, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:29 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00187, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00175, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00180, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00181, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00183, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00178, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00174, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:27:30 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00188, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:27:31 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00185, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:32 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8276214849975077                               │ nan       │\n",
      "│    │ _time_train                              │ 3.415969847002998                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017860568400161962                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001824075402003791                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017860568400161962                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001824075402003791                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004761833607012199                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000509204060238387                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004761833607012199                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000509204060238387                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001784818222384072                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0018645821109395748                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00175, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00183, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00185, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00179, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00181, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:27:32 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00199, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00178, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00178, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00181, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00184, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00188, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:33 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00201, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00173, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00178, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00170, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00167, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00191, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:34 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00176, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:35 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00172, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:35 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00187, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:27:35 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00185, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:36 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9008919560001232                               │ nan       │\n",
      "│    │ _time_train                              │ 3.389742369996384                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017993514775298536                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017937408258973304                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017993514775298536                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017937408258973304                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004344972028371154                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004935436350181056                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004344972028371154                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004935436350181056                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017435827214891712                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.0018149710872802389                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:36 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00190, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:36 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00169, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:27:36 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00163, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:27:36 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00179, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00170, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00170, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00165, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00191, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00173, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:37 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00175, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00188, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00175, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00049\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00177, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00167, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00186, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:38 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00169, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:39 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00182, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:39 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00173, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:39 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00177, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:39 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00164, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:39 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00174, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:40 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9387383600042085                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4460454959989875                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001806973938881937                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017465813489162864                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001806973938881937                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017465813489162864                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004187049965063731                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00047623365085223333                           │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004187049965063731                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047623365085223333                           │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001730530608134965                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.0017453694252728535                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:40 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00175, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:40 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00166, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00166, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00164, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00165, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00161, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00169, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:41 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00170, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00176, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00159, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00160, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00168, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00158, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:42 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00164, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:43 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00168, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:43 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00162, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:43 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00164, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:43 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00159, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:43 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00165, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:44 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00170, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:44 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00170, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:45 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8617955669978983                               │ nan       │\n",
      "│    │ _time_train                              │ 3.593796701003157                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001621639010651658                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016905708058411485                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001621639010651658                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016905708058411485                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00046114278148808                              │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004692543624886518                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00046114278148808                              │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004692543624886518                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00160093626473099                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0016802883657668097                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:45 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00174, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:45 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00160, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:45 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00163, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:45 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00178, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:27:45 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00161, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00170, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00163, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00175, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00172, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00166, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:46 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00153, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:27:47 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00165, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:27:47 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00167, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:27:47 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00159, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:47 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00164, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:27:47 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00155, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:27:48 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00165, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:27:48 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00179, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:27:48 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00170, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:27:48 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00162, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:27:48 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00154, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:49 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.947015598998405                                │ nan       │\n",
      "│    │ _time_train                              │ 3.567649665004865                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0015878929860062068                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016590021642411284                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0015878929860062068                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016590021642411284                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004362340990660919                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00047078235373084474                           │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004362340990660919                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047078235373084474                           │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0015354856965132057                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.001650794505559165                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:27:49 TCNTrainer] INFO: Saving checkpoint to 230629_042749_model.pt\u001b[0m\n",
      "\u001b[32m[04:27:49 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00028893887271952276, 5.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:49 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02987, embedding_loss_attractive=   0.02986, embedding_loss_repulsive=   0.00191\u001b[0m\n",
      "\u001b[36m[04:27:49 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00424, embedding_loss_attractive=   0.00376, embedding_loss_repulsive=   0.00737\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00311, embedding_loss_attractive=   0.00259, embedding_loss_repulsive=   0.00490\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00279, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00468\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00262, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00380\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00274, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00367\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00261, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00286\u001b[0m\n",
      "\u001b[36m[04:27:50 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00266, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00256\u001b[0m\n",
      "\u001b[36m[04:27:51 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00270, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00230\u001b[0m\n",
      "\u001b[36m[04:27:51 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00290, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00195\u001b[0m\n",
      "\u001b[36m[04:27:51 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00278, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00185\u001b[0m\n",
      "\u001b[36m[04:27:51 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00283, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00186\u001b[0m\n",
      "\u001b[36m[04:27:51 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00279, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00169\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00273, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00132\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00271, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00146\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00287, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00139\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00267, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00122\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00259, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00129\u001b[0m\n",
      "\u001b[36m[04:27:52 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00268, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00121\u001b[0m\n",
      "\u001b[36m[04:27:53 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00274, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00122\u001b[0m\n",
      "\u001b[36m[04:27:53 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00275, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00124\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:54 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8005840410041856                                │ nan       │\n",
      "│    │ _time_train                              │ 3.571006400998158                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002281510390134321                              │   0.00012 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0025197928123350433                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002281510390134321                              │   0.00012 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0025197928123350433                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0010318107210979279                             │   0.00016 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0024117702151433074                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0010318107210979279                             │   0.00016 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0024117702151433074                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002753684866345591                              │   0.00016 │\n",
      "│    │ total_train                              │ 0.0031223608332277813                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00270, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00282, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00278, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00290, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00305, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00139\u001b[0m\n",
      "\u001b[36m[04:27:54 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00281, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00128\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00274, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00115\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00276, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00270, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00283, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00261, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:27:55 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00283, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00095\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00274, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00273, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00270, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00258, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00293, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00274, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:27:56 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00265, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:27:57 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00281, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:27:57 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00285, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:27:58 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8379488060018048                                │ nan       │\n",
      "│    │ _time_train                              │ 3.173146564993658                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0023751379896162286                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022442797554445824                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0023751379896162286                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022442797554445824                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0007012883071891136                             │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009674516115383489                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0007012883071891136                             │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009674516115383489                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002781780571159389                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.00278733944144155                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:27:58 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00294, embedding_loss_attractive=   0.00242, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:27:58 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00277, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:27:58 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00287, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:27:58 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00267, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:27:58 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00289, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00280, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00260, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00271, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00258, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00276, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00280, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:27:59 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00280, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00264, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00271, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00288, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00263, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00260, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00287, embedding_loss_attractive=   0.00250, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:28:00 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00260, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:28:01 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00264, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:28:01 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00266, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:02 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9078630139993038                                │ nan       │\n",
      "│    │ _time_train                              │ 3.1542923220040393                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0023184936312544676                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022495043386012463                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0023184936312544676                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022495043386012463                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0005279091571638774                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006932709824862738                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005279091571638774                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006932709824862738                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026205140114244486                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0027018487992526716                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:02 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00276, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:28:02 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00276, embedding_loss_attractive=   0.00236, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:28:02 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00266, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:28:02 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00264, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:28:02 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00249, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00272, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00271, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00265, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00253, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00238, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00240, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:28:03 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00249, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00246, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00253, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00252, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00263, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00242, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00251, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:28:04 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00244, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:05 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00280, embedding_loss_attractive=   0.00263, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:28:05 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00258, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:06 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 1.0814496759994654                                │ nan       │\n",
      "│    │ _time_train                              │ 3.1733596159974695                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0023302503628656267                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002205195555146875                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0023302503628656267                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002205195555146875                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00041003141345249283                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005534654137680738                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00041003141345249283                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005534654137680738                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002510343109154039                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.002567996602422865                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:06 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00260, embedding_loss_attractive=   0.00237, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:06 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00259, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:28:06 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00244, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:28:06 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00248, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00250, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00242, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00248, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00237, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00247, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00244, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:28:07 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00252, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00243, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00043\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00283, embedding_loss_attractive=   0.00243, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00249, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00247, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00241, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:08 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00238, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:28:09 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00241, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:28:09 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00242, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:28:09 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00225, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:09 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00239, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:10 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.7941760060057277                                │ nan       │\n",
      "│    │ _time_train                              │ 3.07635027699871                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020676188339065348                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021524006991734354                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020676188339065348                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021524006991734354                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00044448895011252413                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00047193814182007145                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00044448895011252413                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047193814182007145                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002327803279169732                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0024409851981464573                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:10 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00245, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:28:10 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00239, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:10 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00227, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:10 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00224, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00230, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00250, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00235, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00260, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00260, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00227, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:11 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00257, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00238, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00225, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00229, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00226, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00217, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:12 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00233, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:28:13 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00214, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:13 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00220, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:13 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00220, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:13 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00218, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:14 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8799859800055856                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2715083499933826                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020882524782791733                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020994293225901615                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020882524782791733                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020994293225901615                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00034586635463508883                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004101502749233924                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00034586635463508883                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004101502749233924                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021916438709013164                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0023105783924790733                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:14 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00217, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:14 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00230, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:14 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00235, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:14 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00205, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00211, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00207, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00220, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00228, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00206, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:15 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00212, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00224, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00217, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00213, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00217, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00210, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:16 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00210, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:17 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00213, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:17 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00201, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:17 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:17 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00214, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:17 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00205, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:18 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9078791540014208                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3373199210036546                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019294225727207958                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020198558934326535                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019294225727207958                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020198558934326535                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003362752922435498                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00036410554131525085                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003362752922435498                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036410554131525085                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020202699039752283                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002160105523951535                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:18 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00200, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:18 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00217, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00203, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00223, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00214, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00200, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00186, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00220, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:19 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00238, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00201, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00212, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00190, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00029\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00200, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00210, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:20 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00224, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00198, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00211, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00197, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00194, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:21 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00207, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:22 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8594895749993157                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3274013990012463                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001982631472249826                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001965137034553686                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001982631472249826                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001965137034553686                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002744030825043511                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003372863411910263                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002744030825043511                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003372863411910263                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0019536534544183975                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.002059666338959351                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:22 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00189, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00193, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00199, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00214, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00206, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00196, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:23 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00196, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00207, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00203, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00199, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00198, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00201, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:24 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00189, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00178, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00178, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00182, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00196, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00176, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:25 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00196, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:28:26 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00192, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:26 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00198, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:27 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9313518780036247                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3518777639983455                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017452689702622593                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019136957785233929                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017452689702622593                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019136957785233929                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002943570631840784                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003188264762966669                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002943570631840784                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003188264762966669                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001756366059028854                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.001974057201456664                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00193, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00215, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00197, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00187, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00170, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:28:27 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00227, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00210, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00203, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00185, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00195, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00185, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:28 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00186, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00214, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00207, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00201, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00179, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00198, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:29 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00215, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:28:30 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00207, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:30 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00193, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:30 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00189, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:31 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9290647169982549                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2694178510064376                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018143941311993532                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018827309751690462                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018143941311993532                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018827309751690462                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002828528905714241                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003083238600101781                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002828528905714241                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003083238600101781                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001801709256445368                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0019227708259762582                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:28:31 TCNTrainer] INFO: Saving checkpoint to 230629_042831_model.pt\u001b[0m\n",
      "\u001b[32m[04:28:31 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00016839923191582784, 5.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:31 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03441, embedding_loss_attractive=   0.03440, embedding_loss_repulsive=   0.00135\u001b[0m\n",
      "\u001b[36m[04:28:31 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00439, embedding_loss_attractive=   0.00409, embedding_loss_repulsive=   0.00502\u001b[0m\n",
      "\u001b[36m[04:28:31 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00314, embedding_loss_attractive=   0.00255, embedding_loss_repulsive=   0.00542\u001b[0m\n",
      "\u001b[36m[04:28:31 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00260, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00337\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00258, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00366\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00250, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00271\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00261, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00247\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00273, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00278\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00231\u001b[0m\n",
      "\u001b[36m[04:28:32 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00268, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00210\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00269, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00174\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00276, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00208\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00263, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00165\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00295, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00179\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00279, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00159\u001b[0m\n",
      "\u001b[36m[04:28:33 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00285, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00142\u001b[0m\n",
      "\u001b[36m[04:28:34 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00297, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00151\u001b[0m\n",
      "\u001b[36m[04:28:34 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00287, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:28:34 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00301, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00159\u001b[0m\n",
      "\u001b[36m[04:28:34 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00288, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00121\u001b[0m\n",
      "\u001b[36m[04:28:34 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00280, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:35 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.804840630000399                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.5027530649967957                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020839063657654656                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0025421929180970775                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020839063657654656                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0025421929180970775                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00116464794264175                               │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002413508528728827                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00116464794264175                               │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002413508528728827                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0027308850523291364                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.003190611231053624                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:35 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00285, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00137\u001b[0m\n",
      "\u001b[36m[04:28:35 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00280, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00288, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00277, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00317, embedding_loss_attractive=   0.00242, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00317, embedding_loss_attractive=   0.00237, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00283, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:28:36 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00285, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00272, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00273, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00267, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00275, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00094\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00281, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:28:37 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00268, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00281, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00273, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00286, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00300, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00303, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00278, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:28:38 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00283, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:39 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.92208555599791                                  │ nan       │\n",
      "│    │ _time_train                              │ 3.38065164800355                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022205727950980266                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00219802115699781                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022205727950980266                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00219802115699781                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000683894892831126                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009341191158061067                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000683894892831126                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009341191158061067                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0027495451297404037                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0028306866085312785                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00291, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00287, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00283, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00274, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00256, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00308, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:28:40 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00296, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00271, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00286, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00269, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00286, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:28:41 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00265, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00281, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00288, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00279, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00263, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00266, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:28:42 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00252, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:28:43 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00274, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:28:43 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00271, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:44 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8743827910002437                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2833459690009477                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002007238502200279                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021787230865892224                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002007238502200279                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021787230865892224                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006032660619692049                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006577971844711419                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006032660619692049                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006577971844711419                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002591622638930049                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0027551076723620516                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:44 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00266, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:28:44 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00266, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:44 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00286, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:28:44 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00265, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:28:44 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00264, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00281, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00282, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00249, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00249, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00249, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00264, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:28:45 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00257, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00251, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00264, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00255, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00240, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00245, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:46 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00248, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:47 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00246, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:28:47 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00254, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:47 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00238, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:48 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8881726670006174                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3819998190010665                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019463035433242718                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020997869544731353                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019463035433242718                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020997869544731353                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00043307855861106267                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004982908469684459                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00043307855861106267                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004982908469684459                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002371639830784665                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0025848545939225723                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:48 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00242, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:28:48 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00244, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:48 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00283, embedding_loss_attractive=   0.00234, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:28:48 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00251, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00251, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00247, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00238, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00249, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00235, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:49 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00242, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00268, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00246, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00044\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00223, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00230, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00241, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:50 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00226, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:51 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00247, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:51 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00243, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:51 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00237, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:51 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00239, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:28:51 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00226, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:52 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9165173130022595                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3236513119991287                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019138092718397578                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020122685999577417                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019138092718397578                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020122685999577417                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00037694301281590017                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00040800354456106946                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00037694301281590017                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00040800354456106946                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0022962218564417628                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0024236071914471137                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:52 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00237, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:28:52 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00226, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00241, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00230, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00238, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00222, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00238, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00237, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:53 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00234, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00226, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00263, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00248, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00226, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00220, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:54 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00234, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00246, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00217, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00227, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00236, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00232, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:28:55 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00227, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:28:56 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8551689830055693                                │ nan       │\n",
      "│    │ _time_train                              │ 3.340474163996987                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019161853421893384                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001986729716331574                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019161853421893384                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001986729716331574                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00031817919387119924                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003559091062459342                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031817919387119924                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003559091062459342                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00222185084130615                               │   0.00014 │\n",
      "│    │ total_train                              │ 0.002349409944575115                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:28:56 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00215, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00244, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00227, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00246, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00224, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00242, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:57 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00236, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00231, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00225, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00238, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00237, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00219, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:58 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00220, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00214, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00235, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00218, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00234, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00231, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00217, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:28:59 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00222, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:00 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00213, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:01 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9131196090020239                                │ nan       │\n",
      "│    │ _time_train                              │ 3.35661300799984                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018784733903076914                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019572867938487106                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018784733903076914                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019572867938487106                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00029153373325243594                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00031797395241469733                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00029153373325243594                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00031797395241469733                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021531078899796638                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0022762936340378864                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00211, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00223, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00219, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00240, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00264, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:01 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00238, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00224, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00219, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00233, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00216, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00228, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:02 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00219, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00027\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00229, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00233, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00220, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00212, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00208, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:03 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00205, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:04 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00228, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:04 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00219, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:04 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00216, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:05 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.911222133996489                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.265912635004497                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019200494952706826                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00196418867405192                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019200494952706826                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00196418867405192                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00025455235704107                               │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002918394122317461                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00025455235704107                               │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002918394122317461                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002127513259701017                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.002250147712751856                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:05 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00211, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:05 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00222, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:05 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00217, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:05 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00232, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00238, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00235, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00215, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00246, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00231, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00209, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:06 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00224, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00214, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00225, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00244, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00245, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00211, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:07 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00216, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:08 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00220, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:08 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00214, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:08 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00216, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:08 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00210, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:09 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8591230439997162                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3583665920014028                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019559337822203007                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019481054058738897                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019559337822203007                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019481054058738897                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00024566061571628474                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00026793042648001                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00024566061571628474                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00026793042648001                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021545624282831946                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.002195646174325438                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:09 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00231, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:09 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00225, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:09 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00217, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00206, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00213, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00219, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00210, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00204, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:10 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00207, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00212, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00217, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00209, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00212, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00218, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:11 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00218, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00217, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00225, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00203, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00215, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00216, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:12 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00212, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:13 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9284989609950571                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3530217000006814                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019243077581955327                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019296560559210738                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019243077581955327                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019296560559210738                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00022652718180324883                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00025088702336531775                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00022652718180324883                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00025088702336531775                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020825255959708655                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0021477507776007277                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:29:13 TCNTrainer] INFO: Saving checkpoint to 230629_042913_model.pt\u001b[0m\n",
      "\u001b[32m[04:29:13 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00010062895363475553, 5.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:13 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02427, embedding_loss_attractive=   0.02427, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00397, embedding_loss_attractive=   0.00351, embedding_loss_repulsive=   0.00701\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00295, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00644\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00257, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00378\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00241, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00312\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00245, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00282\u001b[0m\n",
      "\u001b[36m[04:29:14 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00257, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00264\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00256, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00232\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00262, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00215\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00253, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00223\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00257, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00162\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00250, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00158\u001b[0m\n",
      "\u001b[36m[04:29:15 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00266, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00182\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00289, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00264, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00141\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00271, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00155\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00248, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00284, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00155\u001b[0m\n",
      "\u001b[36m[04:29:16 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00263, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:29:17 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00257, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00131\u001b[0m\n",
      "\u001b[36m[04:29:17 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00259, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:18 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8717128029966261                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4699397630029125                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018419374491915935                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002316363709244712                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018419374491915935                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002316363709244712                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0011379151314031334                             │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002364144864976131                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0011379151314031334                             │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002364144864976131                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002507253086918758                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.0029701556209841824                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:18 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00260, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00130\u001b[0m\n",
      "\u001b[36m[04:29:18 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00258, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:29:18 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00256, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:29:18 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00255, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:29:18 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00254, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00260, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00254, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00252, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00263, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:29:19 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00241, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00235, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00074\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00253, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00242, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00259, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00253, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00250, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:29:20 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00248, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:29:21 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00263, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:29:21 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00266, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:29:21 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00256, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:22 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8882060549949529                                │ nan       │\n",
      "│    │ _time_train                              │ 3.371887963003246                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018370592578624686                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019156335178634216                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018370592578624686                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019156335178634216                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006602654825352753                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0008578159797046646                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006602654825352753                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0008578159797046646                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024002992237607637                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0025313578470348578                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:22 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00233, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:29:22 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00254, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:29:22 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00249, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:29:22 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00255, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00238, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00246, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00230, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00227, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00227, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:29:23 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00251, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00247, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00245, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00255, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00249, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00250, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:29:24 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00253, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:29:25 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00238, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:25 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00236, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:29:25 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00249, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:29:25 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00237, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:29:25 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00237, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:26 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9381351859992719                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2841002169952844                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018489930895157158                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018622571352428844                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018489930895157158                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018622571352428844                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004745922174252984                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005887411602169477                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004745922174252984                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005887411602169477                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0023418146651238204                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002426204597348063                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:26 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00232, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:29:26 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00253, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00243, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00227, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00236, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00236, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00241, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00236, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:29:27 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00249, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00257, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00252, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00249, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00237, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00250, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:29:28 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00243, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00247, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00245, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00255, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00232, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00242, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:29:29 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00234, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:30 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9060534970049048                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3443481649956084                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018938093148689304                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018951749983083072                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018938093148689304                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018951749983083072                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003785354395707448                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004672703701369648                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003785354395707448                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004672703701369648                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002339499821472499                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0024288969625962044                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:30 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00236, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00252, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00248, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00244, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00238, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00245, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:29:31 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00245, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00237, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00244, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00229, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00234, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00240, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00039\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:29:32 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00242, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00237, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00246, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00232, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00230, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00231, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:33 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00252, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:29:34 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00241, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:34 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00240, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:35 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9118984230008209                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3997377249979763                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019525778531614276                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019123398829618552                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019525778531614276                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019123398829618552                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000320102784381662                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003916257355830132                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000320102784381662                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003916257355830132                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002363796019926667                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.002417096983999234                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:35 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00237, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:35 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00229, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:35 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00244, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:35 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00255, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:29:35 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00253, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00232, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00249, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00244, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00252, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00246, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:29:36 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00243, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00236, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00241, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00242, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00240, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00248, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:29:37 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00248, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:29:38 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00237, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:38 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00245, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:29:38 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00234, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:38 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00242, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:39 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9126538660057122                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2941426319957827                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019466231809929014                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019495685451797076                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019466231809929014                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019495685451797076                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00032138968688539334                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00034313208983839476                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032138968688539334                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034313208983839476                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024176974625637134                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.002434539561632452                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:39 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00239, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:39 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00234, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:39 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00240, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00241, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00244, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00235, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00239, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00251, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00237, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:40 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00261, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00254, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00254, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00245, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00258, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00250, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:41 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00244, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:42 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00245, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:42 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00241, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:42 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00246, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:42 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00250, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:29:42 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00230, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:43 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8511366970051313                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3709091779965092                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019205167941335176                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019957418467509997                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019205167941335176                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019957418467509997                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00030163802487853295                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00030834923668654425                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030163802487853295                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00030834923668654425                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024007894719640416                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.0024647134220860717                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:43 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00248, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:43 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00239, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00249, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00242, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00262, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00259, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00254, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:44 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00242, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00244, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00233, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00233, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00235, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00024\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00249, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00252, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:29:45 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00243, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00238, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00247, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00262, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00232, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00240, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:46 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00258, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:47 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9356250560012995                                │ nan       │\n",
      "│    │ _time_train                              │ 3.345904406000045                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020035716911984813                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002023195085147845                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020035716911984813                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002023195085147845                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00024525064590205956                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00027743598754205765                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00024525064590205956                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00027743598754205765                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002384623758391374                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.002466737827185193                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:47 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00221, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00248, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00249, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00250, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00248, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00238, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:48 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00240, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00249, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00247, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00250, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00252, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00253, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:29:49 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00244, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00258, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00241, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00235, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00249, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00258, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:29:50 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00255, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:29:51 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00241, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:51 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00233, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:52 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8717442819979624                                │ nan       │\n",
      "│    │ _time_train                              │ 3.289259700002731                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002021664148196578                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020477591728186007                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002021664148196578                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020477591728186007                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00022489239442317435                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00025348611936727857                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00022489239442317435                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00025348611936727857                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002378891835299631                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0024683810197037134                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:52 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00252, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:52 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00249, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:52 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00262, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:52 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00233, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:29:52 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00229, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00235, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00242, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00233, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00236, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00246, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00255, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:53 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00243, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00240, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00248, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00237, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00254, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00252, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:54 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00245, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:55 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00243, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:29:55 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00228, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:29:55 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00252, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:29:56 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9170574750023661                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4049028999943403                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002116920449770987                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002064443016172446                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002116920449770987                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002064443016172446                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0001947274973240888                             │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00023369996797286544                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0001947274973240888                             │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00023369996797286544                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 5.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002409439101918704                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0024621627337111187                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:29:56 TCNTrainer] INFO: Saving checkpoint to 230629_042956_model.pt\u001b[0m\n",
      "\u001b[32m[04:29:56 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.0009653154573041118, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:29:56 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.04068, embedding_loss_attractive=   0.04068, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:29:56 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00549, embedding_loss_attractive=   0.00515, embedding_loss_repulsive=   0.00778\u001b[0m\n",
      "\u001b[36m[04:29:56 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00318, embedding_loss_attractive=   0.00287, embedding_loss_repulsive=   0.00410\u001b[0m\n",
      "\u001b[36m[04:29:56 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00262, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00402\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00224, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00328\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00235, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00345\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00239, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00311\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00259, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00322\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00237, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00278\u001b[0m\n",
      "\u001b[36m[04:29:57 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00236, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00239\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00247, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00220\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00251, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00222\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00231, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00161\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00240, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00246, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00174\u001b[0m\n",
      "\u001b[36m[04:29:58 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00247, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00169\u001b[0m\n",
      "\u001b[36m[04:29:59 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00218, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00130\u001b[0m\n",
      "\u001b[36m[04:29:59 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00230, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00174\u001b[0m\n",
      "\u001b[36m[04:29:59 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00202, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00137\u001b[0m\n",
      "\u001b[36m[04:29:59 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00226, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00154\u001b[0m\n",
      "\u001b[36m[04:29:59 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00217, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00131\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:00 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.908849186002044                                │ nan       │\n",
      "│    │ _time_train                              │ 3.389004909993673                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022512435499164795                            │   0.00012 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0026522670078820785                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022512435499164795                            │   0.00012 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0026522670078820785                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0011921299000581106                            │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0026213108978275523                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0011921299000581106                            │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0026213108978275523                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002370332453089456                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.003024678200189703                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:00 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00236, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:30:00 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00234, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00130\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00231, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00143\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00214, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00132\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00206, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00115\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00202, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00202, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00126\u001b[0m\n",
      "\u001b[36m[04:30:01 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00203, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00185, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00193, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00133\u001b[0m\n",
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00171, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00174, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00093\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00192, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00117\u001b[0m\n",
      "\u001b[36m[04:30:02 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00191, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00185, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00182, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00180, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00185, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00218, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00186, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:30:03 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00198, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:04 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.86011608899571                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.340708445000928                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017250722739845514                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019030140705975553                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017250722739845514                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019030140705975553                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0011016614838606782                            │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001168761827313144                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0011016614838606782                            │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.001168761827313144                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018137748068612484                            │   0.00015 │\n",
      "│    │ total_train                              │ 0.0020215987110868317                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00178, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00175, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00196, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00175, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00171, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00177, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:30:05 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00172, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00179, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00178, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00167, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00161, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00165, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:06 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00154, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00154, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00162, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00156, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00174, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00156, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:30:07 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00160, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:30:08 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00163, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:08 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00153, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:09 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8627936949997093                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4012826370017137                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0015204899905559917                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016755749215715992                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0015204899905559917                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016755749215715992                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009761113964486867                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009647514269655121                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009761113964486867                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009647514269655121                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001527533274040454                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.001675182907025472                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:09 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00161, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:09 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00162, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:30:09 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00152, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:09 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00157, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:30:09 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00166, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00173, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00152, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00159, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00155, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00149, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:30:10 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00156, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00152, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00158, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00155, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00171, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00149, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00141, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:11 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00152, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:12 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00153, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:12 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00153, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:12 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00142, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:13 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9243129579990637                               │ nan       │\n",
      "│    │ _time_train                              │ 3.334148431997164                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001449107513245609                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0015548360602829258                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001449107513245609                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0015548360602829258                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000917785464035761                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009356381966883839                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000917785464035761                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009356381966883839                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014195859807336496                            │   0.00011 │\n",
      "│    │ total_train                              │ 0.0015359290203382375                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:13 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00146, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:13 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00144, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:13 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00141, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:13 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00180, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00161, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00152, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00149, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00141, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00147, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:14 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00150, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00150, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00147, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00090\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00152, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00138, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00129, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:15 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00151, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:16 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00140, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:16 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00144, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:30:16 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00142, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:16 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00145, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:16 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00152, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:17 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8847903779969783                               │ nan       │\n",
      "│    │ _time_train                              │ 3.2743765830018674                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001530982120635195                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014771664804168816                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001530982120635195                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014771664804168816                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008290782202190409                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009436711827820634                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008290782202190409                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009436711827820634                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014502093001889686                            │   0.00010 │\n",
      "│    │ total_train                              │ 0.00146400497416539                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:17 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00159, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:17 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00128, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00131, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00139, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00142, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00146, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00135, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00145, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:18 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00134, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00133, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00143, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00129, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00135, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00143, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:19 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00141, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00146, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00134, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00146, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00146, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00145, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:30:20 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00144, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:21 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8514329039971926                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3636878649995197                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013583110684218505                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014113606781850161                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013583110684218505                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014113606781850161                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000992557178122095                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009505334637114054                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000992557178122095                             │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009505334637114054                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001373901431603978                             │   0.00011 │\n",
      "│    │ total_train                              │ 0.0014027220700532565                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:21 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00133, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00133, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00148, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00144, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00134, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00129, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:22 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00127, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00143, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00137, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00138, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00140, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00135, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00128, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:23 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00133, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00134, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00122, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00152, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00123\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00134, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00135, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:30:24 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00135, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:25 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00125, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:26 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8996128880025935                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3109245429950533                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0013265730489769743                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001357790415308305                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0013265730489769743                            │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001357790415308305                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009398993137033863                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009544097656140957                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009398993137033863                            │   0.00015 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009544097656140957                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013124904142589205                            │   0.00010 │\n",
      "│    │ total_train                              │ 0.0013516333368216902                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00139, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00143, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00124, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00128, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00138, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:26 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00141, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00137, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00134, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00132, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00130, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00145, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:30:27 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00132, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00091\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00127, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00121, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00132, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00129, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00126, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:30:28 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00121, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:30:29 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00137, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:30:29 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00119, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:29 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00123, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:30 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9230366600022535                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3326590719952947                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001316934419123249                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013209719238389888                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001316934419123249                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013209719238389888                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0009072243565848719                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009575700745780223                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0009072243565848719                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009575700745780223                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001285590894985944                             │   0.00010 │\n",
      "│    │ total_train                              │ 0.0013167050602877963                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:30 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00138, embedding_loss_attractive=   0.00136, embedding_loss_repulsive=   0.00102\u001b[0m\n",
      "\u001b[36m[04:30:30 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00123, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:30:30 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00124, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:30 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00130, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00120, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00124, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00128, embedding_loss_attractive=   0.00130, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00133, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00131, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:31 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00132, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00119, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00127, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00110\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00134, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00128, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00087\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00120, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00119, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:32 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00124, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:33 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00126, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:33 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:30:33 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00129, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:34 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8432582889945479                               │ nan       │\n",
      "│    │ _time_train                              │ 3.1710220480017597                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001282652984890673                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00127350818709897                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001282652984890673                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00127350818709897                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008747454299332781                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009571496569872673                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008747454299332781                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009571496569872673                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0012353373846660058                            │   0.00009 │\n",
      "│    │ total_train                              │ 0.0012691531197300979                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:34 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00112, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:30:34 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00125, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:30:34 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00123, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:34 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00126, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00127, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00122, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00128, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00138, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:35 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00130, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00120, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00123, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00139, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00144, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00140, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:30:36 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00128, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:37 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00133, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:37 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:30:37 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:30:37 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00123, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:38 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.891708312999981                                │ nan       │\n",
      "│    │ _time_train                              │ 3.392826547002187                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001304699934553355                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0012604385789878336                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001304699934553355                             │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0012604385789878336                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008618307299911975                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009671458920000552                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008618307299911975                            │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009671458920000552                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0009653154573041118, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00124988039670926                              │   0.00009 │\n",
      "│    │ total_train                              │ 0.001261312808162473                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:30:38 TCNTrainer] INFO: Saving checkpoint to 230629_043038_model.pt\u001b[0m\n",
      "\u001b[32m[04:30:38 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.0004771597001106582, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:38 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03034, embedding_loss_attractive=   0.03034, embedding_loss_repulsive=   0.00148\u001b[0m\n",
      "\u001b[36m[04:30:38 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00478, embedding_loss_attractive=   0.00437, embedding_loss_repulsive=   0.00843\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00314, embedding_loss_attractive=   0.00267, embedding_loss_repulsive=   0.00505\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00255, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00410\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00252, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00348\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00258, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00376\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00260, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00302\u001b[0m\n",
      "\u001b[36m[04:30:39 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00265, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00248\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00251, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00249\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00254, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00226\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00262, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00206\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00273, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00257, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00185\u001b[0m\n",
      "\u001b[36m[04:30:40 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00258, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00266, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00176\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00261, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00149\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00261, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00162\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00256, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00152\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00250, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00134\u001b[0m\n",
      "\u001b[36m[04:30:41 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00241, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00125\u001b[0m\n",
      "\u001b[36m[04:30:42 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00241, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00112\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:42 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9063332229998196                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4782391560001997                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019685440244049664                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0025427982525376997                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019685440244049664                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0025427982525376997                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0012847560063366675                            │   0.00021 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0026766683285817357                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0012847560063366675                            │   0.00021 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0026766683285817357                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002423690080953141                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.003072991402902333                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00247, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00139\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00243, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00118\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00249, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00127\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00254, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00234, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00240, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:30:43 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00236, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00116\u001b[0m\n",
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00236, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00104\u001b[0m\n",
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00231, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00242, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00240, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00236, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00102\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:30:44 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00222, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00092\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00226, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00245, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00241, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00224, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00080\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00234, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:30:45 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00237, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:30:46 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00233, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:30:46 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00219, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:47 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8932850340061123                               │ nan       │\n",
      "│    │ _time_train                              │ 3.244343291997211                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019049627589993179                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020029409788548946                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019049627589993179                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020029409788548946                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008384539545254989                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0010276418933392562                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008384539545254989                            │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0010276418933392562                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002203146297122455                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.00238046753380833                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:47 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00224, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:30:47 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00218, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:30:47 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00227, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:30:47 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00224, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:47 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00238, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00231, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00214, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00212, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00225, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00218, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00225, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00077\u001b[0m\n",
      "\u001b[36m[04:30:48 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00236, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00229, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00200, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00220, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00229, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00230, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:30:49 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00224, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:30:50 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:30:50 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00217, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:30:50 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00207, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:51 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8327487639980973                               │ nan       │\n",
      "│    │ _time_train                              │ 3.303941682999721                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001980024344649994                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019672596225001173                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001980024344649994                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019672596225001173                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006165111689673115                            │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0007569844025334004                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006165111689673115                            │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0007569844025334004                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021200697403401135                            │   0.00014 │\n",
      "│    │ total_train                              │ 0.00222206181442833                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:51 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00210, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:30:51 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00231, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:30:51 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00215, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:30:51 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00207, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00223, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00219, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00207, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00212, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00201, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:30:52 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00205, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00213, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00212, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00191, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00192, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00204, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00190, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:30:53 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00197, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:30:54 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00193, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:30:54 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00189, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:30:54 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00199, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:30:54 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00195, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:55 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9315351949990145                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3776404579984955                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018462083263633153                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019023859078578394                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018462083263633153                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019023859078578394                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0005398383153887051                            │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006295063898765615                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005398383153887051                            │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006295063898765615                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0019167931142470074                            │   0.00013 │\n",
      "│    │ total_train                              │ 0.0020642902161880153                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:55 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00197, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:30:55 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00200, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:30:55 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00204, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00201, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00213, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00197, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00180, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00186, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:30:56 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00185, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00207, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00201, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00189, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00049\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00168, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00194, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:30:57 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00195, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00191, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00188, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00183, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00188, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00182, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:30:58 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00179, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:30:59 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9043953000000329                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3121785770053975                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016856928317186732                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018114356193324468                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016856928317186732                            │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018114356193324468                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0005086025611187021                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005491416932317644                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005086025611187021                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005491416932317644                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001723310858425167                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.001894907194896248                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:30:59 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00175, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00160, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00186, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00182, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00190, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00165, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:00 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00182, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00177, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00169, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00173, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00160, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00177, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00161, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:01 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00171, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00162, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00165, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00187, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00173, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00183, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:02 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00193, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:31:03 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00159, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:04 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8581506429982255                               │ nan       │\n",
      "│    │ _time_train                              │ 3.379930944996886                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001669758370715297                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017209766585187152                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001669758370715297                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017209766585187152                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004669612066613303                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005096494740051828                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004669612066613303                            │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005096494740051828                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0016571938616430594                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.001760400860373096                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00175, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00160, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00160, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00171, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00172, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:04 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00165, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00169, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00182, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00174, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00173, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00165, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:31:05 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00186, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00181, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00154, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00168, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00168, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00162, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00155, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:06 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00166, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:07 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00170, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:31:07 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00200, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:08 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9445753000036348                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3042540829992504                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0019221082446165382                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016874384772711493                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0019221082446165382                            │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016874384772711493                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004345701661400704                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000494050743887568                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004345701661400704                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000494050743887568                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0018687437340203258                            │   0.00013 │\n",
      "│    │ total_train                              │ 0.0017084333841491655                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:08 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00193, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:08 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00167, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:08 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00159, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:08 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00154, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:08 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00156, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00158, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00153, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00164, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00164, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00176, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:09 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00172, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00164, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00047\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00163, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00159, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00160, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00152, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:10 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00159, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:11 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00159, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:11 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00143, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:11 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00151, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:11 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00154, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:12 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8725474640013999                               │ nan       │\n",
      "│    │ _time_train                              │ 3.3121520770000643                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001571724616870698                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016188307773101654                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001571724616870698                             │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016188307773101654                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004290243225922394                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004772212506292544                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004290243225922394                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004772212506292544                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0015112706240163081                            │   0.00011 │\n",
      "│    │ total_train                              │ 0.001618900862774752                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:12 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00160, embedding_loss_attractive=   0.00159, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:12 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00176, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:12 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00164, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00144, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00153, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00163, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00164, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00160, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:13 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00143, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00156, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00165, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00186, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00171, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00145, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:14 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00157, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00141, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00140, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00147, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00152, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00149, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:15 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00150, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:16 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8942797790004988                               │ nan       │\n",
      "│    │ _time_train                              │ 3.4426210329984315                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014681709619859854                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0015862787817948925                            │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014681709619859854                            │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0015862787817948925                            │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004519097856245935                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00047265321375880153                           │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004519097856245935                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047265321375880153                           │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0014366706108881367                            │   0.00012 │\n",
      "│    │ total_train                              │ 0.0015806303442148445                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:16 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00137, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00148, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00155, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00147, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00148, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00155, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00153, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:17 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00155, embedding_loss_attractive=   0.00149, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00141, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00149, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00150, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00151, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00150, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:18 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00146, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00166, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00173, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00156, embedding_loss_attractive=   0.00155, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00165, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00146, embedding_loss_attractive=   0.00150, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:19 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00153, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:20 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00149, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:20 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬──────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                            │       Std │\n",
      "├────┼──────────────────────────────────────────┼──────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9258549219957786                               │ nan       │\n",
      "│    │ _time_train                              │ 3.2802765669985092                               │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.00143650461298724                              │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00151512427014652                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.00143650461298724                              │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00151512427014652                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00042096392458511724                           │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004649424555268726                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00042096392458511724                           │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004649424555268726                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.0004771597001106582, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0013675971512889695                            │   0.00011 │\n",
      "│    │ total_train                              │ 0.0015000024497316316                            │ nan       │\n",
      "└────┴──────────────────────────────────────────┴──────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:31:21 TCNTrainer] INFO: Saving checkpoint to 230629_043121_model.pt\u001b[0m\n",
      "\u001b[32m[04:31:21 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00028893887271952276, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:21 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02285, embedding_loss_attractive=   0.02285, embedding_loss_repulsive=   0.00141\u001b[0m\n",
      "\u001b[36m[04:31:21 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00440, embedding_loss_attractive=   0.00387, embedding_loss_repulsive=   0.01010\u001b[0m\n",
      "\u001b[36m[04:31:21 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00295, embedding_loss_attractive=   0.00242, embedding_loss_repulsive=   0.00548\u001b[0m\n",
      "\u001b[36m[04:31:21 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00266, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00495\u001b[0m\n",
      "\u001b[36m[04:31:21 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00250, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00424\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00246, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00324\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00262, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00279\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00256, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00276\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00242, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00221\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00252, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00212\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00265, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00185\u001b[0m\n",
      "\u001b[36m[04:31:22 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00258, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00187\u001b[0m\n",
      "\u001b[36m[04:31:23 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00248, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00229\u001b[0m\n",
      "\u001b[36m[04:31:23 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00239, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00154\u001b[0m\n",
      "\u001b[36m[04:31:23 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00265, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00189\u001b[0m\n",
      "\u001b[36m[04:31:23 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00246, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00164\u001b[0m\n",
      "\u001b[36m[04:31:23 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00255, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00185\u001b[0m\n",
      "\u001b[36m[04:31:24 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00256, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00147\u001b[0m\n",
      "\u001b[36m[04:31:24 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00255, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00156\u001b[0m\n",
      "\u001b[36m[04:31:24 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00251, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00131\u001b[0m\n",
      "\u001b[36m[04:31:24 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00251, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00136\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:25 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8395119970009546                                │ nan       │\n",
      "│    │ _time_train                              │ 3.5419127639979706                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.00192087049363181                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022650660701518137                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.00192087049363181                               │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022650660701518137                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0013107492349162283                             │   0.00021 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0027803637751428584                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0013107492349162283                             │   0.00021 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0027803637751428584                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0024972514430474904                             │   0.00016 │\n",
      "│    │ total_train                              │ 0.002873629114815387                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:25 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00248, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00140\u001b[0m\n",
      "\u001b[36m[04:31:25 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00236, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:31:25 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00252, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:31:25 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00242, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00115\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00253, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00134\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00244, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00109\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00245, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00129\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00237, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00246, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:31:26 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00244, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00241, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00245, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00090\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00244, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00242, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00096\u001b[0m\n",
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00271, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00079\u001b[0m\n",
      "\u001b[36m[04:31:27 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00255, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:31:28 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00228, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00083\u001b[0m\n",
      "\u001b[36m[04:31:28 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00235, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:31:28 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00230, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:31:28 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00227, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00093\u001b[0m\n",
      "\u001b[36m[04:31:28 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00226, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:29 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9474821850017179                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3641791889967863                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018789102090522648                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0019175873867470056                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018789102090522648                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0019175873867470056                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0008570523880836036                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001016663802326588                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0008570523880836036                             │   0.00014 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.001016663802326588                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002369866251117653                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.00243027963597566                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:29 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00246, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:31:29 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00232, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00218, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00225, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00215, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00218, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00212, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:31:30 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00235, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00231, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00074\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00227, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00067\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00215, embedding_loss_attractive=   0.00182, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00221, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00221, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:31:31 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00215, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00207, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00215, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00222, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00241, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00221, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:31:32 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00215, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:31:33 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00207, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:33 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9290399499950581                                │ nan       │\n",
      "│    │ _time_train                              │ 3.353859322000062                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018848713215750953                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018435663909518293                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018848713215750953                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018435663909518293                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0006330047633835218                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00069772578567138                               │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0006330047633835218                             │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00069772578567138                               │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0022601451879988113                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.002240954667761041                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00228, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00233, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00218, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00209, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00207, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:31:34 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00236, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00211, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00208, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00212, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00216, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00216, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00206, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:35 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00239, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00219, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00210, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00203, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00220, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00227, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:31:36 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00216, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:31:37 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00224, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:31:37 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00228, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:38 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.911489807003818                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3577800409984775                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001782814075704664                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0018131542306283278                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001782814075704664                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0018131542306283278                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004664999167693572                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005641959967743146                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004664999167693572                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005641959967743146                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020098003804580205                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002138139690923977                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:38 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00204, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:38 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00213, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:31:38 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00211, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:31:38 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00205, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00203, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00206, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00203, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00202, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00213, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:31:39 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00210, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00201, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00214, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00051\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00206, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00211, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00202, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00199, embedding_loss_attractive=   0.00188, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:40 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00202, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:41 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00194, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:41 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00207, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:41 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00215, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:41 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00220, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:42 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.923439125996083                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.417589240001689                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0018272203044034541                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017884603530636488                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0018272203044034541                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017884603530636488                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004274296891202943                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004802660909343713                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004274296891202943                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004802660909343713                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0020259342492661542                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0020475406216897913                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:42 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00213, embedding_loss_attractive=   0.00184, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:31:42 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00203, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00189, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00203, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00201, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00205, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00189, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00189, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:43 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00191, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00192, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00193, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00191, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00184, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00189, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:44 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00206, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00185, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00191, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00186, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00197, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00185, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:45 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00194, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:46 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8937692330000573                                │ nan       │\n",
      "│    │ _time_train                              │ 3.275084062006499                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0017550479775915544                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017609357153721558                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0017550479775915544                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017609357153721558                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00039998366329301564                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0004255129807179087                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00039998366329301564                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004255129807179087                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001928918159359859                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0019655738363106704                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:46 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00182, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00216, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00199, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00185, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00176, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00180, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:47 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00186, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00186, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00185, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00188, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00190, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00195, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00176, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:48 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00201, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00188, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00189, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00190, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00187, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00183, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:49 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00183, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:50 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00187, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:51 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.876882108997961                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3691858559977845                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016734259487647149                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001733634891011408                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016734259487647149                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001733634891011408                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003541716152944395                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003887140993522067                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003541716152944395                             │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003887140993522067                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001782754202011145                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0018951808007232147                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00178, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00195, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00184, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00182, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00199, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:31:51 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00195, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00193, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00192, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00178, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00184, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00183, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:52 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00177, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00036\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00182, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00192, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00209, embedding_loss_attractive=   0.00190, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00194, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00192, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00181, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:53 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00197, embedding_loss_attractive=   0.00180, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:31:54 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00169, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:54 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00182, embedding_loss_attractive=   0.00172, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:55 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9216803320014151                                │ nan       │\n",
      "│    │ _time_train                              │ 3.325286466999387                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016944229007802075                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0017230832866700413                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016944229007802075                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0017230832866700413                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003116672776134995                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003656139792278042                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003116672776134995                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003656139792278042                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017347054077415831                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.001855152698114441                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:55 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00172, embedding_loss_attractive=   0.00165, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:55 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00183, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:55 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00181, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:31:55 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00189, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00169, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00191, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00173, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00184, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00177, embedding_loss_attractive=   0.00163, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00178, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:56 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00181, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00175, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00193, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00192, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00184, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00186, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:31:57 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00186, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:58 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00178, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:31:58 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00195, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:31:58 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00176, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:31:58 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00189, embedding_loss_attractive=   0.00181, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:31:59 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8341132720015594                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4083902560014394                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0016679889134441812                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016995158041896198                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0016679889134441812                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016995158041896198                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00031515390001004564                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003449828627753287                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031515390001004564                            │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003449828627753287                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0017165425207672847                             │   0.00012 │\n",
      "│    │ total_train                              │ 0.001801013323739905                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:31:59 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00178, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:31:59 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00165, embedding_loss_attractive=   0.00164, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:31:59 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00182, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00187, embedding_loss_attractive=   0.00175, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00169, embedding_loss_attractive=   0.00170, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00183, embedding_loss_attractive=   0.00168, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00179, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00182, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:00 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00177, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00165, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00165, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00178, embedding_loss_attractive=   0.00176, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00182, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00173, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00172, embedding_loss_attractive=   0.00171, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:01 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00183, embedding_loss_attractive=   0.00174, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:02 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00171, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:02 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00186, embedding_loss_attractive=   0.00178, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:02 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00172, embedding_loss_attractive=   0.00167, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:02 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00191, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:02 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00180, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:03 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9257825420063455                                │ nan       │\n",
      "│    │ _time_train                              │ 3.352095878995897                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001599683615172075                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0016870203948685248                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001599683615172075                              │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0016870203948685248                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000312846154136221                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003302354040713935                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000312846154136221                              │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003302354040713935                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00028893887271952276, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.001645564216758228                              │   0.00012 │\n",
      "│    │ total_train                              │ 0.0017648419752955584                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:32:03 TCNTrainer] INFO: Saving checkpoint to 230629_043203_model.pt\u001b[0m\n",
      "\u001b[32m[04:32:03 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00016839923191582784, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:03 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03068, embedding_loss_attractive=   0.03068, embedding_loss_repulsive=   0.00162\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00481, embedding_loss_attractive=   0.00438, embedding_loss_repulsive=   0.00819\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00339, embedding_loss_attractive=   0.00279, embedding_loss_repulsive=   0.00592\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00291, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00489\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00262, embedding_loss_attractive=   0.00186, embedding_loss_repulsive=   0.00397\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00261, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00295\u001b[0m\n",
      "\u001b[36m[04:32:04 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00283, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00280\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00298, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00276\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00291, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00258\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00282, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00240\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00274, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00215\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00266, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00164\u001b[0m\n",
      "\u001b[36m[04:32:05 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00288, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00172\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00286, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00181\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00278, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00150\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00275, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00151\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00278, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00157\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00272, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00133\u001b[0m\n",
      "\u001b[36m[04:32:06 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00278, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00139\u001b[0m\n",
      "\u001b[36m[04:32:07 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00280, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00129\u001b[0m\n",
      "\u001b[36m[04:32:07 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00286, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:08 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9506973940005992                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4611086820004857                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022388794045481416                             │   0.00012 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002585847111386729                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022388794045481416                             │   0.00012 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002585847111386729                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001171973678154043                              │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002695007140179316                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.001171973678154043                              │   0.00018 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.002695007140179316                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0028550024532402554                             │   0.00017 │\n",
      "│    │ total_train                              │ 0.003265399716363238                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:08 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00272, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:32:08 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00275, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00120\u001b[0m\n",
      "\u001b[36m[04:32:08 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00267, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00106\u001b[0m\n",
      "\u001b[36m[04:32:08 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00275, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "\u001b[36m[04:32:08 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00279, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00288, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00272, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00099\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00267, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00285, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00266, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:32:09 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00271, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00097\u001b[0m\n",
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00266, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00090\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00269, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00094\u001b[0m\n",
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00267, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00267, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00272, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:32:10 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00272, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00084\u001b[0m\n",
      "\u001b[36m[04:32:11 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00261, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:32:11 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00267, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00081\u001b[0m\n",
      "\u001b[36m[04:32:11 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00267, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00085\u001b[0m\n",
      "\u001b[36m[04:32:11 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00251, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:12 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.822912143994472                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.3899367100020754                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020665843337256875                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002099200339960347                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020665843337256875                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002099200339960347                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000694741700297325                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0009657561534148609                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000694741700297325                              │   0.00011 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0009657561534148609                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002568654868648284                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.0027156764402838764                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:12 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00252, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:32:12 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00252, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "\u001b[36m[04:32:12 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00276, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00076\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00272, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00256, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00256, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00266, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00262, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:32:13 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00259, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00266, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00266, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00264, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00257, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00261, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00264, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00063\u001b[0m\n",
      "\u001b[36m[04:32:14 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00257, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00066\u001b[0m\n",
      "\u001b[36m[04:32:15 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00257, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:32:15 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00261, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:15 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00254, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:15 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00253, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:32:15 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00263, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00068\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:16 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9280910719971871                                │ nan       │\n",
      "│    │ _time_train                              │ 3.418887978004932                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020927086032720074                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002064044637779884                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020927086032720074                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002064044637779884                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0005234248865033603                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0006721635667946685                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005234248865033603                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006721635667946685                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025322222616523506                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0026128313703728573                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:16 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00258, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00257, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00256, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00263, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00057\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00279, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00261, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00252, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:32:17 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00262, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00243, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00253, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00254, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00254, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00252, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:32:18 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00253, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00255, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00251, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00262, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00259, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00282, embedding_loss_attractive=   0.00237, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:32:19 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00256, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:32:20 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00252, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:20 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8897458909996203                                │ nan       │\n",
      "│    │ _time_train                              │ 3.276311485002225                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020786895093301105                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020766996461989845                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020786895093301105                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020766996461989845                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00045257852470967916                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000530400488346705                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00045257852470967916                            │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000530400488346705                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025021262906698718                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.002568834056398772                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00254, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00258, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00050\u001b[0m\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00265, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00258, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00236, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:32:21 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00244, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00255, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00270, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00238, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00239, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00246, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:32:22 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00242, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00037\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00241, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00243, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00239, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00247, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00229, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:32:23 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00247, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:32:24 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00255, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:32:24 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00250, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:24 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00236, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:25 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8704360319970874                                │ nan       │\n",
      "│    │ _time_train                              │ 3.442648604999704                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020242558947453897                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020458435982676783                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020242558947453897                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020458435982676783                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00038852110931960243                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00044049773313072993                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00038852110931960243                            │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00044049773313072993                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002401801731644405                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.00248097582362372                               │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:25 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00257, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:32:25 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00246, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:32:25 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00258, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:32:25 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00241, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:25 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00240, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00041\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00240, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00260, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00240, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00235, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00249, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:26 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00247, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00235, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00238, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00246, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00251, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00252, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:32:27 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00234, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:28 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00236, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:32:28 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00243, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:28 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00242, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:32:28 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00230, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:29 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.905139031005092                                 │ nan       │\n",
      "│    │ _time_train                              │ 3.2909863859968027                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020320490998629896                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020368037265931886                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020320490998629896                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020368037265931886                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003294628173332765                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003855556561875391                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003294628173332765                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003855556561875391                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0023419226390413114                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.002431330342176072                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:29 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00246, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:29 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00245, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:29 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00236, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00242, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00244, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00245, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00252, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00251, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00236, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:30 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00223, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00232, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00254, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00241, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00239, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00232, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00220, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:31 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00244, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:32 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00242, embedding_loss_attractive=   0.00199, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:32 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00240, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:32 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00253, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:32 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00246, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:33 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8243718469966552                                │ nan       │\n",
      "│    │ _time_train                              │ 3.1047373980036355                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002088805629561345                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020493528871049216                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002088805629561345                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020493528871049216                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002925868611782789                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00034584335393841225                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002925868611782789                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034584335393841225                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002352144444982211                              │   0.00013 │\n",
      "│    │ total_train                              │ 0.0024077523536429617                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:33 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00244, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:33 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00234, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:33 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00245, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:32:33 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00224, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00234, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00233, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00244, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00234, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00224, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00239, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:32:34 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00235, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00240, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00033\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00225, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00240, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00241, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00225, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00246, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:35 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00246, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:32:36 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00238, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:36 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00242, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:36 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00242, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:37 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8409046739980113                                │ nan       │\n",
      "│    │ _time_train                              │ 2.9948956340012955                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020488513271427816                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0020638076850015836                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020488513271427816                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0020638076850015836                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002787504565074212                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.000316996277043862                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002787504565074212                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000316996277043862                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0023034333406637113                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0023924436757405258                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:37 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00228, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:32:37 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00229, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:37 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00242, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:37 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00241, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:32:37 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00228, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00227, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00231, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00239, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00246, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00223, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00242, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:38 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00224, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00236, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00229, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00235, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00214, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00212, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00228, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:32:39 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00240, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:32:40 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00209, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:32:40 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00222, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:41 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8036602880019927                                │ nan       │\n",
      "│    │ _time_train                              │ 3.0133567930024583                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020497444551438094                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002010946113873145                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020497444551438094                             │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002010946113873145                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002659953120099898                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002858712936618498                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002659953120099898                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002858712936618498                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002291303601426383                              │   0.00014 │\n",
      "│    │ total_train                              │ 0.002291607428479738                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00240, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00252, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00217, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00229, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00241, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00226, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:41 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00225, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00215, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00216, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00225, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00227, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00226, embedding_loss_attractive=   0.00202, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:32:42 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00230, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00235, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00220, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00200, embedding_loss_attractive=   0.00189, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00230, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00225, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00217, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:32:43 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00212, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:32:44 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00204, embedding_loss_attractive=   0.00194, embedding_loss_repulsive=   0.00021\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:44 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8135537310008658                                │ nan       │\n",
      "│    │ _time_train                              │ 3.050014733998978                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0020313065382651986                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.00198973645169398                               │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0020313065382651986                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.00198973645169398                               │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00022351998535062497                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00026558271409677606                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00022351998535062497                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00026558271409677606                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00016839923191582784, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0021762941291348804                             │   0.00013 │\n",
      "│    │ total_train                              │ 0.0022376089051751227                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:32:44 TCNTrainer] INFO: Saving checkpoint to 230629_043244_model.pt\u001b[0m\n",
      "\u001b[32m[04:32:44 TCNTrainer] INFO: Using device cuda\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training for = (0.00010062895363475553, 1.0, 1.0, 1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:44 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.03166, embedding_loss_attractive=   0.03166, embedding_loss_repulsive=   0.00176\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.00392, embedding_loss_attractive=   0.00356, embedding_loss_repulsive=   0.00692\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00291, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00646\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00265, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00434\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00256, embedding_loss_attractive=   0.00193, embedding_loss_repulsive=   0.00339\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00269, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00343\u001b[0m\n",
      "\u001b[36m[04:32:45 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00252, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00256\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00258, embedding_loss_attractive=   0.00183, embedding_loss_repulsive=   0.00264\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00265, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00220\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00263, embedding_loss_attractive=   0.00185, embedding_loss_repulsive=   0.00232\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00269, embedding_loss_attractive=   0.00195, embedding_loss_repulsive=   0.00207\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00265, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00180\u001b[0m\n",
      "\u001b[36m[04:32:46 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00263, embedding_loss_attractive=   0.00192, embedding_loss_repulsive=   0.00177\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00250, embedding_loss_attractive=   0.00196, embedding_loss_repulsive=   0.00131\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00272, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00154\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00274, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00188\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00261, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00128\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00283, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00152\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00289, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00149\u001b[0m\n",
      "\u001b[36m[04:32:47 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00265, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00121\u001b[0m\n",
      "\u001b[36m[04:32:48 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00267, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00119\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:49 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9312470109944115                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2716742060001707                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002074356463789526                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0024374063583228536                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002074356463789526                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0024374063583228536                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0011801346134032225                             │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0027424836613861798                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0011801346134032225                             │   0.00019 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0027424836613861798                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002720344299450517                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.003122412453960682                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00285, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00132\u001b[0m\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00279, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00111\u001b[0m\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00270, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00108\u001b[0m\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00279, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00113\u001b[0m\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00280, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00114\u001b[0m\n",
      "\u001b[36m[04:32:49 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00268, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00100\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00272, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00107\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00270, embedding_loss_attractive=   0.00201, embedding_loss_repulsive=   0.00105\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00278, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00098\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00272, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00285, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:32:50 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00282, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00078\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00286, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00103\u001b[0m\n",
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00283, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00101\u001b[0m\n",
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00280, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00299, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00091\u001b[0m\n",
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00285, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00095\u001b[0m\n",
      "\u001b[36m[04:32:51 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00290, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00089\u001b[0m\n",
      "\u001b[36m[04:32:52 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00292, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00090\u001b[0m\n",
      "\u001b[36m[04:32:52 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00278, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:32:52 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00282, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:53 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8702625269943383                                │ nan       │\n",
      "│    │ _time_train                              │ 3.33840107099968                                  │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022047825442213153                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021208837964420617                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022047825442213153                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021208837964420617                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0007836560291859011                             │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001004205179673716                              │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0007836560291859011                             │   0.00013 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.001004205179673716                              │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0028625232452112767                             │   0.00016 │\n",
      "│    │ total_train                              │ 0.002816556124796568                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:53 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00290, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:32:53 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00275, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:32:53 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00284, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00075\u001b[0m\n",
      "\u001b[36m[04:32:53 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00295, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00082\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00294, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00286, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00078\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00288, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00088\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00293, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00086\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00280, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00071\u001b[0m\n",
      "\u001b[36m[04:32:54 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00285, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00291, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00059\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00299, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00070\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00291, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00073\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00292, embedding_loss_attractive=   0.00228, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00295, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00281, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:32:55 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00294, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:32:56 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00271, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00060\u001b[0m\n",
      "\u001b[36m[04:32:56 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00288, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:32:56 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00301, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00064\u001b[0m\n",
      "\u001b[36m[04:32:56 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00278, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:32:57 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9127568069961853                                │ nan       │\n",
      "│    │ _time_train                              │ 3.332420771002944                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002146579343308177                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022265119604270886                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002146579343308177                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022265119604270886                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.000647378234942961                              │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0007057532152273403                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000647378234942961                              │   0.00010 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0007057532152273403                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0028493087092000577                             │   0.00016 │\n",
      "│    │ total_train                              │ 0.0029024950611712457                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:32:57 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00276, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00065\u001b[0m\n",
      "\u001b[36m[04:32:57 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00299, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00069\u001b[0m\n",
      "\u001b[36m[04:32:57 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00295, embedding_loss_attractive=   0.00243, embedding_loss_repulsive=   0.00049\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00290, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00300, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00337, embedding_loss_attractive=   0.00279, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00284, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00306, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00072\u001b[0m\n",
      "\u001b[36m[04:32:58 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00298, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00290, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00056\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00277, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00054\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00275, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00285, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00051\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00292, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:32:59 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00273, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00276, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00295, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00058\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00297, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00062\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00290, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00303, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00061\u001b[0m\n",
      "\u001b[36m[04:33:00 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00280, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00052\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:01 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9206111879975651                                │ nan       │\n",
      "│    │ _time_train                              │ 3.2139802539968514                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022225358290597795                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022653936124016794                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022225358290597795                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022653936124016794                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004830775394414862                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0005528612093589374                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004830775394414862                             │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005528612093589374                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0028264553932886983                             │   0.00015 │\n",
      "│    │ total_train                              │ 0.0029106968876176278                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:01 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00288, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:33:01 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00290, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00283, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00053\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00283, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00280, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00288, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00288, embedding_loss_attractive=   0.00231, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:33:02 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00316, embedding_loss_attractive=   0.00240, embedding_loss_repulsive=   0.00055\u001b[0m\n",
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00292, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00290, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00048\u001b[0m\n",
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00276, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00284, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00045\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00294, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:33:03 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00282, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00046\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00278, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00044\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00278, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00045\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00293, embedding_loss_attractive=   0.00236, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00283, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00047\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00277, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:33:04 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00272, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:33:05 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00276, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:05 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8523613229990588                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3751190249968204                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021618455544941957                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0022507586660955488                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021618455544941957                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0022507586660955488                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0004118374830189471                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00045111055415366855                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004118374830189471                             │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00045111055415366855                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0027376907515443034                             │   0.00016 │\n",
      "│    │ total_train                              │ 0.0028502644771443947                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00271, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00043\u001b[0m\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00284, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00280, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00269, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00274, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:06 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00279, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00267, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00263, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00263, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00266, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00288, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:07 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00270, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00272, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00042\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00271, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00276, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00039\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00276, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00284, embedding_loss_attractive=   0.00233, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00266, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:33:08 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00257, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:33:09 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00279, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:33:09 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00267, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00035\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:10 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9521744790035882                                │ nan       │\n",
      "│    │ _time_train                              │ 3.337507607997395                                 │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021750909780773025                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021787468507986863                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021750909780773025                             │   0.00011 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021787468507986863                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0003245646484881743                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003779948368758018                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003245646484881743                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003779948368758018                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.002645114687685337                              │   0.00015 │\n",
      "│    │ total_train                              │ 0.002725344064090346                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:10 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00259, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:10 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00251, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:10 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00264, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:10 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00270, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00040\u001b[0m\n",
      "\u001b[36m[04:33:10 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00268, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00264, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00038\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00258, embedding_loss_attractive=   0.00200, embedding_loss_repulsive=   0.00037\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00263, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00260, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:11 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00279, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00264, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00262, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00266, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00258, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00274, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00036\u001b[0m\n",
      "\u001b[36m[04:33:12 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00265, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:13 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00272, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:13 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00254, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:13 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00252, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:13 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00271, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:14 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8744177280022996                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3520228909983416                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0022014885985602935                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002143559194046068                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0022014885985602935                             │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002143559194046068                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002753997924931658                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0003312653106387264                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002753997924931658                             │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003312653106387264                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026095305491859715                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0026540490053135334                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:14 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00267, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:14 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00258, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:33:14 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00254, embedding_loss_attractive=   0.00212, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00274, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00255, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00265, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00274, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00264, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:15 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00250, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00280, embedding_loss_attractive=   0.00239, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00284, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00034\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00262, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00030\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00250, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00255, embedding_loss_attractive=   0.00206, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00266, embedding_loss_attractive=   0.00213, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:16 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00263, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:17 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00276, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:17 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00251, embedding_loss_attractive=   0.00198, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:17 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00253, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:33:17 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00264, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:17 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00284, embedding_loss_attractive=   0.00238, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:18 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9296776839983067                                │ nan       │\n",
      "│    │ _time_train                              │ 3.4182384430023376                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0021917319908324215                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021398442142454934                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0021917319908324215                             │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021398442142454934                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0002605282121092185                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00029734862963164344                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002605282121092185                             │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00029734862963164344                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.00260165516131868                               │   0.00015 │\n",
      "│    │ total_train                              │ 0.0026210698177533344                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:18 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00254, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00266, embedding_loss_attractive=   0.00220, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00266, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00032\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00261, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00259, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00261, embedding_loss_attractive=   0.00211, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:19 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00262, embedding_loss_attractive=   0.00207, embedding_loss_repulsive=   0.00031\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00264, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00033\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00267, embedding_loss_attractive=   0.00229, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00267, embedding_loss_attractive=   0.00232, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00258, embedding_loss_attractive=   0.00214, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00258, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:20 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00245, embedding_loss_attractive=   0.00208, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00267, embedding_loss_attractive=   0.00217, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00277, embedding_loss_attractive=   0.00225, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00257, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00270, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00261, embedding_loss_attractive=   0.00224, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00263, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:21 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00256, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:33:22 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00271, embedding_loss_attractive=   0.00227, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:23 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9458290710026631                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3427219039949705                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002280487570290764                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.002160674626273768                              │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002280487570290764                              │   0.00010 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.002160674626273768                              │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00023549757946360236                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00027066099982458655                            │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00023549757946360236                            │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00027066099982458655                            │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0026562938906459343                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.0026150499264081125                             │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00273, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00262, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00259, embedding_loss_attractive=   0.00209, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00258, embedding_loss_attractive=   0.00216, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00268, embedding_loss_attractive=   0.00230, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:23 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00254, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00028\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00261, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00256, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00268, embedding_loss_attractive=   0.00235, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00257, embedding_loss_attractive=   0.00204, embedding_loss_repulsive=   0.00029\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00267, embedding_loss_attractive=   0.00226, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:24 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00255, embedding_loss_attractive=   0.00218, embedding_loss_repulsive=   0.00023\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00251, embedding_loss_attractive=   0.00222, embedding_loss_repulsive=   0.00020\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00277, embedding_loss_attractive=   0.00219, embedding_loss_repulsive=   0.00030\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00263, embedding_loss_attractive=   0.00221, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00251, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00022\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00270, embedding_loss_attractive=   0.00223, embedding_loss_repulsive=   0.00026\u001b[0m\n",
      "\u001b[36m[04:33:25 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00255, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:26 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00256, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00024\u001b[0m\n",
      "\u001b[36m[04:33:26 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00256, embedding_loss_attractive=   0.00205, embedding_loss_repulsive=   0.00027\u001b[0m\n",
      "\u001b[36m[04:33:26 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00247, embedding_loss_attractive=   0.00203, embedding_loss_repulsive=   0.00025\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[04:33:27 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                             │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8340776580007514                                │ nan       │\n",
      "│    │ _time_train                              │ 3.3175457899997127                                │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.002210502924087147                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0021650193825712787                             │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.002210502924087147                              │   0.00009 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0021650193825712787                             │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.00020787193223239026                            │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0002462149554980561                             │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00020787193223239026                            │   0.00003 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0002462149554980561                             │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'repulsive': (1.0, 0.00010062895363475553, 1.0)} │ nan       │\n",
      "│    │ total                                    │ 0.0025322989690014057                             │   0.00014 │\n",
      "│    │ total_train                              │ 0.002585931246402889                              │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[04:33:27 TCNTrainer] INFO: Saving checkpoint to 230629_043327_model.pt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_loss_history(constraints_2, \"loss_histories/damping_05\")\n",
    "save_loss_history(constraints_3, \"loss_histories/damping_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530ac50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
