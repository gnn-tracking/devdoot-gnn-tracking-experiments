{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "206e8591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from gnn_tracking_legacy.postprocessing.dbscanscanner import dbscan_scan\n",
    "\n",
    "from gnn_tracking_legacy.models.graph_construction import GraphConstructionFCNN\n",
    "from tcn_trainer import TCNTrainer\n",
    "\n",
    "from gnn_tracking_legacy.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from gnn_tracking_legacy.utils.loading import get_loaders, TrackingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d190aae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = Path(\"/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1\")\n",
    "assert graph_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e34b836",
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = []\n",
    "for file in os.listdir(graph_dir):\n",
    "    d = os.path.join(graph_dir, file)\n",
    "    if os.path.isdir(d):\n",
    "        dirs.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45064ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_6',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_2',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_8',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_4',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_5',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_1',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_7',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_3',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9',\n",
       " '/scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_0']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b15eec7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10:16:40] INFO: DataLoader will load 810 graphs (out of 900 available).\u001b[0m\n",
      "\u001b[36m[10:16:40] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_0/data21025_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_8/data21999_s0.pt\u001b[0m\n",
      "\u001b[32m[10:16:40] INFO: DataLoader will load 90 graphs (out of 900 available).\u001b[0m\n",
      "\u001b[36m[10:16:40] DEBUG: First graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9/data21009_s0.pt, last graph is /scratch/gpfs/IOJALVO/gnn-tracking/object_condensation/point_clouds_v5/part1_pt1/part_1_9/data21986_s0.pt\u001b[0m\n",
      "\u001b[36m[10:16:40] DEBUG: Parameters for data loader 'train': {'batch_size': 4, 'num_workers': 1, 'sampler': <torch.utils.data.sampler.RandomSampler object at 0x1545690650c0>, 'pin_memory': True, 'shuffle': None}\u001b[0m\n",
      "\u001b[36m[10:16:40] DEBUG: Parameters for data loader 'val': {'batch_size': 1, 'num_workers': 1, 'sampler': None, 'pin_memory': True, 'shuffle': False}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    \"train\": TrackingDataset(dirs, stop=810),\n",
    "    \"val\": TrackingDataset(dirs, start=810, stop=900),\n",
    "}\n",
    "loaders = get_loaders(datasets, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a18da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = {}\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key, value in dictionary.items():\n",
    "            dict_of_lists.setdefault(key, []).append(value)\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c922b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(weight_attractive, weight_repulsive, loaders, num_epochs=50):\n",
    "\n",
    "    loss_functions = {\n",
    "        \"embedding_loss\": (GraphConstructionHingeEmbeddingLoss(), {\"attractive\": weight_attractive,\n",
    "                                                                   \"repulsive\": weight_repulsive})\n",
    "    }\n",
    "\n",
    "    model = GraphConstructionFCNN(\n",
    "        in_dim = 14,\n",
    "        hidden_dim = 64,\n",
    "        out_dim = 10,\n",
    "        depth = 5,\n",
    "        beta = 0.4\n",
    "    )\n",
    "\n",
    "    trainer = TCNTrainer(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        loss_functions=loss_functions,\n",
    "        lr=7e-4,\n",
    "    )\n",
    "\n",
    "    loss_history = trainer.train(epochs=num_epochs)\n",
    "    return list_of_dicts_to_dict_of_lists(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9d19e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[10:21:41 TCNTrainer] INFO: Using device cuda\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:21:41 TCNTrainer] DEBUG: Epoch 1 (    0/203): Total=   0.02519, embedding_loss_attractive=   0.02493, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:41 TCNTrainer] DEBUG: Epoch 1 (   10/203): Total=   0.01018, embedding_loss_attractive=   0.00980, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:41 TCNTrainer] DEBUG: Epoch 1 (   20/203): Total=   0.00610, embedding_loss_attractive=   0.00551, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:41 TCNTrainer] DEBUG: Epoch 1 (   30/203): Total=   0.00498, embedding_loss_attractive=   0.00432, embedding_loss_repulsive=   0.00066 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:41 TCNTrainer] DEBUG: Epoch 1 (   40/203): Total=   0.00378, embedding_loss_attractive=   0.00330, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (   50/203): Total=   0.00346, embedding_loss_attractive=   0.00287, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (   60/203): Total=   0.00314, embedding_loss_attractive=   0.00254, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (   70/203): Total=   0.00299, embedding_loss_attractive=   0.00246, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (   80/203): Total=   0.00280, embedding_loss_attractive=   0.00215, embedding_loss_repulsive=   0.00065 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (   90/203): Total=   0.00267, embedding_loss_attractive=   0.00210, embedding_loss_repulsive=   0.00057 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:42 TCNTrainer] DEBUG: Epoch 1 (  100/203): Total=   0.00257, embedding_loss_attractive=   0.00197, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  110/203): Total=   0.00252, embedding_loss_attractive=   0.00187, embedding_loss_repulsive=   0.00065 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  120/203): Total=   0.00250, embedding_loss_attractive=   0.00191, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  130/203): Total=   0.00240, embedding_loss_attractive=   0.00179, embedding_loss_repulsive=   0.00062 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  140/203): Total=   0.00239, embedding_loss_attractive=   0.00169, embedding_loss_repulsive=   0.00071 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  150/203): Total=   0.00242, embedding_loss_attractive=   0.00177, embedding_loss_repulsive=   0.00065 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:43 TCNTrainer] DEBUG: Epoch 1 (  160/203): Total=   0.00239, embedding_loss_attractive=   0.00173, embedding_loss_repulsive=   0.00066 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:44 TCNTrainer] DEBUG: Epoch 1 (  170/203): Total=   0.00219, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00061 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:44 TCNTrainer] DEBUG: Epoch 1 (  180/203): Total=   0.00223, embedding_loss_attractive=   0.00160, embedding_loss_repulsive=   0.00063 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:44 TCNTrainer] DEBUG: Epoch 1 (  190/203): Total=   0.00223, embedding_loss_attractive=   0.00162, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:44 TCNTrainer] DEBUG: Epoch 1 (  200/203): Total=   0.00228, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00066 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:21:45 TCNTrainer] INFO: Results 1: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8646595058962703                    │ nan       │\n",
      "│    │ _time_train                              │ 3.3792140618897974                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001629815937485546                  │   0.00008 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.003163867756228876                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001629815937485546                  │   0.00008 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.003163867756228876                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002952299520580305                  │   0.00047 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0029157165379442383                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000590459904116061                  │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005831433075888477                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.002220275858417153                  │   0.00012 │\n",
      "│    │ total_train                              │ 0.0037470110841187204                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:21:45 TCNTrainer] DEBUG: Epoch 2 (    0/203): Total=   0.00221, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:45 TCNTrainer] DEBUG: Epoch 2 (   10/203): Total=   0.00224, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00058 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:45 TCNTrainer] DEBUG: Epoch 2 (   20/203): Total=   0.00224, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00066 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:45 TCNTrainer] DEBUG: Epoch 2 (   30/203): Total=   0.00221, embedding_loss_attractive=   0.00161, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   40/203): Total=   0.00221, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00069 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   50/203): Total=   0.00216, embedding_loss_attractive=   0.00166, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   60/203): Total=   0.00216, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   70/203): Total=   0.00216, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   80/203): Total=   0.00207, embedding_loss_attractive=   0.00156, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:46 TCNTrainer] DEBUG: Epoch 2 (   90/203): Total=   0.00208, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00058 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  100/203): Total=   0.00200, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  110/203): Total=   0.00211, embedding_loss_attractive=   0.00147, embedding_loss_repulsive=   0.00063 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  120/203): Total=   0.00207, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  130/203): Total=   0.00211, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00058 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  140/203): Total=   0.00215, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00057 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:47 TCNTrainer] DEBUG: Epoch 2 (  150/203): Total=   0.00207, embedding_loss_attractive=   0.00157, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:48 TCNTrainer] DEBUG: Epoch 2 (  160/203): Total=   0.00208, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:48 TCNTrainer] DEBUG: Epoch 2 (  170/203): Total=   0.00209, embedding_loss_attractive=   0.00158, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:48 TCNTrainer] DEBUG: Epoch 2 (  180/203): Total=   0.00221, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00069 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:48 TCNTrainer] DEBUG: Epoch 2 (  190/203): Total=   0.00213, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00067 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:48 TCNTrainer] DEBUG: Epoch 2 (  200/203): Total=   0.00197, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:21:49 TCNTrainer] INFO: Results 2: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8511955668218434                    │ nan       │\n",
      "│    │ _time_train                              │ 3.3267893618904054                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0014709857848679854                 │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001539859340383942                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0014709857848679854                 │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001539859340383942                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002819352206360135                  │   0.00045 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0030275734400679473                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000563870441272027                  │   0.00009 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0006055146880135895                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.002034856232866231                  │   0.00011 │\n",
      "│    │ total_train                              │ 0.002145374043537258                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:21:49 TCNTrainer] DEBUG: Epoch 3 (    0/203): Total=   0.00213, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00067 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:49 TCNTrainer] DEBUG: Epoch 3 (   10/203): Total=   0.00205, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00062 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   20/203): Total=   0.00209, embedding_loss_attractive=   0.00154, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   30/203): Total=   0.00202, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   40/203): Total=   0.00198, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   50/203): Total=   0.00194, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   60/203): Total=   0.00189, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   70/203): Total=   0.00198, embedding_loss_attractive=   0.00151, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:50 TCNTrainer] DEBUG: Epoch 3 (   80/203): Total=   0.00203, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00064 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (   90/203): Total=   0.00199, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (  100/203): Total=   0.00206, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (  110/203): Total=   0.00205, embedding_loss_attractive=   0.00153, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (  120/203): Total=   0.00212, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00064 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (  130/203): Total=   0.00195, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00058 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:51 TCNTrainer] DEBUG: Epoch 3 (  140/203): Total=   0.00206, embedding_loss_attractive=   0.00148, embedding_loss_repulsive=   0.00058 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  150/203): Total=   0.00194, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  160/203): Total=   0.00197, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  170/203): Total=   0.00206, embedding_loss_attractive=   0.00152, embedding_loss_repulsive=   0.00054 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  180/203): Total=   0.00190, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00056 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  190/203): Total=   0.00197, embedding_loss_attractive=   0.00145, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:52 TCNTrainer] DEBUG: Epoch 3 (  200/203): Total=   0.00196, embedding_loss_attractive=   0.00144, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:21:53 TCNTrainer] INFO: Results 3: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8508802349679172                    │ nan       │\n",
      "│    │ _time_train                              │ 3.352289890870452                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001388190211986916                  │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0014419378414566588                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001388190211986916                  │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0014419378414566588                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002635260381632381                  │   0.00042 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002795977274809271                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0005270520763264761                 │   0.00008 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005591954549618542                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0019152423004723258                 │   0.00010 │\n",
      "│    │ total_train                              │ 0.0020011333081173927                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:21:53 TCNTrainer] DEBUG: Epoch 4 (    0/203): Total=   0.00189, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   10/203): Total=   0.00199, embedding_loss_attractive=   0.00139, embedding_loss_repulsive=   0.00060 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   20/203): Total=   0.00197, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00056 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   30/203): Total=   0.00198, embedding_loss_attractive=   0.00146, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   40/203): Total=   0.00190, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   50/203): Total=   0.00193, embedding_loss_attractive=   0.00140, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:54 TCNTrainer] DEBUG: Epoch 4 (   60/203): Total=   0.00196, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00059 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (   70/203): Total=   0.00191, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (   80/203): Total=   0.00189, embedding_loss_attractive=   0.00142, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (   90/203): Total=   0.00184, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (  100/203): Total=   0.00185, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00057 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (  110/203): Total=   0.00196, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (  120/203): Total=   0.00180, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00054 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:55 TCNTrainer] DEBUG: Epoch 4 (  130/203): Total=   0.00189, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  140/203): Total=   0.00178, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  150/203): Total=   0.00180, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  160/203): Total=   0.00184, embedding_loss_attractive=   0.00135, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  170/203): Total=   0.00180, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  180/203): Total=   0.00184, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:56 TCNTrainer] DEBUG: Epoch 4 (  190/203): Total=   0.00186, embedding_loss_attractive=   0.00141, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:57 TCNTrainer] DEBUG: Epoch 4 (  200/203): Total=   0.00186, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:21:57 TCNTrainer] INFO: Results 4: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.84511864092201                      │ nan       │\n",
      "│    │ _time_train                              │ 3.283792283385992                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001369749949986322                  │   0.00007 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0013574082880358905                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001369749949986322                  │   0.00007 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0013574082880358905                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002396951668844041                  │   0.00037 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0026210648199231387                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00047939033376880817                │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0005242129639846277                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001849140296690166                  │   0.00010 │\n",
      "│    │ total_train                              │ 0.0018816212630312285                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (    0/203): Total=   0.00189, embedding_loss_attractive=   0.00138, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   10/203): Total=   0.00186, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   20/203): Total=   0.00171, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   30/203): Total=   0.00176, embedding_loss_attractive=   0.00132, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   40/203): Total=   0.00185, embedding_loss_attractive=   0.00133, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   50/203): Total=   0.00174, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:58 TCNTrainer] DEBUG: Epoch 5 (   60/203): Total=   0.00180, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (   70/203): Total=   0.00177, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00054 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (   80/203): Total=   0.00178, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (   90/203): Total=   0.00175, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (  100/203): Total=   0.00167, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (  110/203): Total=   0.00178, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:21:59 TCNTrainer] DEBUG: Epoch 5 (  120/203): Total=   0.00175, embedding_loss_attractive=   0.00127, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  130/203): Total=   0.00171, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  140/203): Total=   0.00169, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  150/203): Total=   0.00179, embedding_loss_attractive=   0.00137, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  160/203): Total=   0.00179, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00062 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  170/203): Total=   0.00178, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:00 TCNTrainer] DEBUG: Epoch 5 (  180/203): Total=   0.00173, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00056 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:01 TCNTrainer] DEBUG: Epoch 5 (  190/203): Total=   0.00164, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:01 TCNTrainer] DEBUG: Epoch 5 (  200/203): Total=   0.00183, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:02 TCNTrainer] INFO: Results 5: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8477370701730251                    │ nan       │\n",
      "│    │ _time_train                              │ 3.3015796607360244                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0012348681364932822                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001271243527728899                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0012348681364932822                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001271243527728899                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0022426369160206783                 │   0.00036 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0024922702167862153                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004485273832041356                 │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004984540433572431                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0016833955318563514                 │   0.00009 │\n",
      "│    │ total_train                              │ 0.0017696975775090564                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (    0/203): Total=   0.00176, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (   10/203): Total=   0.00194, embedding_loss_attractive=   0.00143, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (   20/203): Total=   0.00173, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (   30/203): Total=   0.00181, embedding_loss_attractive=   0.00134, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (   40/203): Total=   0.00173, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:02 TCNTrainer] DEBUG: Epoch 6 (   50/203): Total=   0.00167, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (   60/203): Total=   0.00166, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (   70/203): Total=   0.00173, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (   80/203): Total=   0.00169, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (   90/203): Total=   0.00171, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (  100/203): Total=   0.00165, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:03 TCNTrainer] DEBUG: Epoch 6 (  110/203): Total=   0.00169, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  120/203): Total=   0.00173, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  130/203): Total=   0.00169, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  140/203): Total=   0.00168, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  150/203): Total=   0.00182, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00061 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  160/203): Total=   0.00175, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00055 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:04 TCNTrainer] DEBUG: Epoch 6 (  170/203): Total=   0.00166, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:05 TCNTrainer] DEBUG: Epoch 6 (  180/203): Total=   0.00174, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:05 TCNTrainer] DEBUG: Epoch 6 (  190/203): Total=   0.00170, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:05 TCNTrainer] DEBUG: Epoch 6 (  200/203): Total=   0.00166, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:06 TCNTrainer] INFO: Results 6: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8493850138038397                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2967081032693386                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0012044336620925201                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001229440780205129                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0012044336620925201                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001229440780205129                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002202199584442294                  │   0.00035 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002383129962541053                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004404399168884589                 │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00047662599250821054                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0016448735919160147                 │   0.00009 │\n",
      "│    │ total_train                              │ 0.0017060667810860674                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:06 TCNTrainer] DEBUG: Epoch 7 (    0/203): Total=   0.00166, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:06 TCNTrainer] DEBUG: Epoch 7 (   10/203): Total=   0.00157, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:06 TCNTrainer] DEBUG: Epoch 7 (   20/203): Total=   0.00161, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:06 TCNTrainer] DEBUG: Epoch 7 (   30/203): Total=   0.00164, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:06 TCNTrainer] DEBUG: Epoch 7 (   40/203): Total=   0.00171, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (   50/203): Total=   0.00168, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (   60/203): Total=   0.00159, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (   70/203): Total=   0.00159, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (   80/203): Total=   0.00169, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (   90/203): Total=   0.00162, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:07 TCNTrainer] DEBUG: Epoch 7 (  100/203): Total=   0.00167, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  110/203): Total=   0.00162, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  120/203): Total=   0.00176, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  130/203): Total=   0.00177, embedding_loss_attractive=   0.00125, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  140/203): Total=   0.00178, embedding_loss_attractive=   0.00129, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  150/203): Total=   0.00168, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  160/203): Total=   0.00177, embedding_loss_attractive=   0.00131, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:08 TCNTrainer] DEBUG: Epoch 7 (  170/203): Total=   0.00170, embedding_loss_attractive=   0.00124, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:09 TCNTrainer] DEBUG: Epoch 7 (  180/203): Total=   0.00167, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:09 TCNTrainer] DEBUG: Epoch 7 (  190/203): Total=   0.00161, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:09 TCNTrainer] DEBUG: Epoch 7 (  200/203): Total=   0.00162, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:10 TCNTrainer] INFO: Results 7: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8498937203548849                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2638305607251823                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0011512475734990503                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0011899820408494822                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0011512475734990503                 │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0011899820408494822                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0022075519224421846                 │   0.00035 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0023397380460897835                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0004415103844884369                 │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00046794760921795676                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0015927579771313402                 │   0.00009 │\n",
      "│    │ total_train                              │ 0.0016579296619957083                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:10 TCNTrainer] DEBUG: Epoch 8 (    0/203): Total=   0.00166, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:10 TCNTrainer] DEBUG: Epoch 8 (   10/203): Total=   0.00167, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:10 TCNTrainer] DEBUG: Epoch 8 (   20/203): Total=   0.00171, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:10 TCNTrainer] DEBUG: Epoch 8 (   30/203): Total=   0.00161, embedding_loss_attractive=   0.00121, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   40/203): Total=   0.00176, embedding_loss_attractive=   0.00123, embedding_loss_repulsive=   0.00053 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   50/203): Total=   0.00159, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   60/203): Total=   0.00160, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   70/203): Total=   0.00155, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   80/203): Total=   0.00159, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (   90/203): Total=   0.00160, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00050 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:11 TCNTrainer] DEBUG: Epoch 8 (  100/203): Total=   0.00164, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  110/203): Total=   0.00154, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  120/203): Total=   0.00159, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  130/203): Total=   0.00173, embedding_loss_attractive=   0.00119, embedding_loss_repulsive=   0.00054 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  140/203): Total=   0.00160, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  150/203): Total=   0.00153, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:12 TCNTrainer] DEBUG: Epoch 8 (  160/203): Total=   0.00158, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:13 TCNTrainer] DEBUG: Epoch 8 (  170/203): Total=   0.00153, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:13 TCNTrainer] DEBUG: Epoch 8 (  180/203): Total=   0.00156, embedding_loss_attractive=   0.00120, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:13 TCNTrainer] DEBUG: Epoch 8 (  190/203): Total=   0.00175, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:13 TCNTrainer] DEBUG: Epoch 8 (  200/203): Total=   0.00166, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:14 TCNTrainer] INFO: Results 8: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8478085547685623                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2573726759292185                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001172798540857103                  │   0.00006 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001164174579047217                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001172798540857103                  │   0.00006 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001164174579047217                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.002178361039194796                  │   0.00034 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0022405262565093455                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00043567220783895916                │   0.00007 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004481052513018691                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0016084707613723973                 │   0.00009 │\n",
      "│    │ total_train                              │ 0.0016122798434243047                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:14 TCNTrainer] DEBUG: Epoch 9 (    0/203): Total=   0.00167, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:14 TCNTrainer] DEBUG: Epoch 9 (   10/203): Total=   0.00162, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:14 TCNTrainer] DEBUG: Epoch 9 (   20/203): Total=   0.00161, embedding_loss_attractive=   0.00116, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   30/203): Total=   0.00156, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   40/203): Total=   0.00155, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   50/203): Total=   0.00161, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   60/203): Total=   0.00157, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   70/203): Total=   0.00154, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   80/203): Total=   0.00159, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:15 TCNTrainer] DEBUG: Epoch 9 (   90/203): Total=   0.00169, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  100/203): Total=   0.00155, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  110/203): Total=   0.00163, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00054 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  120/203): Total=   0.00156, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  130/203): Total=   0.00165, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  140/203): Total=   0.00151, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:16 TCNTrainer] DEBUG: Epoch 9 (  150/203): Total=   0.00166, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00051 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:17 TCNTrainer] DEBUG: Epoch 9 (  160/203): Total=   0.00155, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:17 TCNTrainer] DEBUG: Epoch 9 (  170/203): Total=   0.00154, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:17 TCNTrainer] DEBUG: Epoch 9 (  180/203): Total=   0.00151, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:17 TCNTrainer] DEBUG: Epoch 9 (  190/203): Total=   0.00157, embedding_loss_attractive=   0.00115, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:17 TCNTrainer] DEBUG: Epoch 9 (  200/203): Total=   0.00154, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:18 TCNTrainer] INFO: Results 9: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8512633410282433                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2616171748377383                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0011261379863652919                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001128073654990882                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0011261379863652919                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001128073654990882                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001953484778965099                  │   0.00030 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0022313001317894226                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00039069695579301976                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00044626002635788454                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0015168349569042523                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.001574333691786002                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:18 TCNTrainer] DEBUG: Epoch 10 (    0/203): Total=   0.00154, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:18 TCNTrainer] DEBUG: Epoch 10 (   10/203): Total=   0.00144, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:18 TCNTrainer] DEBUG: Epoch 10 (   20/203): Total=   0.00170, embedding_loss_attractive=   0.00122, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   30/203): Total=   0.00166, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   40/203): Total=   0.00159, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   50/203): Total=   0.00159, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   60/203): Total=   0.00152, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   70/203): Total=   0.00154, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:19 TCNTrainer] DEBUG: Epoch 10 (   80/203): Total=   0.00150, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (   90/203): Total=   0.00155, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  100/203): Total=   0.00149, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  110/203): Total=   0.00148, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  120/203): Total=   0.00158, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00049 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  130/203): Total=   0.00170, embedding_loss_attractive=   0.00126, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  140/203): Total=   0.00170, embedding_loss_attractive=   0.00128, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:20 TCNTrainer] DEBUG: Epoch 10 (  150/203): Total=   0.00160, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:21 TCNTrainer] DEBUG: Epoch 10 (  160/203): Total=   0.00155, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:21 TCNTrainer] DEBUG: Epoch 10 (  170/203): Total=   0.00164, embedding_loss_attractive=   0.00118, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:21 TCNTrainer] DEBUG: Epoch 10 (  180/203): Total=   0.00155, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:21 TCNTrainer] DEBUG: Epoch 10 (  190/203): Total=   0.00162, embedding_loss_attractive=   0.00117, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:21 TCNTrainer] DEBUG: Epoch 10 (  200/203): Total=   0.00153, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:22 TCNTrainer] INFO: Results 10: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8509694188833237                    │ nan       │\n",
      "│    │ _time_train                              │ 3.3036146303638816                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001075936953485426                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0011291267428308579                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001075936953485426                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0011291267428308579                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0020025028808352847                 │   0.00031 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0021592203557509655                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00040050057616705697                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00043184407115019307                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0014764375389657086                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0015609708237301174                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:22 TCNTrainer] DEBUG: Epoch 11 (    0/203): Total=   0.00149, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:22 TCNTrainer] DEBUG: Epoch 11 (   10/203): Total=   0.00158, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00048 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   20/203): Total=   0.00156, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   30/203): Total=   0.00147, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   40/203): Total=   0.00157, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00052 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   50/203): Total=   0.00146, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   60/203): Total=   0.00143, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:23 TCNTrainer] DEBUG: Epoch 11 (   70/203): Total=   0.00147, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (   80/203): Total=   0.00153, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (   90/203): Total=   0.00154, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (  100/203): Total=   0.00152, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (  110/203): Total=   0.00156, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (  120/203): Total=   0.00145, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (  130/203): Total=   0.00151, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:24 TCNTrainer] DEBUG: Epoch 11 (  140/203): Total=   0.00143, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  150/203): Total=   0.00149, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  160/203): Total=   0.00142, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  170/203): Total=   0.00143, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  180/203): Total=   0.00142, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  190/203): Total=   0.00144, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:25 TCNTrainer] DEBUG: Epoch 11 (  200/203): Total=   0.00150, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:26 TCNTrainer] INFO: Results 11: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8543097213841975                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2460437449626625                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001079009241786682                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.001064252082529143                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001079009241786682                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.001064252082529143                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001997383968490693                  │   0.00031 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002071944825245719                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003994767936981387                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00041438896504914386                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0014784860515242649                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0014786410536571162                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:26 TCNTrainer] DEBUG: Epoch 12 (    0/203): Total=   0.00148, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   10/203): Total=   0.00156, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   20/203): Total=   0.00152, embedding_loss_attractive=   0.00114, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   30/203): Total=   0.00154, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   40/203): Total=   0.00147, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   50/203): Total=   0.00153, embedding_loss_attractive=   0.00113, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:27 TCNTrainer] DEBUG: Epoch 12 (   60/203): Total=   0.00147, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (   70/203): Total=   0.00148, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (   80/203): Total=   0.00152, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (   90/203): Total=   0.00154, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (  100/203): Total=   0.00142, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (  110/203): Total=   0.00146, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (  120/203): Total=   0.00144, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:28 TCNTrainer] DEBUG: Epoch 12 (  130/203): Total=   0.00138, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  140/203): Total=   0.00144, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  150/203): Total=   0.00146, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  160/203): Total=   0.00139, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  170/203): Total=   0.00147, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  180/203): Total=   0.00143, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:29 TCNTrainer] DEBUG: Epoch 12 (  190/203): Total=   0.00138, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:30 TCNTrainer] DEBUG: Epoch 12 (  200/203): Total=   0.00148, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:30 TCNTrainer] INFO: Results 12: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8498507891781628                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2635337729007006                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.001012247539539304                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0010625219478368245                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.001012247539539304                  │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0010625219478368245                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0019377914231477511                 │   0.00030 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.002047420103157976                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003875582846295502                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004094840206315953                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013998058327059778                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0014720059784468758                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (    0/203): Total=   0.00147, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00047 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   10/203): Total=   0.00138, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   20/203): Total=   0.00147, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   30/203): Total=   0.00142, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   40/203): Total=   0.00148, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   50/203): Total=   0.00149, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:31 TCNTrainer] DEBUG: Epoch 13 (   60/203): Total=   0.00153, embedding_loss_attractive=   0.00109, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (   70/203): Total=   0.00143, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (   80/203): Total=   0.00153, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (   90/203): Total=   0.00149, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (  100/203): Total=   0.00144, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (  110/203): Total=   0.00140, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:32 TCNTrainer] DEBUG: Epoch 13 (  120/203): Total=   0.00139, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  130/203): Total=   0.00147, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  140/203): Total=   0.00144, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  150/203): Total=   0.00146, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  160/203): Total=   0.00142, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  170/203): Total=   0.00144, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  180/203): Total=   0.00140, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:33 TCNTrainer] DEBUG: Epoch 13 (  190/203): Total=   0.00138, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:34 TCNTrainer] DEBUG: Epoch 13 (  200/203): Total=   0.00145, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:35 TCNTrainer] INFO: Results 13: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8467623088508844                    │ nan       │\n",
      "│    │ _time_train                              │ 3.243289112113416                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.00101918119406845                   │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0010393731851759253                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.00101918119406845                   │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0010393731851759253                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0019225598144758907                 │   0.00030 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0020197687425141412                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00038451196289517823                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0004039537485028282                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0014036931718389193                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0014433269452055906                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (    0/203): Total=   0.00141, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (   10/203): Total=   0.00142, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (   20/203): Total=   0.00144, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (   30/203): Total=   0.00144, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (   40/203): Total=   0.00135, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:35 TCNTrainer] DEBUG: Epoch 14 (   50/203): Total=   0.00141, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (   60/203): Total=   0.00146, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (   70/203): Total=   0.00144, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (   80/203): Total=   0.00144, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (   90/203): Total=   0.00149, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (  100/203): Total=   0.00137, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (  110/203): Total=   0.00148, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:36 TCNTrainer] DEBUG: Epoch 14 (  120/203): Total=   0.00134, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  130/203): Total=   0.00144, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  140/203): Total=   0.00144, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  150/203): Total=   0.00143, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  160/203): Total=   0.00143, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00045 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  170/203): Total=   0.00145, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:37 TCNTrainer] DEBUG: Epoch 14 (  180/203): Total=   0.00135, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:38 TCNTrainer] DEBUG: Epoch 14 (  190/203): Total=   0.00145, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:38 TCNTrainer] DEBUG: Epoch 14 (  200/203): Total=   0.00151, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:39 TCNTrainer] INFO: Results 14: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8449258720502257                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2450568107888103                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0010440282756462693                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0010186724793142983                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0010440282756462693                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0010186724793142983                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0018308542526534034                 │   0.00028 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019992620923918823                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003661708505306807                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00039985241847837643                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001410199137818482                  │   0.00008 │\n",
      "│    │ total_train                              │ 0.0014185249092048171                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (    0/203): Total=   0.00139, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (   10/203): Total=   0.00150, embedding_loss_attractive=   0.00110, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (   20/203): Total=   0.00144, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (   30/203): Total=   0.00138, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (   40/203): Total=   0.00139, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:39 TCNTrainer] DEBUG: Epoch 15 (   50/203): Total=   0.00137, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (   60/203): Total=   0.00139, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (   70/203): Total=   0.00135, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (   80/203): Total=   0.00135, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (   90/203): Total=   0.00145, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (  100/203): Total=   0.00150, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:40 TCNTrainer] DEBUG: Epoch 15 (  110/203): Total=   0.00145, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  120/203): Total=   0.00140, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  130/203): Total=   0.00137, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  140/203): Total=   0.00149, embedding_loss_attractive=   0.00111, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  150/203): Total=   0.00141, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  160/203): Total=   0.00139, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  170/203): Total=   0.00142, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:41 TCNTrainer] DEBUG: Epoch 15 (  180/203): Total=   0.00144, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:42 TCNTrainer] DEBUG: Epoch 15 (  190/203): Total=   0.00137, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:42 TCNTrainer] DEBUG: Epoch 15 (  200/203): Total=   0.00131, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:43 TCNTrainer] INFO: Results 15: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8481574291363358                    │ nan       │\n",
      "│    │ _time_train                              │ 3.230495990719646                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009736855456139893                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0010198971561702162                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009736855456139893                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0010198971561702162                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0018964280219127734                 │   0.00029 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001973351778512995                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003792856043825547                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00039467035570259895                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013529711596978208                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0014145675194426784                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:43 TCNTrainer] DEBUG: Epoch 16 (    0/203): Total=   0.00139, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:43 TCNTrainer] DEBUG: Epoch 16 (   10/203): Total=   0.00141, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:43 TCNTrainer] DEBUG: Epoch 16 (   20/203): Total=   0.00142, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:43 TCNTrainer] DEBUG: Epoch 16 (   30/203): Total=   0.00137, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:43 TCNTrainer] DEBUG: Epoch 16 (   40/203): Total=   0.00141, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (   50/203): Total=   0.00138, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (   60/203): Total=   0.00135, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (   70/203): Total=   0.00144, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (   80/203): Total=   0.00144, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (   90/203): Total=   0.00148, embedding_loss_attractive=   0.00108, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:44 TCNTrainer] DEBUG: Epoch 16 (  100/203): Total=   0.00139, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  110/203): Total=   0.00135, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  120/203): Total=   0.00135, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  130/203): Total=   0.00142, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  140/203): Total=   0.00147, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  150/203): Total=   0.00143, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  160/203): Total=   0.00147, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:45 TCNTrainer] DEBUG: Epoch 16 (  170/203): Total=   0.00138, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:46 TCNTrainer] DEBUG: Epoch 16 (  180/203): Total=   0.00146, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:46 TCNTrainer] DEBUG: Epoch 16 (  190/203): Total=   0.00137, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:46 TCNTrainer] DEBUG: Epoch 16 (  200/203): Total=   0.00135, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:47 TCNTrainer] INFO: Results 16: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.850104364566505                     │ nan       │\n",
      "│    │ _time_train                              │ 3.249231720343232                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009922382589947019                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0010081788498032013                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009922382589947019                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0010081788498032013                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0018609196089932488                 │   0.00029 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001959464443374994                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00037218392179864983                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003918928886749988                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013644221989024016                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0014000717485140037                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:47 TCNTrainer] DEBUG: Epoch 17 (    0/203): Total=   0.00136, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:47 TCNTrainer] DEBUG: Epoch 17 (   10/203): Total=   0.00132, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:47 TCNTrainer] DEBUG: Epoch 17 (   20/203): Total=   0.00134, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:47 TCNTrainer] DEBUG: Epoch 17 (   30/203): Total=   0.00136, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   40/203): Total=   0.00141, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   50/203): Total=   0.00134, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   60/203): Total=   0.00142, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   70/203): Total=   0.00131, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   80/203): Total=   0.00136, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (   90/203): Total=   0.00135, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:48 TCNTrainer] DEBUG: Epoch 17 (  100/203): Total=   0.00134, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  110/203): Total=   0.00134, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  120/203): Total=   0.00134, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  130/203): Total=   0.00138, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  140/203): Total=   0.00138, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  150/203): Total=   0.00142, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:49 TCNTrainer] DEBUG: Epoch 17 (  160/203): Total=   0.00140, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:50 TCNTrainer] DEBUG: Epoch 17 (  170/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:50 TCNTrainer] DEBUG: Epoch 17 (  180/203): Total=   0.00142, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:50 TCNTrainer] DEBUG: Epoch 17 (  190/203): Total=   0.00137, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:50 TCNTrainer] DEBUG: Epoch 17 (  200/203): Total=   0.00139, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:51 TCNTrainer] INFO: Results 17: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8489378849044442                    │ nan       │\n",
      "│    │ _time_train                              │ 3.23164194310084                      │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0010079990994806092                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009829204129812128                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0010079990994806092                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009829204129812128                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0019358634780575004                 │   0.00030 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019663239019496247                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003871726956115                    │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00039326478038992493                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013951718031118313                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013761852020306026                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:51 TCNTrainer] DEBUG: Epoch 18 (    0/203): Total=   0.00139, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:51 TCNTrainer] DEBUG: Epoch 18 (   10/203): Total=   0.00136, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:51 TCNTrainer] DEBUG: Epoch 18 (   20/203): Total=   0.00138, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   30/203): Total=   0.00131, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   40/203): Total=   0.00140, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   50/203): Total=   0.00135, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   60/203): Total=   0.00131, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   70/203): Total=   0.00142, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   80/203): Total=   0.00147, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:52 TCNTrainer] DEBUG: Epoch 18 (   90/203): Total=   0.00139, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  100/203): Total=   0.00140, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  110/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  120/203): Total=   0.00138, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  130/203): Total=   0.00134, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  140/203): Total=   0.00138, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  150/203): Total=   0.00136, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:53 TCNTrainer] DEBUG: Epoch 18 (  160/203): Total=   0.00137, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:54 TCNTrainer] DEBUG: Epoch 18 (  170/203): Total=   0.00134, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:54 TCNTrainer] DEBUG: Epoch 18 (  180/203): Total=   0.00150, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:54 TCNTrainer] DEBUG: Epoch 18 (  190/203): Total=   0.00143, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:54 TCNTrainer] DEBUG: Epoch 18 (  200/203): Total=   0.00143, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:55 TCNTrainer] INFO: Results 18: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8473031362518668                    │ nan       │\n",
      "│    │ _time_train                              │ 3.289375213906169                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0010598162227931122                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009904208308053684                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0010598162227931122                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009904208308053684                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0020958096254616978                 │   0.00032 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019551891285302164                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00041916192509233946                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003910378257060433                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0014789781584921811                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0013814586664325204                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:55 TCNTrainer] DEBUG: Epoch 19 (    0/203): Total=   0.00149, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:55 TCNTrainer] DEBUG: Epoch 19 (   10/203): Total=   0.00152, embedding_loss_attractive=   0.00112, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:55 TCNTrainer] DEBUG: Epoch 19 (   20/203): Total=   0.00141, embedding_loss_attractive=   0.00106, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   30/203): Total=   0.00136, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   40/203): Total=   0.00136, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   50/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   60/203): Total=   0.00138, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   70/203): Total=   0.00135, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:56 TCNTrainer] DEBUG: Epoch 19 (   80/203): Total=   0.00138, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (   90/203): Total=   0.00132, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  100/203): Total=   0.00138, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  110/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  120/203): Total=   0.00141, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  130/203): Total=   0.00135, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  140/203): Total=   0.00132, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:57 TCNTrainer] DEBUG: Epoch 19 (  150/203): Total=   0.00141, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:58 TCNTrainer] DEBUG: Epoch 19 (  160/203): Total=   0.00139, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:58 TCNTrainer] DEBUG: Epoch 19 (  170/203): Total=   0.00134, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:58 TCNTrainer] DEBUG: Epoch 19 (  180/203): Total=   0.00131, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:58 TCNTrainer] DEBUG: Epoch 19 (  190/203): Total=   0.00136, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:58 TCNTrainer] DEBUG: Epoch 19 (  200/203): Total=   0.00135, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:22:59 TCNTrainer] INFO: Results 19: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8554359562695026                    │ nan       │\n",
      "│    │ _time_train                              │ 3.236105934716761                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009698148325292601                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009810787073928368                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009698148325292601                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009810787073928368                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0018076873720727033                 │   0.00028 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019460162004510978                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003615374744145406                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003892032400902196                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013313523092721071                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013702819588378512                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:22:59 TCNTrainer] DEBUG: Epoch 20 (    0/203): Total=   0.00142, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:22:59 TCNTrainer] DEBUG: Epoch 20 (   10/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   20/203): Total=   0.00135, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   30/203): Total=   0.00131, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   40/203): Total=   0.00137, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   50/203): Total=   0.00145, embedding_loss_attractive=   0.00104, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   60/203): Total=   0.00151, embedding_loss_attractive=   0.00107, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   70/203): Total=   0.00144, embedding_loss_attractive=   0.00105, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:00 TCNTrainer] DEBUG: Epoch 20 (   80/203): Total=   0.00134, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (   90/203): Total=   0.00137, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (  100/203): Total=   0.00136, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (  110/203): Total=   0.00135, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (  120/203): Total=   0.00130, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (  130/203): Total=   0.00131, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:01 TCNTrainer] DEBUG: Epoch 20 (  140/203): Total=   0.00143, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  150/203): Total=   0.00135, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  160/203): Total=   0.00138, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  170/203): Total=   0.00133, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  180/203): Total=   0.00135, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  190/203): Total=   0.00132, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:02 TCNTrainer] DEBUG: Epoch 20 (  200/203): Total=   0.00145, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:03 TCNTrainer] INFO: Results 20: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8484939662739635                    │ nan       │\n",
      "│    │ _time_train                              │ 3.266979993786663                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009932292489490161                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009776389457427626                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009932292489490161                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009776389457427626                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001813811437589013                  │   0.00028 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001925598193427979                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00036276228751780257                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003851196386855959                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013559915422875848                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.001362758593431908                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:03 TCNTrainer] DEBUG: Epoch 21 (    0/203): Total=   0.00143, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:03 TCNTrainer] DEBUG: Epoch 21 (   10/203): Total=   0.00128, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   20/203): Total=   0.00138, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   30/203): Total=   0.00134, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   40/203): Total=   0.00131, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   50/203): Total=   0.00129, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   60/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:04 TCNTrainer] DEBUG: Epoch 21 (   70/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (   80/203): Total=   0.00133, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (   90/203): Total=   0.00134, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (  100/203): Total=   0.00138, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (  110/203): Total=   0.00134, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (  120/203): Total=   0.00131, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (  130/203): Total=   0.00134, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:05 TCNTrainer] DEBUG: Epoch 21 (  140/203): Total=   0.00146, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00046 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  150/203): Total=   0.00140, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  160/203): Total=   0.00135, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  170/203): Total=   0.00130, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  180/203): Total=   0.00140, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  190/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:06 TCNTrainer] DEBUG: Epoch 21 (  200/203): Total=   0.00140, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:07 TCNTrainer] INFO: Results 21: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8490764191374183                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2448674510233104                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009957726313991266                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009758625712286993                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009957726313991266                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009758625712286993                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001795755287942787                  │   0.00027 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019211315382455516                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003591510575885574                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003842263076491104                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013549237033455737                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013600888890283082                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:07 TCNTrainer] DEBUG: Epoch 22 (    0/203): Total=   0.00137, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   10/203): Total=   0.00134, embedding_loss_attractive=   0.00102, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   20/203): Total=   0.00138, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   30/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   40/203): Total=   0.00129, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   50/203): Total=   0.00133, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:08 TCNTrainer] DEBUG: Epoch 22 (   60/203): Total=   0.00129, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (   70/203): Total=   0.00136, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (   80/203): Total=   0.00137, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (   90/203): Total=   0.00134, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (  100/203): Total=   0.00143, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (  110/203): Total=   0.00129, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (  120/203): Total=   0.00129, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:09 TCNTrainer] DEBUG: Epoch 22 (  130/203): Total=   0.00135, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  140/203): Total=   0.00131, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  150/203): Total=   0.00133, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  160/203): Total=   0.00135, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  170/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  180/203): Total=   0.00134, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:10 TCNTrainer] DEBUG: Epoch 22 (  190/203): Total=   0.00126, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:11 TCNTrainer] DEBUG: Epoch 22 (  200/203): Total=   0.00137, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:11 TCNTrainer] INFO: Results 22: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.84905328694731                      │ nan       │\n",
      "│    │ _time_train                              │ 3.2450058297254145                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009582202771626826                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009622060610073173                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009582202771626826                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009622060610073173                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001885255965559433                  │   0.00029 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001918239512691834                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003770511931118866                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003836479025383668                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013352714724735254                 │   0.00008 │\n",
      "│    │ total_train                              │ 0.0013458539646926332                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (    0/203): Total=   0.00137, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   10/203): Total=   0.00131, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   20/203): Total=   0.00142, embedding_loss_attractive=   0.00100, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   30/203): Total=   0.00134, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   40/203): Total=   0.00138, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   50/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:12 TCNTrainer] DEBUG: Epoch 23 (   60/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (   70/203): Total=   0.00127, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (   80/203): Total=   0.00131, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (   90/203): Total=   0.00140, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (  100/203): Total=   0.00135, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (  110/203): Total=   0.00133, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:13 TCNTrainer] DEBUG: Epoch 23 (  120/203): Total=   0.00129, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  130/203): Total=   0.00136, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  140/203): Total=   0.00131, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  150/203): Total=   0.00133, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  160/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  170/203): Total=   0.00131, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  180/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:14 TCNTrainer] DEBUG: Epoch 23 (  190/203): Total=   0.00132, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:15 TCNTrainer] DEBUG: Epoch 23 (  200/203): Total=   0.00142, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:16 TCNTrainer] INFO: Results 23: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8400203161872923                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2554747867397964                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009418489405005757                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009438827343679634                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009418489405005757                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009438827343679634                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001839856217460086                  │   0.00028 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0019192311917362835                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00036797124349201716                │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003838462383472567                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001309820196022176                  │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013277289782205753                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (    0/203): Total=   0.00135, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (   10/203): Total=   0.00136, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (   20/203): Total=   0.00127, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (   30/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (   40/203): Total=   0.00125, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:16 TCNTrainer] DEBUG: Epoch 24 (   50/203): Total=   0.00131, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (   60/203): Total=   0.00135, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (   70/203): Total=   0.00132, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (   80/203): Total=   0.00131, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (   90/203): Total=   0.00131, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (  100/203): Total=   0.00135, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (  110/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:17 TCNTrainer] DEBUG: Epoch 24 (  120/203): Total=   0.00129, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  130/203): Total=   0.00134, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  140/203): Total=   0.00130, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  150/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  160/203): Total=   0.00141, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  170/203): Total=   0.00135, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:18 TCNTrainer] DEBUG: Epoch 24 (  180/203): Total=   0.00138, embedding_loss_attractive=   0.00103, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:19 TCNTrainer] DEBUG: Epoch 24 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:19 TCNTrainer] DEBUG: Epoch 24 (  200/203): Total=   0.00134, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:20 TCNTrainer] INFO: Results 24: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8543114489875734                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2537012780085206                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009229396120645106                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009418046262500615                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009229396120645106                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009418046262500615                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017705639539700416                 │   0.00027 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018910099098486412                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003541127907940083                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003782019819697283                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012770524080325332                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.001320006612578196                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:20 TCNTrainer] DEBUG: Epoch 25 (    0/203): Total=   0.00136, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:20 TCNTrainer] DEBUG: Epoch 25 (   10/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:20 TCNTrainer] DEBUG: Epoch 25 (   20/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:20 TCNTrainer] DEBUG: Epoch 25 (   30/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:20 TCNTrainer] DEBUG: Epoch 25 (   40/203): Total=   0.00132, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (   50/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (   60/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (   70/203): Total=   0.00130, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (   80/203): Total=   0.00135, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (   90/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (  100/203): Total=   0.00134, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:21 TCNTrainer] DEBUG: Epoch 25 (  110/203): Total=   0.00129, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  120/203): Total=   0.00135, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  130/203): Total=   0.00136, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  140/203): Total=   0.00126, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  150/203): Total=   0.00125, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  160/203): Total=   0.00133, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:22 TCNTrainer] DEBUG: Epoch 25 (  170/203): Total=   0.00128, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:23 TCNTrainer] DEBUG: Epoch 25 (  180/203): Total=   0.00131, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:23 TCNTrainer] DEBUG: Epoch 25 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:23 TCNTrainer] DEBUG: Epoch 25 (  200/203): Total=   0.00133, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:24 TCNTrainer] INFO: Results 25: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8449444910511374                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2763100587762892                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009428600159784158                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009354231764420206                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009428600159784158                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009354231764420206                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0018619366947354543                 │   0.00028 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018818432223189421                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003723873389470908                 │   0.00006 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00037636864446378846                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0013152473606169225                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013117918227409273                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:24 TCNTrainer] DEBUG: Epoch 26 (    0/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:24 TCNTrainer] DEBUG: Epoch 26 (   10/203): Total=   0.00130, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:24 TCNTrainer] DEBUG: Epoch 26 (   20/203): Total=   0.00132, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:24 TCNTrainer] DEBUG: Epoch 26 (   30/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:24 TCNTrainer] DEBUG: Epoch 26 (   40/203): Total=   0.00122, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (   50/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (   60/203): Total=   0.00127, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (   70/203): Total=   0.00129, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (   80/203): Total=   0.00133, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (   90/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:25 TCNTrainer] DEBUG: Epoch 26 (  100/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  110/203): Total=   0.00130, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  120/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  130/203): Total=   0.00130, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  140/203): Total=   0.00125, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  150/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  160/203): Total=   0.00129, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:26 TCNTrainer] DEBUG: Epoch 26 (  170/203): Total=   0.00129, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:27 TCNTrainer] DEBUG: Epoch 26 (  180/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:27 TCNTrainer] DEBUG: Epoch 26 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:27 TCNTrainer] DEBUG: Epoch 26 (  200/203): Total=   0.00139, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:28 TCNTrainer] INFO: Results 26: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8532030279748142                    │ nan       │\n",
      "│    │ _time_train                              │ 3.239121742080897                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009505102288029674                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009283286954042286                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009505102288029674                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009283286954042286                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017627087434650295                 │   0.00027 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018670267787913384                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003525417486930059                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003734053557582677                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001303051986421148                  │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013017340590764442                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:28 TCNTrainer] DEBUG: Epoch 27 (    0/203): Total=   0.00134, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:28 TCNTrainer] DEBUG: Epoch 27 (   10/203): Total=   0.00125, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:28 TCNTrainer] DEBUG: Epoch 27 (   20/203): Total=   0.00132, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:28 TCNTrainer] DEBUG: Epoch 27 (   30/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   40/203): Total=   0.00133, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   50/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   60/203): Total=   0.00129, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   70/203): Total=   0.00129, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   80/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (   90/203): Total=   0.00134, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:29 TCNTrainer] DEBUG: Epoch 27 (  100/203): Total=   0.00136, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  110/203): Total=   0.00128, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  120/203): Total=   0.00128, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  130/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  140/203): Total=   0.00143, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  150/203): Total=   0.00132, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:30 TCNTrainer] DEBUG: Epoch 27 (  160/203): Total=   0.00125, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:31 TCNTrainer] DEBUG: Epoch 27 (  170/203): Total=   0.00131, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:31 TCNTrainer] DEBUG: Epoch 27 (  180/203): Total=   0.00134, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:31 TCNTrainer] DEBUG: Epoch 27 (  190/203): Total=   0.00135, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:31 TCNTrainer] DEBUG: Epoch 27 (  200/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:32 TCNTrainer] INFO: Results 27: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8569420780986547                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2285537398420274                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008987791202444997                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000934458465035546                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008987791202444997                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000934458465035546                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017365103160652021                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018559818481797068                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00034730206321304045                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00037119636963594134                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012460811891489558                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0013056548399474526                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:32 TCNTrainer] DEBUG: Epoch 28 (    0/203): Total=   0.00124, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:32 TCNTrainer] DEBUG: Epoch 28 (   10/203): Total=   0.00132, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:32 TCNTrainer] DEBUG: Epoch 28 (   20/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   30/203): Total=   0.00136, embedding_loss_attractive=   0.00099, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   40/203): Total=   0.00131, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   50/203): Total=   0.00125, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   60/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   70/203): Total=   0.00130, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   80/203): Total=   0.00127, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:33 TCNTrainer] DEBUG: Epoch 28 (   90/203): Total=   0.00126, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  100/203): Total=   0.00128, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  110/203): Total=   0.00132, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  120/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  130/203): Total=   0.00126, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  140/203): Total=   0.00134, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  150/203): Total=   0.00123, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:34 TCNTrainer] DEBUG: Epoch 28 (  160/203): Total=   0.00125, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:35 TCNTrainer] DEBUG: Epoch 28 (  170/203): Total=   0.00125, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:35 TCNTrainer] DEBUG: Epoch 28 (  180/203): Total=   0.00130, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:35 TCNTrainer] DEBUG: Epoch 28 (  190/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:35 TCNTrainer] DEBUG: Epoch 28 (  200/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:36 TCNTrainer] INFO: Results 28: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8534055282361805                    │ nan       │\n",
      "│    │ _time_train                              │ 3.238688820041716                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009063882145306303                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009147067072003015                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009063882145306303                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009147067072003015                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017142459090488651                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018528456893735772                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003428491818097731                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00037056913787471545                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012492374090167383                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012852758561430746                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:36 TCNTrainer] DEBUG: Epoch 29 (    0/203): Total=   0.00130, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:36 TCNTrainer] DEBUG: Epoch 29 (   10/203): Total=   0.00128, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:36 TCNTrainer] DEBUG: Epoch 29 (   20/203): Total=   0.00129, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   30/203): Total=   0.00130, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   40/203): Total=   0.00124, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   50/203): Total=   0.00129, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   60/203): Total=   0.00122, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   70/203): Total=   0.00131, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:37 TCNTrainer] DEBUG: Epoch 29 (   80/203): Total=   0.00126, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (   90/203): Total=   0.00125, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  100/203): Total=   0.00121, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  110/203): Total=   0.00133, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00044 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  120/203): Total=   0.00127, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  130/203): Total=   0.00128, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  140/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:38 TCNTrainer] DEBUG: Epoch 29 (  150/203): Total=   0.00127, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:39 TCNTrainer] DEBUG: Epoch 29 (  160/203): Total=   0.00126, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:39 TCNTrainer] DEBUG: Epoch 29 (  170/203): Total=   0.00125, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:39 TCNTrainer] DEBUG: Epoch 29 (  180/203): Total=   0.00129, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:39 TCNTrainer] DEBUG: Epoch 29 (  190/203): Total=   0.00124, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:39 TCNTrainer] DEBUG: Epoch 29 (  200/203): Total=   0.00128, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:40 TCNTrainer] INFO: Results 29: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8525403831154108                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2392997769638896                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008975981831705819                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009066432002206962                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008975981831705819                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009066432002206962                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017094304086640478                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018434026410454527                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00034188608173280955                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003686805282090906                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012394842652914426                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012753237356555652                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:40 TCNTrainer] DEBUG: Epoch 30 (    0/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:40 TCNTrainer] DEBUG: Epoch 30 (   10/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   20/203): Total=   0.00135, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   30/203): Total=   0.00133, embedding_loss_attractive=   0.00096, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   40/203): Total=   0.00124, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   50/203): Total=   0.00126, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   60/203): Total=   0.00123, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   70/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:41 TCNTrainer] DEBUG: Epoch 30 (   80/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (   90/203): Total=   0.00126, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (  100/203): Total=   0.00127, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (  110/203): Total=   0.00132, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (  120/203): Total=   0.00132, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (  130/203): Total=   0.00123, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:42 TCNTrainer] DEBUG: Epoch 30 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  150/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  160/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  170/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  180/203): Total=   0.00136, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  190/203): Total=   0.00126, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:43 TCNTrainer] DEBUG: Epoch 30 (  200/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:44 TCNTrainer] INFO: Results 30: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8497180743142962                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2233407609164715                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008784024174221688                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0009015361257149778                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008784024174221688                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0009015361257149778                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017264428393294414                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018329938805978655                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003452885678658883                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003665987761195731                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012236909902033706                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012681349077413382                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:44 TCNTrainer] DEBUG: Epoch 31 (    0/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:44 TCNTrainer] DEBUG: Epoch 31 (   10/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   20/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   30/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   40/203): Total=   0.00130, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   50/203): Total=   0.00123, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   60/203): Total=   0.00127, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:45 TCNTrainer] DEBUG: Epoch 31 (   70/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (   80/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (   90/203): Total=   0.00126, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (  100/203): Total=   0.00130, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (  110/203): Total=   0.00129, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (  120/203): Total=   0.00121, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (  130/203): Total=   0.00127, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:46 TCNTrainer] DEBUG: Epoch 31 (  140/203): Total=   0.00125, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  150/203): Total=   0.00131, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  160/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  180/203): Total=   0.00129, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  190/203): Total=   0.00132, embedding_loss_attractive=   0.00095, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:47 TCNTrainer] DEBUG: Epoch 31 (  200/203): Total=   0.00137, embedding_loss_attractive=   0.00101, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:48 TCNTrainer] INFO: Results 31: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8436187319457531                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2378449449315667                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009262582649373346                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008974079994067293                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009262582649373346                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008974079994067293                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016256497034596072                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018287389372874583                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032512994069192145                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036574778745749166                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001251388215718584                  │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012631557923122286                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:48 TCNTrainer] DEBUG: Epoch 32 (    0/203): Total=   0.00127, embedding_loss_attractive=   0.00097, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   10/203): Total=   0.00130, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   20/203): Total=   0.00127, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   30/203): Total=   0.00126, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   40/203): Total=   0.00124, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   50/203): Total=   0.00132, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   60/203): Total=   0.00129, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:49 TCNTrainer] DEBUG: Epoch 32 (   70/203): Total=   0.00128, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (   80/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (  100/203): Total=   0.00126, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (  110/203): Total=   0.00132, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (  120/203): Total=   0.00126, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:50 TCNTrainer] DEBUG: Epoch 32 (  130/203): Total=   0.00128, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  140/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  150/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  160/203): Total=   0.00124, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  170/203): Total=   0.00121, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  180/203): Total=   0.00128, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:51 TCNTrainer] DEBUG: Epoch 32 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:52 TCNTrainer] DEBUG: Epoch 32 (  200/203): Total=   0.00132, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:52 TCNTrainer] INFO: Results 32: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.84453439200297                      │ nan       │\n",
      "│    │ _time_train                              │ 3.2885472462512553                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0009055631629760481                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008892148768694457                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0009055631629760481                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008892148768694457                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016645904953798486                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001819354372461478                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00033291809907596973                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036387087449229555                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012384812670966816                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012530857557201474                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (    0/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   10/203): Total=   0.00126, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   20/203): Total=   0.00125, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   30/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   50/203): Total=   0.00122, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:53 TCNTrainer] DEBUG: Epoch 33 (   60/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (   70/203): Total=   0.00129, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (   80/203): Total=   0.00119, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (   90/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (  100/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (  110/203): Total=   0.00130, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:54 TCNTrainer] DEBUG: Epoch 33 (  120/203): Total=   0.00122, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  130/203): Total=   0.00128, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  140/203): Total=   0.00129, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  150/203): Total=   0.00123, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  160/203): Total=   0.00123, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  170/203): Total=   0.00127, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  180/203): Total=   0.00126, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:55 TCNTrainer] DEBUG: Epoch 33 (  190/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:56 TCNTrainer] DEBUG: Epoch 33 (  200/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:23:57 TCNTrainer] INFO: Results 33: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8495637653395534                    │ nan       │\n",
      "│    │ _time_train                              │ 3.239348517730832                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008618765497683651                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008813374772727526                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008618765497683651                 │   0.00005 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008813374772727526                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016783520243027145                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018070613623601315                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00033567040486054287                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036141227247202625                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011975469585094186                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012427497559956509                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (    0/203): Total=   0.00124, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (   10/203): Total=   0.00128, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (   20/203): Total=   0.00132, embedding_loss_attractive=   0.00098, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (   30/203): Total=   0.00124, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:57 TCNTrainer] DEBUG: Epoch 34 (   50/203): Total=   0.00127, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (   60/203): Total=   0.00126, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (   70/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (   90/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (  100/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (  110/203): Total=   0.00127, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:58 TCNTrainer] DEBUG: Epoch 34 (  120/203): Total=   0.00129, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  130/203): Total=   0.00132, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  150/203): Total=   0.00131, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  160/203): Total=   0.00128, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  170/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:23:59 TCNTrainer] DEBUG: Epoch 34 (  180/203): Total=   0.00127, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:00 TCNTrainer] DEBUG: Epoch 34 (  190/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:00 TCNTrainer] DEBUG: Epoch 34 (  200/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:01 TCNTrainer] INFO: Results 34: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8528676577843726                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2242607790976763                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008533726530408279                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008773681199581826                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008533726530408279                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008773681199581826                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017286692422607706                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018025381402273115                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003457338484521541                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036050762804546235                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011991065086072517                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012378757515591867                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (    0/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (   10/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (   20/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (   30/203): Total=   0.00116, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (   40/203): Total=   0.00124, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:01 TCNTrainer] DEBUG: Epoch 35 (   50/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (   60/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (   70/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (   80/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (   90/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (  100/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:24:02 TCNTrainer] DEBUG: Epoch 35 (  110/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  120/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  130/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  140/203): Total=   0.00124, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  150/203): Total=   0.00129, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  160/203): Total=   0.00130, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  170/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:03 TCNTrainer] DEBUG: Epoch 35 (  180/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:04 TCNTrainer] DEBUG: Epoch 35 (  190/203): Total=   0.00126, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:04 TCNTrainer] DEBUG: Epoch 35 (  200/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:05 TCNTrainer] INFO: Results 35: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8480199952609837                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2371489675715566                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008606278227590438                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008760824259898162                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008606278227590438                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008760824259898162                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017750326324150795                 │   0.00027 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018061145640028814                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00035500652648301587                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00036122291280057635                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012156343617890444                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012373053508907043                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:05 TCNTrainer] DEBUG: Epoch 36 (    0/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:05 TCNTrainer] DEBUG: Epoch 36 (   10/203): Total=   0.00121, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:05 TCNTrainer] DEBUG: Epoch 36 (   20/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:05 TCNTrainer] DEBUG: Epoch 36 (   30/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:05 TCNTrainer] DEBUG: Epoch 36 (   40/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (   50/203): Total=   0.00115, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (   60/203): Total=   0.00119, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (   70/203): Total=   0.00125, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (   90/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (  100/203): Total=   0.00114, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:06 TCNTrainer] DEBUG: Epoch 36 (  110/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  120/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  130/203): Total=   0.00121, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  140/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  150/203): Total=   0.00122, embedding_loss_attractive=   0.00092, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  160/203): Total=   0.00123, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:07 TCNTrainer] DEBUG: Epoch 36 (  170/203): Total=   0.00121, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:08 TCNTrainer] DEBUG: Epoch 36 (  180/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:08 TCNTrainer] DEBUG: Epoch 36 (  190/203): Total=   0.00125, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:08 TCNTrainer] DEBUG: Epoch 36 (  200/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:09 TCNTrainer] INFO: Results 36: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8449525539763272                    │ nan       │\n",
      "│    │ _time_train                              │ 3.235302582848817                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008491223574512535                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008623826835336613                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008491223574512535                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008623826835336613                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016970911037383807                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017988776101093061                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003394182207476762                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003597755220218612                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011885405815620388                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012221582121504791                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:09 TCNTrainer] DEBUG: Epoch 37 (    0/203): Total=   0.00114, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:09 TCNTrainer] DEBUG: Epoch 37 (   10/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:09 TCNTrainer] DEBUG: Epoch 37 (   20/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:09 TCNTrainer] DEBUG: Epoch 37 (   30/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:09 TCNTrainer] DEBUG: Epoch 37 (   40/203): Total=   0.00126, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (   50/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (   60/203): Total=   0.00120, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (   70/203): Total=   0.00124, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (   90/203): Total=   0.00128, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:10 TCNTrainer] DEBUG: Epoch 37 (  100/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  110/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  120/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  130/203): Total=   0.00124, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  150/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  160/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:11 TCNTrainer] DEBUG: Epoch 37 (  170/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:12 TCNTrainer] DEBUG: Epoch 37 (  180/203): Total=   0.00122, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:12 TCNTrainer] DEBUG: Epoch 37 (  190/203): Total=   0.00122, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:12 TCNTrainer] DEBUG: Epoch 37 (  200/203): Total=   0.00122, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:13 TCNTrainer] INFO: Results 37: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8604811332188547                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2426980966702104                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008489757220053838                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008621724497921434                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008489757220053838                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008621724497921434                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016766527706446746                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001785412578146189                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003353305541289349                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003570825156292378                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011843062833779387                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012192549728765496                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:13 TCNTrainer] DEBUG: Epoch 38 (    0/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:13 TCNTrainer] DEBUG: Epoch 38 (   10/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:13 TCNTrainer] DEBUG: Epoch 38 (   20/203): Total=   0.00124, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:13 TCNTrainer] DEBUG: Epoch 38 (   30/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   50/203): Total=   0.00133, embedding_loss_attractive=   0.00094, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   60/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   70/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   80/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:14 TCNTrainer] DEBUG: Epoch 38 (  100/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  110/203): Total=   0.00119, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  120/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  130/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  140/203): Total=   0.00125, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  150/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:15 TCNTrainer] DEBUG: Epoch 38 (  160/203): Total=   0.00123, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:16 TCNTrainer] DEBUG: Epoch 38 (  170/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:16 TCNTrainer] DEBUG: Epoch 38 (  180/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:16 TCNTrainer] DEBUG: Epoch 38 (  190/203): Total=   0.00127, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:16 TCNTrainer] DEBUG: Epoch 38 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:17 TCNTrainer] INFO: Results 38: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8526850338093936                    │ nan       │\n",
      "│    │ _time_train                              │ 3.224396290257573                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008586504710062097                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008618158054322445                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008586504710062097                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008618158054322445                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016276563815255131                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0018005629048311138                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003255312763051027                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003601125809662227                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011841817539081805                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012219283951152794                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:17 TCNTrainer] DEBUG: Epoch 39 (    0/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:17 TCNTrainer] DEBUG: Epoch 39 (   10/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:17 TCNTrainer] DEBUG: Epoch 39 (   20/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:17 TCNTrainer] DEBUG: Epoch 39 (   30/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   40/203): Total=   0.00123, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   50/203): Total=   0.00124, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   60/203): Total=   0.00116, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   70/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:18 TCNTrainer] DEBUG: Epoch 39 (   90/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  100/203): Total=   0.00119, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  110/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  120/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  130/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  140/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  150/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:19 TCNTrainer] DEBUG: Epoch 39 (  160/203): Total=   0.00126, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:20 TCNTrainer] DEBUG: Epoch 39 (  170/203): Total=   0.00118, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:20 TCNTrainer] DEBUG: Epoch 39 (  180/203): Total=   0.00113, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:20 TCNTrainer] DEBUG: Epoch 39 (  190/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:20 TCNTrainer] DEBUG: Epoch 39 (  200/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:21 TCNTrainer] INFO: Results 39: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.842239891178906                     │ nan       │\n",
      "│    │ _time_train                              │ 3.241429553832859                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008810229644748486                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008588621672128185                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008810229644748486                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008588621672128185                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017349237382101516                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017865674250204062                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00034698474764203037                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003573134850040812                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012280077194898494                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.001216175661449839                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:21 TCNTrainer] DEBUG: Epoch 40 (    0/203): Total=   0.00122, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:21 TCNTrainer] DEBUG: Epoch 40 (   10/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:21 TCNTrainer] DEBUG: Epoch 40 (   20/203): Total=   0.00124, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   30/203): Total=   0.00119, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   50/203): Total=   0.00120, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   60/203): Total=   0.00119, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   70/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:22 TCNTrainer] DEBUG: Epoch 40 (   80/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (   90/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  100/203): Total=   0.00119, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  110/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  120/203): Total=   0.00122, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  130/203): Total=   0.00127, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  140/203): Total=   0.00121, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:23 TCNTrainer] DEBUG: Epoch 40 (  150/203): Total=   0.00119, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:24 TCNTrainer] DEBUG: Epoch 40 (  160/203): Total=   0.00118, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:24 TCNTrainer] DEBUG: Epoch 40 (  170/203): Total=   0.00125, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:24 TCNTrainer] DEBUG: Epoch 40 (  180/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:24 TCNTrainer] DEBUG: Epoch 40 (  190/203): Total=   0.00128, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:24 TCNTrainer] DEBUG: Epoch 40 (  200/203): Total=   0.00123, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:25 TCNTrainer] INFO: Results 40: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8450567601248622                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2560907737351954                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008417505402273188                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008536578661285047                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008417505402273188                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008536578661285047                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001646252561153637                  │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017826672588369529                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032925051223072746                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003565334517673906                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011710010572440095                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012101913249496317                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:25 TCNTrainer] DEBUG: Epoch 41 (    0/203): Total=   0.00116, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:25 TCNTrainer] DEBUG: Epoch 41 (   10/203): Total=   0.00124, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   20/203): Total=   0.00121, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   30/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   40/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   50/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   60/203): Total=   0.00125, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   70/203): Total=   0.00120, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:26 TCNTrainer] DEBUG: Epoch 41 (   80/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (   90/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (  100/203): Total=   0.00128, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (  110/203): Total=   0.00125, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (  120/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (  130/203): Total=   0.00120, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:27 TCNTrainer] DEBUG: Epoch 41 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  150/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  160/203): Total=   0.00118, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  170/203): Total=   0.00128, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  190/203): Total=   0.00130, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:28 TCNTrainer] DEBUG: Epoch 41 (  200/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:29 TCNTrainer] INFO: Results 41: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8447997770272195                    │ nan       │\n",
      "│    │ _time_train                              │ 3.219585587736219                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000843373715179041                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000852041312647075                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000843373715179041                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000852041312647075                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016411512326966558                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017801784231267833                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032823024653933115                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003560356846253567                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011716039670217368                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0012080769990502026                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:29 TCNTrainer] DEBUG: Epoch 42 (    0/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:29 TCNTrainer] DEBUG: Epoch 42 (   10/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   20/203): Total=   0.00123, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   30/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   40/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   50/203): Total=   0.00125, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   60/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:30 TCNTrainer] DEBUG: Epoch 42 (   70/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (   80/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (   90/203): Total=   0.00123, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (  110/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (  120/203): Total=   0.00127, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:31 TCNTrainer] DEBUG: Epoch 42 (  140/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  150/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  170/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  190/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:32 TCNTrainer] DEBUG: Epoch 42 (  200/203): Total=   0.00123, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:33 TCNTrainer] INFO: Results 42: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8471942637115717                    │ nan       │\n",
      "│    │ _time_train                              │ 3.223011178895831                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008813381505509217                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008463291264955744                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008813381505509217                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008463291264955744                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017027295479137036                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017673682806820705                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00034054590958274076                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003534736561364141                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0012218840691881874                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011998027909473686                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:33 TCNTrainer] DEBUG: Epoch 43 (    0/203): Total=   0.00125, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   10/203): Total=   0.00127, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   20/203): Total=   0.00118, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   30/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   40/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   50/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   60/203): Total=   0.00116, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:34 TCNTrainer] DEBUG: Epoch 43 (   70/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (   80/203): Total=   0.00117, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (   90/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (  100/203): Total=   0.00124, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (  110/203): Total=   0.00122, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (  120/203): Total=   0.00125, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:35 TCNTrainer] DEBUG: Epoch 43 (  130/203): Total=   0.00121, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  140/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  150/203): Total=   0.00126, embedding_loss_attractive=   0.00090, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  160/203): Total=   0.00124, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  170/203): Total=   0.00123, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  180/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  190/203): Total=   0.00123, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:36 TCNTrainer] DEBUG: Epoch 43 (  200/203): Total=   0.00125, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:37 TCNTrainer] INFO: Results 43: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8488316489383578                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2425715480931103                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008594147628173232                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008447932018072625                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008594147628173232                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008447932018072625                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016167787524561086                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017732848198809133                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003233557504912218                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003546569639761827                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001182770516930355                  │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011994501729518763                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:37 TCNTrainer] DEBUG: Epoch 44 (    0/203): Total=   0.00122, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   10/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   20/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   30/203): Total=   0.00120, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   40/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   50/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:38 TCNTrainer] DEBUG: Epoch 44 (   60/203): Total=   0.00123, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (   70/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (  100/203): Total=   0.00123, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (  110/203): Total=   0.00122, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (  120/203): Total=   0.00117, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:39 TCNTrainer] DEBUG: Epoch 44 (  130/203): Total=   0.00115, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  140/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  150/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  170/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:40 TCNTrainer] DEBUG: Epoch 44 (  190/203): Total=   0.00118, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:41 TCNTrainer] DEBUG: Epoch 44 (  200/203): Total=   0.00116, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:41 TCNTrainer] INFO: Results 44: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8477711067534983                    │ nan       │\n",
      "│    │ _time_train                              │ 3.249908677302301                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008399819211465203                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008411550366209609                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008399819211465203                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008411550366209609                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016539489126039875                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017664607509906437                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003307897825207975                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00035329215019812873                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001170771711298989                  │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011944471975430626                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (    0/203): Total=   0.00115, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   10/203): Total=   0.00120, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   20/203): Total=   0.00124, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   30/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   40/203): Total=   0.00123, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   50/203): Total=   0.00121, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:42 TCNTrainer] DEBUG: Epoch 45 (   60/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (   70/203): Total=   0.00124, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (   80/203): Total=   0.00125, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (   90/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (  100/203): Total=   0.00122, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (  110/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:43 TCNTrainer] DEBUG: Epoch 45 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  130/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  140/203): Total=   0.00124, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  150/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  160/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  180/203): Total=   0.00121, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00043 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:44 TCNTrainer] DEBUG: Epoch 45 (  190/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:45 TCNTrainer] DEBUG: Epoch 45 (  200/203): Total=   0.00121, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:46 TCNTrainer] INFO: Results 45: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8567755953408778                    │ nan       │\n",
      "│    │ _time_train                              │ 3.226636486593634                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008301429327629093                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008382867531979393                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008301429327629093                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008382867531979393                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001671346038993862                  │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017659381918581616                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003342692077987724                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003531876383716324                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011644121362931199                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011914743946663338                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (    0/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (   10/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (   20/203): Total=   0.00116, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (   30/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (   40/203): Total=   0.00116, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:46 TCNTrainer] DEBUG: Epoch 46 (   50/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (   70/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (   80/203): Total=   0.00116, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (   90/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (  100/203): Total=   0.00116, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (  110/203): Total=   0.00116, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:47 TCNTrainer] DEBUG: Epoch 46 (  120/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  130/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  140/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  150/203): Total=   0.00120, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  160/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  170/203): Total=   0.00124, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:48 TCNTrainer] DEBUG: Epoch 46 (  180/203): Total=   0.00123, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:49 TCNTrainer] DEBUG: Epoch 46 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:49 TCNTrainer] DEBUG: Epoch 46 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:50 TCNTrainer] INFO: Results 46: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.858355050906539                     │ nan       │\n",
      "│    │ _time_train                              │ 3.22438673209399                      │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008187820966769424                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008304959293248434                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008187820966769424                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008304959293248434                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001661280423609747                  │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017495568230371976                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003322560847219494                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034991136460743957                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011510381914882197                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011804073027064427                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (    0/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (   20/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (   30/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (   40/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:50 TCNTrainer] DEBUG: Epoch 47 (   50/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (   60/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (   70/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (   80/203): Total=   0.00121, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (  100/203): Total=   0.00120, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:24:51 TCNTrainer] DEBUG: Epoch 47 (  110/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  120/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  130/203): Total=   0.00123, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  160/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:52 TCNTrainer] DEBUG: Epoch 47 (  170/203): Total=   0.00108, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:53 TCNTrainer] DEBUG: Epoch 47 (  180/203): Total=   0.00120, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:53 TCNTrainer] DEBUG: Epoch 47 (  190/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:53 TCNTrainer] DEBUG: Epoch 47 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:54 TCNTrainer] INFO: Results 47: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8478402271866798                    │ nan       │\n",
      "│    │ _time_train                              │ 3.284481876064092                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008664878699669821                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008285870059310766                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008664878699669821                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008285870059310766                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016248458388468457                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017491691066823863                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032496916776936913                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034983382133647726                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011914570479550296                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011784208335184258                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:54 TCNTrainer] DEBUG: Epoch 48 (    0/203): Total=   0.00126, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:54 TCNTrainer] DEBUG: Epoch 48 (   10/203): Total=   0.00122, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:54 TCNTrainer] DEBUG: Epoch 48 (   20/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:54 TCNTrainer] DEBUG: Epoch 48 (   30/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:54 TCNTrainer] DEBUG: Epoch 48 (   40/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (   50/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (   70/203): Total=   0.00127, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (   80/203): Total=   0.00118, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (   90/203): Total=   0.00124, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:55 TCNTrainer] DEBUG: Epoch 48 (  100/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  110/203): Total=   0.00130, embedding_loss_attractive=   0.00093, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  120/203): Total=   0.00121, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  130/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  140/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  150/203): Total=   0.00121, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  160/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:56 TCNTrainer] DEBUG: Epoch 48 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:57 TCNTrainer] DEBUG: Epoch 48 (  180/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:57 TCNTrainer] DEBUG: Epoch 48 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:57 TCNTrainer] DEBUG: Epoch 48 (  200/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:24:58 TCNTrainer] INFO: Results 48: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8519707731902599                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2464033737778664                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008361834618780348                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008283186637397323                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008361834618780348                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008283186637397323                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016672703722077939                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017557616885797364                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003334540744415588                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00035115233771594725                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011696375363195936                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011794710121223052                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:24:58 TCNTrainer] DEBUG: Epoch 49 (    0/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:58 TCNTrainer] DEBUG: Epoch 49 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:58 TCNTrainer] DEBUG: Epoch 49 (   20/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:58 TCNTrainer] DEBUG: Epoch 49 (   30/203): Total=   0.00115, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   40/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   50/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   70/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   80/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (   90/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:24:59 TCNTrainer] DEBUG: Epoch 49 (  100/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  110/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  120/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  140/203): Total=   0.00123, embedding_loss_attractive=   0.00089, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  150/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:00 TCNTrainer] DEBUG: Epoch 49 (  160/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:01 TCNTrainer] DEBUG: Epoch 49 (  170/203): Total=   0.00120, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:01 TCNTrainer] DEBUG: Epoch 49 (  180/203): Total=   0.00119, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:01 TCNTrainer] DEBUG: Epoch 49 (  190/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:01 TCNTrainer] DEBUG: Epoch 49 (  200/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:02 TCNTrainer] INFO: Results 49: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8445969461463392                    │ nan       │\n",
      "│    │ _time_train                              │ 3.221076977904886                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008161801385641512                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008244140422329527                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008161801385641512                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008244140422329527                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016520587041870588                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001740616033991496                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003304117408374118                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003481232067982992                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011465918854810298                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011725372559129458                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:02 TCNTrainer] DEBUG: Epoch 50 (    0/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:02 TCNTrainer] DEBUG: Epoch 50 (   10/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:02 TCNTrainer] DEBUG: Epoch 50 (   20/203): Total=   0.00114, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:02 TCNTrainer] DEBUG: Epoch 50 (   30/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   40/203): Total=   0.00116, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   50/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:03 TCNTrainer] DEBUG: Epoch 50 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  100/203): Total=   0.00121, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  110/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  120/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  130/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  140/203): Total=   0.00125, embedding_loss_attractive=   0.00091, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  150/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:04 TCNTrainer] DEBUG: Epoch 50 (  160/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:05 TCNTrainer] DEBUG: Epoch 50 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:05 TCNTrainer] DEBUG: Epoch 50 (  180/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:05 TCNTrainer] DEBUG: Epoch 50 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:05 TCNTrainer] DEBUG: Epoch 50 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:06 TCNTrainer] INFO: Results 50: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8469536821357906                    │ nan       │\n",
      "│    │ _time_train                              │ 3.229041072074324                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008220917622869214                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008186174202779208                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008220917622869214                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008186174202779208                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001603126359016945                  │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017462657372297472                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032062527180338904                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003492531474459495                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011427170338316095                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011678705705338952                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:06 TCNTrainer] DEBUG: Epoch 51 (    0/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:06 TCNTrainer] DEBUG: Epoch 51 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:06 TCNTrainer] DEBUG: Epoch 51 (   20/203): Total=   0.00121, embedding_loss_attractive=   0.00087, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   30/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   40/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   50/203): Total=   0.00115, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   60/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   70/203): Total=   0.00123, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   80/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:07 TCNTrainer] DEBUG: Epoch 51 (   90/203): Total=   0.00110, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  100/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  110/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  120/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  130/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:08 TCNTrainer] DEBUG: Epoch 51 (  150/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:09 TCNTrainer] DEBUG: Epoch 51 (  160/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:09 TCNTrainer] DEBUG: Epoch 51 (  170/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:09 TCNTrainer] DEBUG: Epoch 51 (  180/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:09 TCNTrainer] DEBUG: Epoch 51 (  190/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:09 TCNTrainer] DEBUG: Epoch 51 (  200/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:10 TCNTrainer] INFO: Results 51: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8519643191248178                    │ nan       │\n",
      "│    │ _time_train                              │ 3.218656391836703                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000790219764975417                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000817990569647842                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000790219764975417                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000817990569647842                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0017422916936791605                 │   0.00026 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001731817662174552                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003484583387358321                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003463635324349104                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011386781141886281                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011643541149859284                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:10 TCNTrainer] DEBUG: Epoch 52 (    0/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:10 TCNTrainer] DEBUG: Epoch 52 (   10/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:10 TCNTrainer] DEBUG: Epoch 52 (   20/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   30/203): Total=   0.00118, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   40/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   50/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   60/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   70/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:11 TCNTrainer] DEBUG: Epoch 52 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  100/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  110/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  120/203): Total=   0.00121, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  130/203): Total=   0.00119, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  140/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:12 TCNTrainer] DEBUG: Epoch 52 (  150/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:13 TCNTrainer] DEBUG: Epoch 52 (  160/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:13 TCNTrainer] DEBUG: Epoch 52 (  170/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:13 TCNTrainer] DEBUG: Epoch 52 (  180/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:13 TCNTrainer] DEBUG: Epoch 52 (  190/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:13 TCNTrainer] DEBUG: Epoch 52 (  200/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:14 TCNTrainer] INFO: Results 52: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8462227559648454                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2334166201762855                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008293144010369563                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008080286536425389                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008293144010369563                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008080286536425389                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016181507018498248                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001728857608782724                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000323630140369965                  │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003457715217565448                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011529445454167822                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011538001794707525                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:14 TCNTrainer] DEBUG: Epoch 53 (    0/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:14 TCNTrainer] DEBUG: Epoch 53 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   20/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   30/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   40/203): Total=   0.00122, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   50/203): Total=   0.00121, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   60/203): Total=   0.00110, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   70/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:15 TCNTrainer] DEBUG: Epoch 53 (   80/203): Total=   0.00116, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (   90/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (  100/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (  110/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (  120/203): Total=   0.00123, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (  130/203): Total=   0.00119, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:16 TCNTrainer] DEBUG: Epoch 53 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  150/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  160/203): Total=   0.00112, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  170/203): Total=   0.00117, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  190/203): Total=   0.00115, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:17 TCNTrainer] DEBUG: Epoch 53 (  200/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:18 TCNTrainer] INFO: Results 53: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8435583938844502                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2312625348567963                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008049853889840759                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000814464309693227                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008049853889840759                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000814464309693227                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016578450566157698                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017356833413224942                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.000331569011323154                  │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003471366682644988                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011365544044464413                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011616009809397932                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:18 TCNTrainer] DEBUG: Epoch 54 (    0/203): Total=   0.00118, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:18 TCNTrainer] DEBUG: Epoch 54 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   20/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   30/203): Total=   0.00114, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   40/203): Total=   0.00115, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   50/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   60/203): Total=   0.00116, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:19 TCNTrainer] DEBUG: Epoch 54 (   70/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (   80/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (   90/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (  100/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (  110/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:20 TCNTrainer] DEBUG: Epoch 54 (  140/203): Total=   0.00119, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  150/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  180/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  190/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:21 TCNTrainer] DEBUG: Epoch 54 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:22 TCNTrainer] INFO: Results 54: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8466546265408397                    │ nan       │\n",
      "│    │ _time_train                              │ 3.288815202191472                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008026559921240227                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008044239446713492                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008026559921240227                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008044239446713492                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016020278899102575                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017201257038063236                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032040557798205155                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003440251407612647                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011230615724343806                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011484490964433242                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:22 TCNTrainer] DEBUG: Epoch 55 (    0/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   10/203): Total=   0.00122, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   30/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   40/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   50/203): Total=   0.00115, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:23 TCNTrainer] DEBUG: Epoch 55 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (   80/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (   90/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (  100/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (  110/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:24 TCNTrainer] DEBUG: Epoch 55 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  140/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  150/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  170/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:25 TCNTrainer] DEBUG: Epoch 55 (  190/203): Total=   0.00110, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:26 TCNTrainer] DEBUG: Epoch 55 (  200/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:26 TCNTrainer] INFO: Results 55: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.848926393315196                     │ nan       │\n",
      "│    │ _time_train                              │ 3.2184950918890536                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007996564905624837                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008016106818247531                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007996564905624837                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008016106818247531                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001620834558788273                  │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001714596452576847                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032416691175765465                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034291929051536937                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011238234085289554                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001144529976354444                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (    0/203): Total=   0.00112, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   10/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   30/203): Total=   0.00117, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   40/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:27 TCNTrainer] DEBUG: Epoch 56 (   60/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (   70/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (   80/203): Total=   0.00120, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (   90/203): Total=   0.00119, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (  100/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (  110/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:28 TCNTrainer] DEBUG: Epoch 56 (  120/203): Total=   0.00117, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  130/203): Total=   0.00122, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  140/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  170/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:29 TCNTrainer] DEBUG: Epoch 56 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:30 TCNTrainer] DEBUG: Epoch 56 (  200/203): Total=   0.00117, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:31 TCNTrainer] INFO: Results 56: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8528116582892835                    │ nan       │\n",
      "│    │ _time_train                              │ 3.206247844733298                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000790144380233768                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008059559123856681                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000790144380233768                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008059559123856681                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016023292012202243                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001726631667542964                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003204658402440449                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003453263335085928                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011106102229354697                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011512822565608865                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (    0/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (   10/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (   20/203): Total=   0.00121, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00042 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (   30/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (   40/203): Total=   0.00123, embedding_loss_attractive=   0.00088, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:31 TCNTrainer] DEBUG: Epoch 57 (   50/203): Total=   0.00118, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (   60/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (   70/203): Total=   0.00116, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (   80/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (  110/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:32 TCNTrainer] DEBUG: Epoch 57 (  120/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  130/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  140/203): Total=   0.00113, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  150/203): Total=   0.00113, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  160/203): Total=   0.00118, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:33 TCNTrainer] DEBUG: Epoch 57 (  180/203): Total=   0.00116, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:34 TCNTrainer] DEBUG: Epoch 57 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:34 TCNTrainer] DEBUG: Epoch 57 (  200/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:35 TCNTrainer] INFO: Results 57: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8493955838494003                    │ nan       │\n",
      "│    │ _time_train                              │ 3.234187156893313                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007795267072247548                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0008004022704258451                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007795267072247548                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0008004022704258451                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016043484469668733                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017266700266782432                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003208696893933747                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003453340053356487                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001100396403732399                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011457362845356536                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (    0/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (   10/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (   20/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (   30/203): Total=   0.00119, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (   40/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:35 TCNTrainer] DEBUG: Epoch 58 (   50/203): Total=   0.00110, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (   60/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (   70/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (   80/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (   90/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (  100/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:36 TCNTrainer] DEBUG: Epoch 58 (  110/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  130/203): Total=   0.00119, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  140/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  150/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  160/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  170/203): Total=   0.00118, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:37 TCNTrainer] DEBUG: Epoch 58 (  180/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:38 TCNTrainer] DEBUG: Epoch 58 (  190/203): Total=   0.00108, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:38 TCNTrainer] DEBUG: Epoch 58 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:39 TCNTrainer] INFO: Results 58: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8469316577538848                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2230512369424105                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008009314394762946                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007927562230555661                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008009314394762946                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007927562230555661                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016431021905090246                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017069557502755685                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032862043810180495                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034139115005511365                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001129551885727172                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011341473766662217                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:39 TCNTrainer] DEBUG: Epoch 59 (    0/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:39 TCNTrainer] DEBUG: Epoch 59 (   10/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:39 TCNTrainer] DEBUG: Epoch 59 (   20/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:39 TCNTrainer] DEBUG: Epoch 59 (   30/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:39 TCNTrainer] DEBUG: Epoch 59 (   40/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (   50/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (   60/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (   80/203): Total=   0.00111, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (  100/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:25:40 TCNTrainer] DEBUG: Epoch 59 (  110/203): Total=   0.00120, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  120/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  130/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  140/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  150/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  160/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:41 TCNTrainer] DEBUG: Epoch 59 (  170/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:42 TCNTrainer] DEBUG: Epoch 59 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:42 TCNTrainer] DEBUG: Epoch 59 (  190/203): Total=   0.00117, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:42 TCNTrainer] DEBUG: Epoch 59 (  200/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:43 TCNTrainer] INFO: Results 59: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8450618060305715                    │ nan       │\n",
      "│    │ _time_train                              │ 3.215797306969762                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008562846917710785                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007909523260669572                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008562846917710785                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007909523260669572                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015929878107272088                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001699955061564378                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003185975621454418                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033999101231287565                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011748822638764978                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011309433402722985                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:43 TCNTrainer] DEBUG: Epoch 60 (    0/203): Total=   0.00122, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:43 TCNTrainer] DEBUG: Epoch 60 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:43 TCNTrainer] DEBUG: Epoch 60 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:43 TCNTrainer] DEBUG: Epoch 60 (   30/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:43 TCNTrainer] DEBUG: Epoch 60 (   40/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (   50/203): Total=   0.00111, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (   60/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (   70/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:44 TCNTrainer] DEBUG: Epoch 60 (  100/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  110/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  120/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  130/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  140/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  150/203): Total=   0.00121, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  160/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:45 TCNTrainer] DEBUG: Epoch 60 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:46 TCNTrainer] DEBUG: Epoch 60 (  180/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:46 TCNTrainer] DEBUG: Epoch 60 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:46 TCNTrainer] DEBUG: Epoch 60 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:47 TCNTrainer] INFO: Results 60: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.849136346951127                     │ nan       │\n",
      "│    │ _time_train                              │ 3.2271450138650835                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008143675921019166                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007910180006524905                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008143675921019166                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007910180006524905                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016235634244771468                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017137803416011                    │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003247126848954294                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00034275606832022014                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011390802864399221                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001133774076313184                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:47 TCNTrainer] DEBUG: Epoch 61 (    0/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:47 TCNTrainer] DEBUG: Epoch 61 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:47 TCNTrainer] DEBUG: Epoch 61 (   20/203): Total=   0.00119, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:47 TCNTrainer] DEBUG: Epoch 61 (   30/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   40/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   50/203): Total=   0.00113, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   60/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   70/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (   90/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:48 TCNTrainer] DEBUG: Epoch 61 (  100/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  110/203): Total=   0.00115, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  130/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  140/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  150/203): Total=   0.00115, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:49 TCNTrainer] DEBUG: Epoch 61 (  160/203): Total=   0.00112, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:50 TCNTrainer] DEBUG: Epoch 61 (  170/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:50 TCNTrainer] DEBUG: Epoch 61 (  180/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:50 TCNTrainer] DEBUG: Epoch 61 (  190/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:50 TCNTrainer] DEBUG: Epoch 61 (  200/203): Total=   0.00120, embedding_loss_attractive=   0.00084, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:51 TCNTrainer] INFO: Results 61: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8524406710639596                    │ nan       │\n",
      "│    │ _time_train                              │ 3.231652387883514                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008027174775229974                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007881972379064912                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008027174775229974                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007881972379064912                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001559804456256744                  │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017087641980710068                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003119608912513487                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003417528396142013                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011146783764060173                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011299500847464711                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:51 TCNTrainer] DEBUG: Epoch 62 (    0/203): Total=   0.00116, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:51 TCNTrainer] DEBUG: Epoch 62 (   10/203): Total=   0.00109, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:51 TCNTrainer] DEBUG: Epoch 62 (   20/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:51 TCNTrainer] DEBUG: Epoch 62 (   30/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   40/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   50/203): Total=   0.00116, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   60/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   80/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:52 TCNTrainer] DEBUG: Epoch 62 (   90/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  100/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  120/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  130/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  140/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  150/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:53 TCNTrainer] DEBUG: Epoch 62 (  160/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:54 TCNTrainer] DEBUG: Epoch 62 (  170/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:54 TCNTrainer] DEBUG: Epoch 62 (  180/203): Total=   0.00108, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:54 TCNTrainer] DEBUG: Epoch 62 (  190/203): Total=   0.00121, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:54 TCNTrainer] DEBUG: Epoch 62 (  200/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:55 TCNTrainer] INFO: Results 62: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8523525358177722                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2558083962649107                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007814824141355025                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007893610163591802                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007814824141355025                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007893610163591802                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016606225606261029                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0017031985254765585                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003321245121252206                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003406397050953116                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011136069337630437                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011300007277053638                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:55 TCNTrainer] DEBUG: Epoch 63 (    0/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:55 TCNTrainer] DEBUG: Epoch 63 (   10/203): Total=   0.00119, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:55 TCNTrainer] DEBUG: Epoch 63 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   30/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   40/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   50/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   60/203): Total=   0.00112, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   70/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   80/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:56 TCNTrainer] DEBUG: Epoch 63 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  100/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  110/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  120/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  130/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  140/203): Total=   0.00120, embedding_loss_attractive=   0.00086, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:57 TCNTrainer] DEBUG: Epoch 63 (  150/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:58 TCNTrainer] DEBUG: Epoch 63 (  160/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:58 TCNTrainer] DEBUG: Epoch 63 (  170/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:58 TCNTrainer] DEBUG: Epoch 63 (  180/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:58 TCNTrainer] DEBUG: Epoch 63 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:58 TCNTrainer] DEBUG: Epoch 63 (  200/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:25:59 TCNTrainer] INFO: Results 63: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8440355290658772                    │ nan       │\n",
      "│    │ _time_train                              │ 3.232143369037658                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007924199983891513                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000782192771304674                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007924199983891513                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000782192771304674                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016043345120528507                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001701421260930084                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032086690241057015                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003402842521860168                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001113286906749838                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011224770268168428                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:25:59 TCNTrainer] DEBUG: Epoch 64 (    0/203): Total=   0.00113, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:59 TCNTrainer] DEBUG: Epoch 64 (   10/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:25:59 TCNTrainer] DEBUG: Epoch 64 (   20/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   30/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   50/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   60/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   70/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:00 TCNTrainer] DEBUG: Epoch 64 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  110/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  120/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  130/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  140/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:01 TCNTrainer] DEBUG: Epoch 64 (  150/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:02 TCNTrainer] DEBUG: Epoch 64 (  160/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:02 TCNTrainer] DEBUG: Epoch 64 (  170/203): Total=   0.00110, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:02 TCNTrainer] DEBUG: Epoch 64 (  180/203): Total=   0.00116, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00041 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:02 TCNTrainer] DEBUG: Epoch 64 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:02 TCNTrainer] DEBUG: Epoch 64 (  200/203): Total=   0.00110, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:03 TCNTrainer] INFO: Results 64: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8477141270413995                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2183842440135777                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008077602710626605                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007782130148904077                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008077602710626605                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007782130148904077                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016054048435762524                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016947785929175728                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003210809687152505                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003389557185835145                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011288412419768671                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011171687398968365                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:03 TCNTrainer] DEBUG: Epoch 65 (    0/203): Total=   0.00110, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:03 TCNTrainer] DEBUG: Epoch 65 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   20/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   30/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   50/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   60/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:04 TCNTrainer] DEBUG: Epoch 65 (   80/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (   90/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (  110/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (  120/203): Total=   0.00110, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (  130/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:05 TCNTrainer] DEBUG: Epoch 65 (  140/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  150/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  160/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  190/203): Total=   0.00113, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:06 TCNTrainer] DEBUG: Epoch 65 (  200/203): Total=   0.00107, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:07 TCNTrainer] INFO: Results 65: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8513105185702443                    │ nan       │\n",
      "│    │ _time_train                              │ 3.203916849102825                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007644220669236449                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000777379683049148                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007644220669236449                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000777379683049148                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015751738088308938                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016877281369430384                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031503476176617873                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003375456273886077                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010794568302420278                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001114925314566772                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:07 TCNTrainer] DEBUG: Epoch 66 (    0/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:07 TCNTrainer] DEBUG: Epoch 66 (   10/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   20/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   30/203): Total=   0.00104, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00025 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   50/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   60/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:08 TCNTrainer] DEBUG: Epoch 66 (   70/203): Total=   0.00111, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (   90/203): Total=   0.00115, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (  110/203): Total=   0.00117, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (  120/203): Total=   0.00113, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (  130/203): Total=   0.00119, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:09 TCNTrainer] DEBUG: Epoch 66 (  140/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  150/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  160/203): Total=   0.00112, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  170/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  180/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  190/203): Total=   0.00113, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:10 TCNTrainer] DEBUG: Epoch 66 (  200/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:11 TCNTrainer] INFO: Results 66: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.847115402109921                     │ nan       │\n",
      "│    │ _time_train                              │ 3.2229331857524812                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007987026707269252                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007773952844226962                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007987026707269252                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007773952844226962                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016074953755984703                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001695945393294096                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032149907511969415                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003391890786588192                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011202017498564802                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011165843682427858                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:11 TCNTrainer] DEBUG: Epoch 67 (    0/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   10/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   20/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   30/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   50/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:12 TCNTrainer] DEBUG: Epoch 67 (   70/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (   90/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (  110/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (  120/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:13 TCNTrainer] DEBUG: Epoch 67 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  140/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  150/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  160/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  170/203): Total=   0.00111, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  190/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:14 TCNTrainer] DEBUG: Epoch 67 (  200/203): Total=   0.00115, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:15 TCNTrainer] INFO: Results 67: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8498607380315661                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2384092160500586                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008584923880536938                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007746583042927821                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008584923880536938                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007746583042927821                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016826316533196304                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00168582258902013                   │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003365263306639261                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000337164517804026                  │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011950187242796852                 │   0.00007 │\n",
      "│    │ total_train                              │ 0.0011118228267419515                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:15 TCNTrainer] DEBUG: Epoch 68 (    0/203): Total=   0.00119, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   20/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   30/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   40/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:16 TCNTrainer] DEBUG: Epoch 68 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (   80/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (   90/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (  100/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (  110/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (  120/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:17 TCNTrainer] DEBUG: Epoch 68 (  130/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  140/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  160/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  170/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  180/203): Total=   0.00111, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:18 TCNTrainer] DEBUG: Epoch 68 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:19 TCNTrainer] DEBUG: Epoch 68 (  200/203): Total=   0.00115, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:19 TCNTrainer] INFO: Results 68: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8430389082059264                    │ nan       │\n",
      "│    │ _time_train                              │ 3.234128505922854                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000761604483705014                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007773486939212995                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000761604483705014                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007773486939212995                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015908589664225777                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00170176464483931                   │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031817179328451556                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000340352928967862                  │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001079776285526653                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.001117701629598813                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (    0/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   20/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   30/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   40/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   50/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:20 TCNTrainer] DEBUG: Epoch 69 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (   70/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (   80/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (   90/203): Total=   0.00112, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (  110/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:21 TCNTrainer] DEBUG: Epoch 69 (  120/203): Total=   0.00119, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  130/203): Total=   0.00106, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  140/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  160/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:22 TCNTrainer] DEBUG: Epoch 69 (  190/203): Total=   0.00115, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:23 TCNTrainer] DEBUG: Epoch 69 (  200/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:24 TCNTrainer] INFO: Results 69: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.845276468899101                     │ nan       │\n",
      "│    │ _time_train                              │ 3.2585259112529457                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007840004283934832                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007695835883719984                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007840004283934832                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007695835883719984                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015628103787700335                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016869022281556                    │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003125620757540067                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033738044563112003                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010965625205749852                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011069640400246006                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (    0/203): Total=   0.00120, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (   10/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (   30/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (   40/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:24 TCNTrainer] DEBUG: Epoch 70 (   50/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (   60/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (   80/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (   90/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (  100/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (  110/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:25 TCNTrainer] DEBUG: Epoch 70 (  120/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  140/203): Total=   0.00112, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  150/203): Total=   0.00106, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  160/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  170/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:26 TCNTrainer] DEBUG: Epoch 70 (  180/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:27 TCNTrainer] DEBUG: Epoch 70 (  190/203): Total=   0.00113, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:27 TCNTrainer] DEBUG: Epoch 70 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:28 TCNTrainer] INFO: Results 70: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8477885560132563                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2095426279120147                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007588850159663707                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007685048923153302                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007588850159663707                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007685048923153302                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015875432562703887                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016835719637178215                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003175086512540778                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033671439274356434                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010763936766630246                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011052192903348598                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (   10/203): Total=   0.00116, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (   20/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (   30/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (   40/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:28 TCNTrainer] DEBUG: Epoch 71 (   50/203): Total=   0.00113, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (   60/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (   70/203): Total=   0.00115, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (   80/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (   90/203): Total=   0.00115, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (  100/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:26:29 TCNTrainer] DEBUG: Epoch 71 (  110/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  120/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  130/203): Total=   0.00114, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  140/203): Total=   0.00110, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  150/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  160/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:30 TCNTrainer] DEBUG: Epoch 71 (  180/203): Total=   0.00106, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:31 TCNTrainer] DEBUG: Epoch 71 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:31 TCNTrainer] DEBUG: Epoch 71 (  200/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:32 TCNTrainer] INFO: Results 71: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8416430060751736                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2099300362169743                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007489376041727761                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007716120562511565                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007489376041727761                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007716120562511565                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016459150524396036                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016845685316323133                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032918301048792074                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003369137063264627                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001078120619058609                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011085257664772458                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:32 TCNTrainer] DEBUG: Epoch 72 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:32 TCNTrainer] DEBUG: Epoch 72 (   10/203): Total=   0.00118, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:32 TCNTrainer] DEBUG: Epoch 72 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:32 TCNTrainer] DEBUG: Epoch 72 (   30/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:32 TCNTrainer] DEBUG: Epoch 72 (   40/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (   60/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (   70/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (   90/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (  100/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:33 TCNTrainer] DEBUG: Epoch 72 (  110/203): Total=   0.00116, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  120/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  130/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  140/203): Total=   0.00116, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  160/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:34 TCNTrainer] DEBUG: Epoch 72 (  170/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:35 TCNTrainer] DEBUG: Epoch 72 (  180/203): Total=   0.00119, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:35 TCNTrainer] DEBUG: Epoch 72 (  190/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:35 TCNTrainer] DEBUG: Epoch 72 (  200/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:36 TCNTrainer] INFO: Results 72: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8561153030022979                    │ nan       │\n",
      "│    │ _time_train                              │ 3.225548403803259                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0008105719384426872                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007670300733412758                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0008105719384426872                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007670300733412758                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015770513434997862                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.00168480564096915                   │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031541026869995727                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003369611281938301                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011259822071426444                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0011039912084741471                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:36 TCNTrainer] DEBUG: Epoch 73 (    0/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:36 TCNTrainer] DEBUG: Epoch 73 (   10/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:36 TCNTrainer] DEBUG: Epoch 73 (   20/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:36 TCNTrainer] DEBUG: Epoch 73 (   30/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:36 TCNTrainer] DEBUG: Epoch 73 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (   60/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (   70/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (   90/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:37 TCNTrainer] DEBUG: Epoch 73 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  120/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  130/203): Total=   0.00117, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  140/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  150/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  160/203): Total=   0.00117, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:38 TCNTrainer] DEBUG: Epoch 73 (  170/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:39 TCNTrainer] DEBUG: Epoch 73 (  180/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:39 TCNTrainer] DEBUG: Epoch 73 (  190/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:39 TCNTrainer] DEBUG: Epoch 73 (  200/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:40 TCNTrainer] INFO: Results 73: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8565298612229526                    │ nan       │\n",
      "│    │ _time_train                              │ 3.1943365638144314                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000744031557890897                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007612141634319253                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000744031557890897                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007612141634319253                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015893954318016768                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016731858503513883                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003178790863603354                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003346371700702776                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010619106489078453                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001095851343079227                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:40 TCNTrainer] DEBUG: Epoch 74 (    0/203): Total=   0.00113, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:40 TCNTrainer] DEBUG: Epoch 74 (   10/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:40 TCNTrainer] DEBUG: Epoch 74 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:40 TCNTrainer] DEBUG: Epoch 74 (   30/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   40/203): Total=   0.00116, embedding_loss_attractive=   0.00085, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   70/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (   90/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:41 TCNTrainer] DEBUG: Epoch 74 (  100/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  110/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  120/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  130/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  140/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  150/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  160/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:42 TCNTrainer] DEBUG: Epoch 74 (  170/203): Total=   0.00108, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:43 TCNTrainer] DEBUG: Epoch 74 (  180/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:43 TCNTrainer] DEBUG: Epoch 74 (  190/203): Total=   0.00112, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:43 TCNTrainer] DEBUG: Epoch 74 (  200/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:44 TCNTrainer] INFO: Results 74: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8506394904106855                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2096346905454993                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000789229606743902                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007633121857382908                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000789229606743902                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007633121857382908                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015524301768487527                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001670317180119822                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003104860353697505                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003340634360239644                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010997156476757178                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010973756305364148                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:44 TCNTrainer] DEBUG: Epoch 75 (    0/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:44 TCNTrainer] DEBUG: Epoch 75 (   10/203): Total=   0.00102, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:44 TCNTrainer] DEBUG: Epoch 75 (   20/203): Total=   0.00115, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:44 TCNTrainer] DEBUG: Epoch 75 (   30/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   60/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   70/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (   90/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:45 TCNTrainer] DEBUG: Epoch 75 (  100/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  110/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  120/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  130/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  140/203): Total=   0.00117, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  150/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:46 TCNTrainer] DEBUG: Epoch 75 (  160/203): Total=   0.00109, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:47 TCNTrainer] DEBUG: Epoch 75 (  170/203): Total=   0.00109, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:47 TCNTrainer] DEBUG: Epoch 75 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:47 TCNTrainer] DEBUG: Epoch 75 (  190/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:47 TCNTrainer] DEBUG: Epoch 75 (  200/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:48 TCNTrainer] INFO: Results 75: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8416457707062364                    │ nan       │\n",
      "│    │ _time_train                              │ 3.205987743102014                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007377063137634347                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007550247025611842                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007377063137634347                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007550247025611842                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015480648221758504                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016702386158348745                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030961296443517014                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003340477231669749                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001047319287641181                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010890724272765403                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:48 TCNTrainer] DEBUG: Epoch 76 (    0/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:48 TCNTrainer] DEBUG: Epoch 76 (   10/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:48 TCNTrainer] DEBUG: Epoch 76 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:48 TCNTrainer] DEBUG: Epoch 76 (   30/203): Total=   0.00102, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   50/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   60/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   70/203): Total=   0.00105, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   80/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:49 TCNTrainer] DEBUG: Epoch 76 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  100/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  110/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  120/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  130/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  140/203): Total=   0.00108, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  150/203): Total=   0.00114, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:50 TCNTrainer] DEBUG: Epoch 76 (  160/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:51 TCNTrainer] DEBUG: Epoch 76 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:51 TCNTrainer] DEBUG: Epoch 76 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:51 TCNTrainer] DEBUG: Epoch 76 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:51 TCNTrainer] DEBUG: Epoch 76 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:52 TCNTrainer] INFO: Results 76: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9353641499765217                    │ nan       │\n",
      "│    │ _time_train                              │ 3.212371059227735                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007557373409832103                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007571285662787227                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007557373409832103                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007571285662787227                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015885761292237374                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001661368077414413                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031771522584474757                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033227361548288266                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010734525767879355                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.00108940218772574                   │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:52 TCNTrainer] DEBUG: Epoch 77 (    0/203): Total=   0.00114, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:52 TCNTrainer] DEBUG: Epoch 77 (   10/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:52 TCNTrainer] DEBUG: Epoch 77 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   30/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   40/203): Total=   0.00117, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   60/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   70/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:53 TCNTrainer] DEBUG: Epoch 77 (   80/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  100/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  120/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  130/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  140/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:54 TCNTrainer] DEBUG: Epoch 77 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:55 TCNTrainer] DEBUG: Epoch 77 (  160/203): Total=   0.00110, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:55 TCNTrainer] DEBUG: Epoch 77 (  170/203): Total=   0.00117, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:55 TCNTrainer] DEBUG: Epoch 77 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:55 TCNTrainer] DEBUG: Epoch 77 (  190/203): Total=   0.00113, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:55 TCNTrainer] DEBUG: Epoch 77 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:26:56 TCNTrainer] INFO: Results 77: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8472017250023782                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2373131061904132                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007499578402429405                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007608866154473146                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007499578402429405                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007608866154473146                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015604126331810323                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016757041058764596                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031208252663620647                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003351408211752919                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010620403724412124                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010960274416118346                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:26:56 TCNTrainer] DEBUG: Epoch 78 (    0/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:56 TCNTrainer] DEBUG: Epoch 78 (   10/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   30/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   40/203): Total=   0.00109, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   50/203): Total=   0.00114, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   60/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:57 TCNTrainer] DEBUG: Epoch 78 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (   90/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (  100/203): Total=   0.00120, embedding_loss_attractive=   0.00083, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (  120/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (  130/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:58 TCNTrainer] DEBUG: Epoch 78 (  140/203): Total=   0.00104, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  150/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  160/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  170/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  180/203): Total=   0.00111, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  190/203): Total=   0.00106, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:26:59 TCNTrainer] DEBUG: Epoch 78 (  200/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:00 TCNTrainer] INFO: Results 78: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8476250800304115                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2448355020023882                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007628984781654759                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007541871548221984                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007628984781654759                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007541871548221984                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001598859593893091                  │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001657792787971388                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003197719187786182                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003315585575942776                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010826703988843494                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010857457178071362                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:00 TCNTrainer] DEBUG: Epoch 79 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:00 TCNTrainer] DEBUG: Epoch 79 (   10/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   20/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   30/203): Total=   0.00106, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   40/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   50/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   60/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:01 TCNTrainer] DEBUG: Epoch 79 (   70/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (   90/203): Total=   0.00109, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (  100/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (  110/203): Total=   0.00112, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (  120/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (  130/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:02 TCNTrainer] DEBUG: Epoch 79 (  140/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  150/203): Total=   0.00115, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  160/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  170/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  180/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:03 TCNTrainer] DEBUG: Epoch 79 (  200/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:04 TCNTrainer] INFO: Results 79: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8457929091528058                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2117232028394938                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007519787533156988                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007572475255333952                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007519787533156988                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007572475255333952                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016312759408416847                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016621073749228357                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00032625518816833694                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033242147498456715                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010782339493744075                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010896690064820971                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:04 TCNTrainer] DEBUG: Epoch 80 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   10/203): Total=   0.00114, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   20/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   30/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   40/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   50/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:05 TCNTrainer] DEBUG: Epoch 80 (   70/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (   80/203): Total=   0.00112, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (   90/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (  100/203): Total=   0.00107, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (  110/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (  120/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:06 TCNTrainer] DEBUG: Epoch 80 (  130/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  140/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  150/203): Total=   0.00103, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  160/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  170/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  180/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  190/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:07 TCNTrainer] DEBUG: Epoch 80 (  200/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:08 TCNTrainer] INFO: Results 80: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8559991149231791                    │ nan       │\n",
      "│    │ _time_train                              │ 3.218899588100612                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007509348823481963                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007446341462011261                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007509348823481963                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007446341462011261                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001557484408840537                  │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016530638324381243                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031149688176810745                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003306127664876248                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010624317665739605                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010752469149826489                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:08 TCNTrainer] DEBUG: Epoch 81 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   10/203): Total=   0.00099, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   20/203): Total=   0.00114, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   30/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   40/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   50/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:09 TCNTrainer] DEBUG: Epoch 81 (   60/203): Total=   0.00112, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (  100/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (  110/203): Total=   0.00102, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (  120/203): Total=   0.00100, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:10 TCNTrainer] DEBUG: Epoch 81 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  140/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  150/203): Total=   0.00102, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  160/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  170/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  180/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:11 TCNTrainer] DEBUG: Epoch 81 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:12 TCNTrainer] DEBUG: Epoch 81 (  200/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:12 TCNTrainer] INFO: Results 81: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8477930151857436                    │ nan       │\n",
      "│    │ _time_train                              │ 3.221903308760375                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007322683846319302                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007470627048676788                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007322683846319302                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007470627048676788                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015387277222341962                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016430853338444175                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003077455444468393                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003286170667688835                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010400139345114845                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010756797735290281                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (    0/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   20/203): Total=   0.00113, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   30/203): Total=   0.00110, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   50/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:13 TCNTrainer] DEBUG: Epoch 82 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (   70/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (   80/203): Total=   0.00105, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (   90/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (  100/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (  110/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:14 TCNTrainer] DEBUG: Epoch 82 (  120/203): Total=   0.00102, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  130/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  140/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  150/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  160/203): Total=   0.00108, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  170/203): Total=   0.00100, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  180/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:15 TCNTrainer] DEBUG: Epoch 82 (  190/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:16 TCNTrainer] DEBUG: Epoch 82 (  200/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:17 TCNTrainer] INFO: Results 82: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8576750727370381                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2218690938316286                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007964825399944352                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007497713372932536                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007964825399944352                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007497713372932536                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001556715705535478                  │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016516965826551243                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031134314110709563                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00033033931653102483                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0011078256861461948                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001080110659386981                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (    0/203): Total=   0.00108, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (   10/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (   20/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (   30/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (   40/203): Total=   0.00105, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:17 TCNTrainer] DEBUG: Epoch 83 (   50/203): Total=   0.00111, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (   60/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (   70/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (   80/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (   90/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (  100/203): Total=   0.00102, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (  110/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:18 TCNTrainer] DEBUG: Epoch 83 (  120/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  130/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  140/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  150/203): Total=   0.00098, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  160/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  170/203): Total=   0.00102, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:19 TCNTrainer] DEBUG: Epoch 83 (  180/203): Total=   0.00100, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:20 TCNTrainer] DEBUG: Epoch 83 (  190/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:20 TCNTrainer] DEBUG: Epoch 83 (  200/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:21 TCNTrainer] INFO: Results 83: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8496447568759322                    │ nan       │\n",
      "│    │ _time_train                              │ 3.22773329121992                      │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007534312039044582                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007393916869843424                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007534312039044582                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007393916869843424                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015505626707130837                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016313926064468957                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003101125341426167                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032627852128937917                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010635437430917389                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010656702132629497                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (    0/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (   10/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (   20/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (   30/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (   40/203): Total=   0.00102, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:21 TCNTrainer] DEBUG: Epoch 84 (   50/203): Total=   0.00110, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (   60/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (   70/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (  100/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:22 TCNTrainer] DEBUG: Epoch 84 (  110/203): Total=   0.00118, embedding_loss_attractive=   0.00081, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  120/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  130/203): Total=   0.00105, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  140/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  150/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  160/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  170/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:23 TCNTrainer] DEBUG: Epoch 84 (  180/203): Total=   0.00115, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:24 TCNTrainer] DEBUG: Epoch 84 (  190/203): Total=   0.00114, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:24 TCNTrainer] DEBUG: Epoch 84 (  200/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:25 TCNTrainer] INFO: Results 84: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8530309558846056                    │ nan       │\n",
      "│    │ _time_train                              │ 3.244514327030629                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007263553341747158                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007516083300682564                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007263553341747158                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007516083300682564                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0016592232523382538                 │   0.00025 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016428445686076955                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003318446504676508                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032856891372153913                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010581999849010673                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010801772518184384                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:25 TCNTrainer] DEBUG: Epoch 85 (    0/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:25 TCNTrainer] DEBUG: Epoch 85 (   10/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:25 TCNTrainer] DEBUG: Epoch 85 (   20/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:25 TCNTrainer] DEBUG: Epoch 85 (   30/203): Total=   0.00111, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:25 TCNTrainer] DEBUG: Epoch 85 (   40/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (   60/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (   70/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (   80/203): Total=   0.00114, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (   90/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (  100/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:26 TCNTrainer] DEBUG: Epoch 85 (  110/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  120/203): Total=   0.00106, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  130/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  140/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  150/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  160/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:27 TCNTrainer] DEBUG: Epoch 85 (  170/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:28 TCNTrainer] DEBUG: Epoch 85 (  180/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:28 TCNTrainer] DEBUG: Epoch 85 (  190/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:28 TCNTrainer] DEBUG: Epoch 85 (  200/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:29 TCNTrainer] INFO: Results 85: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8487273142673075                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2165274037979543                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007410724841368695                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007353248992209952                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007410724841368695                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007353248992209952                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015911225345917046                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016370781145650399                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003182245069183409                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032741562291300797                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010592969949357212                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010627405276393582                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:29 TCNTrainer] DEBUG: Epoch 86 (    0/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:29 TCNTrainer] DEBUG: Epoch 86 (   10/203): Total=   0.00113, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:29 TCNTrainer] DEBUG: Epoch 86 (   20/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:29 TCNTrainer] DEBUG: Epoch 86 (   30/203): Total=   0.00104, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:29 TCNTrainer] DEBUG: Epoch 86 (   40/203): Total=   0.00112, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (   60/203): Total=   0.00109, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (   70/203): Total=   0.00108, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (   80/203): Total=   0.00110, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (   90/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:30 TCNTrainer] DEBUG: Epoch 86 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  110/203): Total=   0.00103, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  120/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  130/203): Total=   0.00106, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  140/203): Total=   0.00100, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  160/203): Total=   0.00104, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:31 TCNTrainer] DEBUG: Epoch 86 (  170/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:32 TCNTrainer] DEBUG: Epoch 86 (  180/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:32 TCNTrainer] DEBUG: Epoch 86 (  190/203): Total=   0.00102, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:32 TCNTrainer] DEBUG: Epoch 86 (  200/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:33 TCNTrainer] INFO: Results 86: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8513176077976823                    │ nan       │\n",
      "│    │ _time_train                              │ 3.216170793864876                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007558917303362654                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007389387416336777                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007558917303362654                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007389387416336777                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015324296183987624                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016326779893053502                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030648592367975253                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032653559786107003                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001062377661259638                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.001065474342190078                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:33 TCNTrainer] DEBUG: Epoch 87 (    0/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:33 TCNTrainer] DEBUG: Epoch 87 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:33 TCNTrainer] DEBUG: Epoch 87 (   20/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:33 TCNTrainer] DEBUG: Epoch 87 (   30/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   40/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   50/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   60/203): Total=   0.00111, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   80/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (   90/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:34 TCNTrainer] DEBUG: Epoch 87 (  100/203): Total=   0.00108, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  110/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  120/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  140/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  150/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:35 TCNTrainer] DEBUG: Epoch 87 (  160/203): Total=   0.00100, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:36 TCNTrainer] DEBUG: Epoch 87 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:36 TCNTrainer] DEBUG: Epoch 87 (  180/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:36 TCNTrainer] DEBUG: Epoch 87 (  190/203): Total=   0.00101, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:36 TCNTrainer] DEBUG: Epoch 87 (  200/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:37 TCNTrainer] INFO: Results 87: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8531996221281588                    │ nan       │\n",
      "│    │ _time_train                              │ 3.215334403794259                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007187584440948234                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007364662809141458                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007187584440948234                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007364662809141458                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015247324598021805                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001626846128776486                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003049464919604361                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003253692257552972                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010237049460152371                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010618355172213735                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:37 TCNTrainer] DEBUG: Epoch 88 (    0/203): Total=   0.00105, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:37 TCNTrainer] DEBUG: Epoch 88 (   10/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:37 TCNTrainer] DEBUG: Epoch 88 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:37 TCNTrainer] DEBUG: Epoch 88 (   30/203): Total=   0.00117, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00039 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   40/203): Total=   0.00114, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   60/203): Total=   0.00113, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00038 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   70/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:38 TCNTrainer] DEBUG: Epoch 88 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  100/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  110/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  120/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  130/203): Total=   0.00113, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  140/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  150/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:39 TCNTrainer] DEBUG: Epoch 88 (  160/203): Total=   0.00109, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:40 TCNTrainer] DEBUG: Epoch 88 (  170/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:40 TCNTrainer] DEBUG: Epoch 88 (  180/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:40 TCNTrainer] DEBUG: Epoch 88 (  190/203): Total=   0.00109, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:40 TCNTrainer] DEBUG: Epoch 88 (  200/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:41 TCNTrainer] INFO: Results 88: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8518004142679274                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2277338076382875                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007585511361766192                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007367370772895775                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007585511361766192                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007367370772895775                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0014950080063297518                 │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016276390467233625                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00029900160126595035                │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003255278093446725                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010575527435220365                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001062264891623478                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:41 TCNTrainer] DEBUG: Epoch 89 (    0/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:41 TCNTrainer] DEBUG: Epoch 89 (   10/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:41 TCNTrainer] DEBUG: Epoch 89 (   20/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   30/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   40/203): Total=   0.00105, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   50/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   60/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   70/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   80/203): Total=   0.00100, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:42 TCNTrainer] DEBUG: Epoch 89 (   90/203): Total=   0.00100, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  100/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  110/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  120/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  130/203): Total=   0.00104, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  140/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:43 TCNTrainer] DEBUG: Epoch 89 (  150/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:44 TCNTrainer] DEBUG: Epoch 89 (  160/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:44 TCNTrainer] DEBUG: Epoch 89 (  170/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:44 TCNTrainer] DEBUG: Epoch 89 (  180/203): Total=   0.00105, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:44 TCNTrainer] DEBUG: Epoch 89 (  190/203): Total=   0.00095, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00023 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:44 TCNTrainer] DEBUG: Epoch 89 (  200/203): Total=   0.00096, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:45 TCNTrainer] INFO: Results 89: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8471842366270721                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2101784823462367                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007583755462999559                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007333735189299682                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007583755462999559                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007333735189299682                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015628132024883396                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016187489459410383                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00031256264049766793                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003237497891882077                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001070938196628251                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010571233123045397                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:45 TCNTrainer] DEBUG: Epoch 90 (    0/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:45 TCNTrainer] DEBUG: Epoch 90 (   10/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:45 TCNTrainer] DEBUG: Epoch 90 (   20/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   30/203): Total=   0.00112, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   40/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   60/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   70/203): Total=   0.00101, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:46 TCNTrainer] DEBUG: Epoch 90 (   80/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  110/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  120/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  130/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  140/203): Total=   0.00108, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:47 TCNTrainer] DEBUG: Epoch 90 (  150/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:48 TCNTrainer] DEBUG: Epoch 90 (  160/203): Total=   0.00111, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:48 TCNTrainer] DEBUG: Epoch 90 (  170/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:48 TCNTrainer] DEBUG: Epoch 90 (  180/203): Total=   0.00104, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:48 TCNTrainer] DEBUG: Epoch 90 (  190/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:48 TCNTrainer] DEBUG: Epoch 90 (  200/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:49 TCNTrainer] INFO: Results 90: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8527167052961886                    │ nan       │\n",
      "│    │ _time_train                              │ 3.221487056929618                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000724152394104749                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007320331083266569                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000724152394104749                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007320331083266569                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015373298846599129                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016211411861812906                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030746597693198256                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032422823723625817                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010316183829369645                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010562613503227534                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:49 TCNTrainer] DEBUG: Epoch 91 (    0/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:49 TCNTrainer] DEBUG: Epoch 91 (   10/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   30/203): Total=   0.00102, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   40/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   50/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   60/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   70/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:50 TCNTrainer] DEBUG: Epoch 91 (   80/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (   90/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (  100/203): Total=   0.00111, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (  110/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (  120/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (  130/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:51 TCNTrainer] DEBUG: Epoch 91 (  140/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  150/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  160/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  170/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  180/203): Total=   0.00105, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  190/203): Total=   0.00095, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:52 TCNTrainer] DEBUG: Epoch 91 (  200/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:53 TCNTrainer] INFO: Results 91: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8481284398585558                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2651141290552914                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0006972660834435373                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007308340199025613                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0006972660834435373                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007308340199025613                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015493158302787278                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016164148015341735                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030986316605574555                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032328296030683474                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010071292532504432                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010541169852559716                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:53 TCNTrainer] DEBUG: Epoch 92 (    0/203): Total=   0.00100, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:53 TCNTrainer] DEBUG: Epoch 92 (   10/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   20/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   30/203): Total=   0.00100, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   50/203): Total=   0.00110, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   60/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:54 TCNTrainer] DEBUG: Epoch 92 (   70/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (   80/203): Total=   0.00105, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (   90/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (  100/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (  110/203): Total=   0.00101, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (  120/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (  130/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:55 TCNTrainer] DEBUG: Epoch 92 (  140/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  150/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  160/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  180/203): Total=   0.00109, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  190/203): Total=   0.00099, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:56 TCNTrainer] DEBUG: Epoch 92 (  200/203): Total=   0.00097, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:27:57 TCNTrainer] INFO: Results 92: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.855348136741668                     │ nan       │\n",
      "│    │ _time_train                              │ 3.220895897131413                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007241153132377399                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007284233705576491                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007241153132377399                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007284233705576491                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015841885715619558                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001610080085309415                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003168377143123912                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032201601706188305                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010409530388036123                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010504393936983617                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:27:57 TCNTrainer] DEBUG: Epoch 93 (    0/203): Total=   0.00106, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   10/203): Total=   0.00108, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   20/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   30/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   40/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   50/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   60/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:58 TCNTrainer] DEBUG: Epoch 93 (   70/203): Total=   0.00103, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (   80/203): Total=   0.00104, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (   90/203): Total=   0.00099, embedding_loss_attractive=   0.00066, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (  100/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (  110/203): Total=   0.00095, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (  120/203): Total=   0.00102, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:27:59 TCNTrainer] DEBUG: Epoch 93 (  130/203): Total=   0.00103, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  140/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  150/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  160/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  170/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  180/203): Total=   0.00104, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  190/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:00 TCNTrainer] DEBUG: Epoch 93 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:01 TCNTrainer] INFO: Results 93: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.848923372104764                     │ nan       │\n",
      "│    │ _time_train                              │ 3.210411438718438                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007222113982101695                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007272810139578625                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007222113982101695                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007272810139578625                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015696531355691454                 │   0.00024 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016140481571809118                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003139306271138291                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032280963143618233                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010361420352839761                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010500906512434848                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:01 TCNTrainer] DEBUG: Epoch 94 (    0/203): Total=   0.00110, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00040 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   10/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   20/203): Total=   0.00106, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   30/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   40/203): Total=   0.00103, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   50/203): Total=   0.00101, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:02 TCNTrainer] DEBUG: Epoch 94 (   60/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (   70/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (   80/203): Total=   0.00115, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (   90/203): Total=   0.00105, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (  100/203): Total=   0.00108, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (  110/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (  120/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:03 TCNTrainer] DEBUG: Epoch 94 (  130/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  140/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  150/203): Total=   0.00095, embedding_loss_attractive=   0.00065, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  160/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  180/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:04 TCNTrainer] DEBUG: Epoch 94 (  190/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:05 TCNTrainer] DEBUG: Epoch 94 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:05 TCNTrainer] INFO: Results 94: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8466691691428423                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2267441488802433                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007541694891794274                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007254817790301343                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007541694891794274                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007254817790301343                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0014926675137960249                 │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.001612171759539894                  │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00029853350275920506                │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003224343519079788                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001052703001510559                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010479161357552988                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (    0/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   10/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   20/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   30/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   40/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   50/203): Total=   0.00104, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:06 TCNTrainer] DEBUG: Epoch 95 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (   70/203): Total=   0.00099, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (   80/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (   90/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (  100/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (  110/203): Total=   0.00099, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:07 TCNTrainer] DEBUG: Epoch 95 (  120/203): Total=   0.00101, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  130/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  140/203): Total=   0.00098, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  150/203): Total=   0.00101, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  160/203): Total=   0.00103, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  170/203): Total=   0.00100, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  180/203): Total=   0.00107, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:08 TCNTrainer] DEBUG: Epoch 95 (  190/203): Total=   0.00110, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:09 TCNTrainer] DEBUG: Epoch 95 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:10 TCNTrainer] INFO: Results 95: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8553779227659106                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2111809751950204                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007187406590674073                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000724430412187108                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007187406590674073                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000724430412187108                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001504406181629747                  │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016042837733173459                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030088123632594945                │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003208567546634692                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010196219062587866                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010452871744204403                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (    0/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (   10/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (   20/203): Total=   0.00101, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (   30/203): Total=   0.00099, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (   40/203): Total=   0.00104, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:10 TCNTrainer] DEBUG: Epoch 96 (   50/203): Total=   0.00102, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (   60/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (   70/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (   80/203): Total=   0.00117, embedding_loss_attractive=   0.00082, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (   90/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (  100/203): Total=   0.00103, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (  110/203): Total=   0.00102, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:11 TCNTrainer] DEBUG: Epoch 96 (  120/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  130/203): Total=   0.00098, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  140/203): Total=   0.00101, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  150/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  160/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  170/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:12 TCNTrainer] DEBUG: Epoch 96 (  180/203): Total=   0.00110, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:13 TCNTrainer] DEBUG: Epoch 96 (  190/203): Total=   0.00102, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:13 TCNTrainer] DEBUG: Epoch 96 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:14 TCNTrainer] INFO: Results 96: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8497614772059023                    │ nan       │\n",
      "│    │ _time_train                              │ 3.210985907819122                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007317703902824885                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007235600078005913                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007317703902824885                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007235600078005913                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015302225041927563                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016036206596012554                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003060445008385513                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00032072413192025113                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001037814899528813                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010442841468892735                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (    0/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (   20/203): Total=   0.00108, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (   30/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (   40/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:14 TCNTrainer] DEBUG: Epoch 97 (   50/203): Total=   0.00109, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (   60/203): Total=   0.00108, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (   70/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (   80/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (   90/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (  100/203): Total=   0.00107, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (  110/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:15 TCNTrainer] DEBUG: Epoch 97 (  120/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  130/203): Total=   0.00109, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  140/203): Total=   0.00100, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  150/203): Total=   0.00100, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  160/203): Total=   0.00106, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  170/203): Total=   0.00102, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:16 TCNTrainer] DEBUG: Epoch 97 (  180/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:17 TCNTrainer] DEBUG: Epoch 97 (  190/203): Total=   0.00111, embedding_loss_attractive=   0.00080, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:17 TCNTrainer] DEBUG: Epoch 97 (  200/203): Total=   0.00110, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:18 TCNTrainer] INFO: Results 97: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8533202791586518                    │ nan       │\n",
      "│    │ _time_train                              │ 3.192144467961043                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007554507775542637                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.000722174454718975                  │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007554507775542637                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.000722174454718975                  │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0014971926213345594                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0016022814062694685                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002994385242669119                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003204562812538937                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001054889301303774                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010426307433706896                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (    0/203): Total=   0.00102, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (   10/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (   20/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (   30/203): Total=   0.00110, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (   40/203): Total=   0.00107, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:18 TCNTrainer] DEBUG: Epoch 98 (   50/203): Total=   0.00099, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (   60/203): Total=   0.00103, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (   70/203): Total=   0.00107, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (   80/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (   90/203): Total=   0.00102, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (  100/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[10:28:19 TCNTrainer] DEBUG: Epoch 98 (  110/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  120/203): Total=   0.00105, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  130/203): Total=   0.00103, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  140/203): Total=   0.00095, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  150/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  160/203): Total=   0.00106, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  170/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:20 TCNTrainer] DEBUG: Epoch 98 (  180/203): Total=   0.00099, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:21 TCNTrainer] DEBUG: Epoch 98 (  190/203): Total=   0.00095, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00028 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:21 TCNTrainer] DEBUG: Epoch 98 (  200/203): Total=   0.00106, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:22 TCNTrainer] INFO: Results 98: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.9202935099601746                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2191423238255084                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.000701238099201065                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007204140678135302                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.000701238099201065                  │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007204140678135302                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015037890743567711                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0015983383843372462                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0003007578148713543                 │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.0003196676768674493                 │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010019959138137186                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010400817453118014                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:22 TCNTrainer] DEBUG: Epoch 99 (    0/203): Total=   0.00098, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00026 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:22 TCNTrainer] DEBUG: Epoch 99 (   10/203): Total=   0.00108, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:22 TCNTrainer] DEBUG: Epoch 99 (   20/203): Total=   0.00104, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:22 TCNTrainer] DEBUG: Epoch 99 (   30/203): Total=   0.00099, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:22 TCNTrainer] DEBUG: Epoch 99 (   40/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (   50/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (   60/203): Total=   0.00101, embedding_loss_attractive=   0.00067, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (   70/203): Total=   0.00101, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (   80/203): Total=   0.00105, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (   90/203): Total=   0.00108, embedding_loss_attractive=   0.00075, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:23 TCNTrainer] DEBUG: Epoch 99 (  100/203): Total=   0.00098, embedding_loss_attractive=   0.00068, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  110/203): Total=   0.00109, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  120/203): Total=   0.00107, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  130/203): Total=   0.00108, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00037 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  140/203): Total=   0.00102, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00033 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  150/203): Total=   0.00102, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  160/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:24 TCNTrainer] DEBUG: Epoch 99 (  170/203): Total=   0.00103, embedding_loss_attractive=   0.00072, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:25 TCNTrainer] DEBUG: Epoch 99 (  180/203): Total=   0.00109, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:25 TCNTrainer] DEBUG: Epoch 99 (  190/203): Total=   0.00102, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:25 TCNTrainer] DEBUG: Epoch 99 (  200/203): Total=   0.00101, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:26 TCNTrainer] INFO: Results 99: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8497723829932511                    │ nan       │\n",
      "│    │ _time_train                              │ 3.2021309132687747                    │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007504884936174171                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007185133144241102                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007504884936174171                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007185133144241102                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.001496108085848391                  │   0.00022 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0015988131334748247                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.0002992216171696782                 │   0.00004 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.000319762626694965                  │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.001049710108458789                  │   0.00006 │\n",
      "│    │ total_train                              │ 0.0010382759455348288                 │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[36m[10:28:26 TCNTrainer] DEBUG: Epoch 100 (    0/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:26 TCNTrainer] DEBUG: Epoch 100 (   10/203): Total=   0.00104, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:26 TCNTrainer] DEBUG: Epoch 100 (   20/203): Total=   0.00104, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00035 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:26 TCNTrainer] DEBUG: Epoch 100 (   30/203): Total=   0.00098, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00027 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   40/203): Total=   0.00106, embedding_loss_attractive=   0.00074, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   50/203): Total=   0.00102, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   60/203): Total=   0.00112, embedding_loss_attractive=   0.00076, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   70/203): Total=   0.00100, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00031 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   80/203): Total=   0.00111, embedding_loss_attractive=   0.00077, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (   90/203): Total=   0.00102, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:27 TCNTrainer] DEBUG: Epoch 100 (  100/203): Total=   0.00109, embedding_loss_attractive=   0.00079, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  110/203): Total=   0.00105, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  120/203): Total=   0.00100, embedding_loss_attractive=   0.00070, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  130/203): Total=   0.00105, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  140/203): Total=   0.00105, embedding_loss_attractive=   0.00069, embedding_loss_repulsive=   0.00036 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  150/203): Total=   0.00111, embedding_loss_attractive=   0.00078, embedding_loss_repulsive=   0.00032 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:28 TCNTrainer] DEBUG: Epoch 100 (  160/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:29 TCNTrainer] DEBUG: Epoch 100 (  170/203): Total=   0.00107, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00034 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:29 TCNTrainer] DEBUG: Epoch 100 (  180/203): Total=   0.00096, embedding_loss_attractive=   0.00066, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:29 TCNTrainer] DEBUG: Epoch 100 (  190/203): Total=   0.00100, embedding_loss_attractive=   0.00071, embedding_loss_repulsive=   0.00029 (weighted)\u001b[0m\n",
      "\u001b[36m[10:28:29 TCNTrainer] DEBUG: Epoch 100 (  200/203): Total=   0.00103, embedding_loss_attractive=   0.00073, embedding_loss_repulsive=   0.00030 (weighted)\u001b[0m\n",
      "/scratch/gpfs/dc3896/micromamba/envs/gnn/lib/python3.10/site-packages/torch_geometric/data/collate.py:145: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "\u001b[32m[10:28:30 TCNTrainer] INFO: Results 100: \n",
      "┌────┬──────────────────────────────────────────┬───────────────────────────────────────┬───────────┐\n",
      "│    │ Metric                                   │ Value                                 │       Std │\n",
      "├────┼──────────────────────────────────────────┼───────────────────────────────────────┼───────────┤\n",
      "│    │ _time_test                               │ 0.8540350957773626                    │ nan       │\n",
      "│    │ _time_train                              │ 3.215696293860674                     │ nan       │\n",
      "│    │ embedding_loss_attractive                │ 0.0007311660492430544                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_train          │ 0.0007195773817199681                 │ nan       │\n",
      "│    │ embedding_loss_attractive_weighted       │ 0.0007311660492430544                 │   0.00004 │\n",
      "│    │ embedding_loss_attractive_weighted_train │ 0.0007195773817199681                 │ nan       │\n",
      "│    │ embedding_loss_repulsive                 │ 0.0015365250007663336                 │   0.00023 │\n",
      "│    │ embedding_loss_repulsive_train           │ 0.0015951637780807655                 │ nan       │\n",
      "│    │ embedding_loss_repulsive_weighted        │ 0.00030730500015326674                │   0.00005 │\n",
      "│    │ embedding_loss_repulsive_weighted_train  │ 0.00031903275561615316                │ nan       │\n",
      "│    │ lw_embedding_loss                        │ {'attractive': 1.0, 'repulsive': 0.2} │ nan       │\n",
      "│    │ total                                    │ 0.0010384710492669707                 │   0.00006 │\n",
      "│    │ total_train                              │ 0.001038610144963332                  │ nan       │\n",
      "└────┴──────────────────────────────────────────┴───────────────────────────────────────┴───────────┘\u001b[0m\n",
      "\u001b[32m[10:28:30 TCNTrainer] INFO: Saving checkpoint to 230707_102830_model.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "loss_history = train_model(weight_attractive=1.0,\n",
    "                             weight_repulsive=0.2,\n",
    "                             loaders=loaders,\n",
    "                             num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e89340ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b749cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = f'Scaling coefficient = {1.0} (attractive), {0.2} (repulsive)'\n",
    "plt.plot(loss_history['embedding_loss_attractive_train'],loss_history['embedding_loss_repulsive_train'],\n",
    "         marker='o', markersize=3, label=label)\n",
    "\n",
    "plt.xlabel('embedding_loss_attractive_train')\n",
    "plt.ylabel('embedding_loss_repulsive_train')\n",
    "plt.title('Loss Functions')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b87bb6fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlEAAAHHCAYAAACfqw0dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACoFklEQVR4nOzdeVhU1RsH8O/MADOsg8guq4jijqIgqOCCYVlJlnu55JaVaWpuqWipmEuaaeFSoj9309RMScQtkQDFDRUFBVFgWEQGZWfm/P5Aro4MOIzAAL6f5+GROXPuue+MyLyee+57eIwxBkIIIYQQUi18TQdACCGEENIQURJFCCGEEKIGSqIIIYQQQtRASRQhhBBCiBooiSKEEEIIUQMlUYQQQgghaqAkihBCCCFEDZREEUIIIYSogZIoQgghhBA1UBJFCCENwJkzZ8Dj8XDmzBlNh0IIeYaSKEJIjQkODgaPx8PFixc1HUqVFi1aBB6Pp/QrKChIo7H98ssvCA4O1mgMhBDVaGk6AEII0ZRff/0VBgYGCm0eHh4aiqbML7/8AlNTU4wZM0ah3dvbGwUFBdDR0dFMYISQCiiJIoS8sT766COYmppqOgyV8Pl8iEQiTYdBCHkBXc4jhNS5y5cv4+2334aRkREMDAzQt29f/Pfffwp9SkpKsHjxYjg7O0MkEqFp06bo0aMHQkNDuT4SiQRjx46FjY0NhEIhrKysMHDgQCQlJb1WfElJSeDxeEovq/F4PCxatIh7XH5pMCEhAWPGjIGxsTHEYjHGjh2L/Pz8Csfv2LED7u7u0NPTQ5MmTeDt7Y0TJ04AABwcHHDjxg2cPXuWu7zYq1cvAJWvidq/fz/c3Nygq6sLU1NTfPzxx0hJSVHoM2bMGBgYGCAlJQX+/v4wMDCAmZkZZs6cCZlMptB3z549cHNzg6GhIYyMjNC+fXv89NNP1X8TCXkD0EwUIaRO3bhxAz179oSRkRFmzZoFbW1tbNy4Eb169cLZs2e5y2mLFi1CYGAgxo8fD3d3d+Tm5uLixYuIiYlBv379AAAffvghbty4gSlTpsDBwQEZGRkIDQ1FcnIyHBwcXhlLdna2wmOBQIAmTZqo9bqGDBkCR0dHBAYGIiYmBlu2bIG5uTl++OEHrs/ixYuxaNEieHl54bvvvoOOjg4iIyNx6tQpvPXWW1i7di2mTJkCAwMDfPvttwAACwuLSs8ZHByMsWPHomvXrggMDER6ejp++uknhIeH4/LlyzA2Nub6ymQy+Pn5wcPDA6tWrcLJkyexevVqODk5YfLkyQCA0NBQDB8+HH379uXivnXrFsLDwzF16lS13hdCGjVGCCE1ZOvWrQwAi46OrrSPv78/09HRYXfv3uXaUlNTmaGhIfP29ubaOnbsyAYMGFDpOI8fP2YA2MqVK6sdZ0BAAANQ4cve3p4xxlhiYiIDwLZu3VrhWAAsICCgwliffvqpQr8PPviANW3alHscHx/P+Hw+++CDD5hMJlPoK5fLue/btm3LfHx8Kpz39OnTDAA7ffo0Y4yx4uJiZm5uztq1a8cKCgq4fkePHmUA2MKFC7m20aNHMwDsu+++UxizU6dOzM3NjXs8depUZmRkxEpLSyucnxBSEV3OI4TUGZlMhhMnTsDf3x/Nmzfn2q2srDBixAicP38eubm5AABjY2PcuHED8fHxSsfS1dWFjo4Ozpw5g8ePH6sVz4EDBxAaGsp97dy5U61xAOCzzz5TeNyzZ088evSIez2HDh2CXC7HwoULwecr/url8XjVPt/FixeRkZGBzz//XGGt1IABA+Di4oK///5bpRjv3bvHPTY2NkZeXp7CJVNCSOUoiSKE1JnMzEzk5+ejVatWFZ5r3bo15HI5Hjx4AAD47rvvkJOTg5YtW6J9+/b45ptvcO3aNa6/UCjEDz/8gOPHj8PCwgLe3t5YsWIFJBKJyvF4e3vD19eX++revbvar83Ozk7hcfllwfIE7+7du+Dz+WjTpo3a53jR/fv3AUDpe+ni4sI9X04kEsHMzKxCjC8moJ9//jlatmyJt99+GzY2Nvj0008REhJSI/ES0hhREkUIqZe8vb1x9+5d/P7772jXrh22bNmCzp07Y8uWLVyfadOm4c6dOwgMDIRIJMKCBQvQunVrXL58+bXOXdnM0MuLsF8kEAiUtjPGXiuWmlJZfC8yNzfHlStXcOTIEbz//vs4ffo03n77bYwePboOIiSk4aEkihBSZ8zMzKCnp4fbt29XeC4uLg58Ph+2trZcm4mJCcaOHYvdu3fjwYMH6NChg8KdcQDg5OSEGTNm4MSJE4iNjUVxcTFWr179WnGWzyLl5OQotL88u1MdTk5OkMvluHnzZpX9VL20Z29vDwBK38vbt29zz1eXjo4O3nvvPfzyyy+4e/cuJk2ahO3btyMhIUGt8QhpzCiJIoTUGYFAgLfeeguHDx9WKEOQnp6OXbt2oUePHjAyMgIAPHr0SOFYAwMDtGjRAkVFRQCA/Px8FBYWKvRxcnKCoaEh10ddRkZGMDU1xblz5xTaf/nlF7XH9Pf3B5/Px3fffQe5XK7w3IuzVfr6+hWSN2W6dOkCc3NzBAUFKbze48eP49atWxgwYEC1Y3z5Pefz+ejQoQMAvPZ7SkhjRCUOCCE17vfff1e6lmbq1KlYsmQJQkND0aNHD3z++efQ0tLCxo0bUVRUhBUrVnB927Rpg169esHNzQ0mJia4ePEi/vjjD3z55ZcAgDt37qBv374YMmQI2rRpAy0tLfz5559IT0/HsGHDXvs1jB8/HsuXL8f48ePRpUsXnDt3Dnfu3FF7vBYtWuDbb7/F999/j549e2LQoEEQCoWIjo6GtbU1AgMDAQBubm749ddfsWTJErRo0QLm5ubo06dPhfG0tbXxww8/YOzYsfDx8cHw4cO5EgcODg74+uuv1XrN2dnZ6NOnD2xsbHD//n38/PPPcHV1RevWrdV+7YQ0Wpq+PZAQ0niUlzio7OvBgweMMcZiYmKYn58fMzAwYHp6eqx3797swoULCmMtWbKEubu7M2NjY6arq8tcXFzY0qVLWXFxMWOMsaysLPbFF18wFxcXpq+vz8RiMfPw8GD79u17ZZzlZQkyMzMr7ZOfn8/GjRvHxGIxMzQ0ZEOGDGEZGRmVljh4eazy9yIxMVGh/ffff2edOnViQqGQNWnShPn4+LDQ0FDueYlEwgYMGMAMDQ0ZAK7cwcslDsrt3buXG8/ExISNHDmSPXz4UKHP6NGjmb6+fqXvQ7k//viDvfXWW8zc3Jzp6OgwOzs7NmnSJJaWllbp+0TIm4zHWD1Z9UgIIYQQ0oDQmihCCCGEEDVQEkUIIYQQogZKogghhBBC1EBJFCGEEEKIGiiJIoQQQghRAyVRhBBCCCFqoGKbtUgulyM1NRWGhoZq7dJOCCGEkLrHGMOTJ09gbW0NPr/y+SZKompRamqqwj5ghBBCCGk4Hjx4ABsbm0qfpySqFhkaGgIo+0so3w+MEEIIIfVbbm4ubG1tuc/xSmm4YjpjjLH169cze3t7JhQKmbu7O4uMjKyy/759+1irVq2YUChk7dq1Y3///bfC83K5nC1YsIBZWloykUjE+vbty+7cuaPQ57333mO2trZMKBQyS0tL9vHHH7OUlBSFPlevXmU9evRgQqGQ2djYsB9++KFar0sqlTIATCqVVus4QgghhGiOqp/fGl9YvnfvXkyfPh0BAQGIiYlBx44d4efnh4yMDKX9L1y4gOHDh2PcuHG4fPky/P394e/vj9jYWK7PihUrsG7dOgQFBSEyMhL6+vrw8/NT2PG9d+/e2LdvH27fvo0DBw7g7t27+Oijj7jnc3Nz8dZbb8He3h6XLl3CypUrsWjRImzatKn23gxCCCGENBx1lNRVyt3dnX3xxRfcY5lMxqytrVlgYKDS/kOGDGEDBgxQaPPw8GCTJk1ijJXNQllaWrKVK1dyz+fk5DChUMh2795daRyHDx9mPB6P29z0l19+YU2aNGFFRUVcn9mzZ7NWrVqp/NpoJooQQghpeBrETFRxcTEuXboEX19fro3P58PX1xcRERFKj4mIiFDoDwB+fn5c/8TEREgkEoU+YrEYHh4elY6ZnZ2NnTt3wsvLC9ra2tx5vL29oaOjo3Ce27dv4/Hjx0rHKSoqQm5ursIXIYQQQhonjS4sz8rKgkwmg4WFhUK7hYUF4uLilB4jkUiU9pdIJNzz5W2V9Sk3e/ZsrF+/Hvn5+ejWrRuOHj2qcB5HR8cKY5Q/16RJkwqxBQYGYvHixZW+3srIZDKUlJRU+zhCCCGkKtra2hAIBJoOo9F6o+/O++abbzBu3Djcv38fixcvxqhRo3D06FG1azrNnTsX06dP5x6Xr+6vDGMMEokEOTk5ap2PEEIIeRVjY2NYWlpSvcJaoNEkytTUFAKBAOnp6Qrt6enpsLS0VHqMpaVllf3L/0xPT4eVlZVCH1dX1wrnNzU1RcuWLdG6dWvY2triv//+g6enZ6XnefEcLxMKhRAKha941c+VJ1Dm5ubQ09OjH3BCCCE1hjGG/Px87katFz8TSc3QaBKlo6MDNzc3hIWFwd/fH0BZle+wsDB8+eWXSo/x9PREWFgYpk2bxrWFhobC09MTAODo6AhLS0uEhYVxSVNubi4iIyMxefLkSmORy+UAytY1lZ/n22+/RUlJCbdOKjQ0FK1atVJ6Ka+6ZDIZl0A1bdr0tccjhBBCXqarqwsAyMjIgLm5OV3aq2EaL3Ewffp0bN68Gdu2bcOtW7cwefJk5OXlYezYsQCAUaNGYe7cuVz/qVOnIiQkBKtXr0ZcXBwWLVqEixcvckkXj8fDtGnTsGTJEhw5cgTXr1/HqFGjYG1tzSVqkZGRWL9+Pa5cuYL79+/j1KlTGD58OJycnLhkbMSIEdDR0cG4ceNw48YN7N27Fz/99JPC5brXUb4GSk9Pr0bGI4QQQpQp/5yhtbc1T+NrooYOHYrMzEwsXLgQEokErq6uCAkJ4RZxJycnK+xb4+XlhV27dmH+/PmYN28enJ2dcejQIbRr147rM2vWLOTl5WHixInIyclBjx49EBISApFIBKDsB+rgwYMICAhAXl4erKys0L9/f8yfP5+7HCcWi3HixAl88cUXcHNzg6mpKRYuXIiJEyfW6OunS3iEEEJqE33O1B4eY4xpOojGKjc3F2KxGFKptMK2L4WFhUhMTISjoyOX3BFCCCE1jT5vqq+qz+8XafxyHiEv69Wrl8KaNwcHB6xdu1Zj8dSkQ4cOoUWLFhAIBNxrfLktODgYxsbGKo/ZmN6f2ubt7Y1du3ZpOowaUdN/7zdv3oSNjQ3y8vJqbMzacPv2bVhaWuLJkyeaDgVAxd9XNdVXFVlZWTA3N8fDhw9rbExSPZREkWrJzMzE5MmTYWdnB6FQCEtLS/j5+SE8PLzWzhkdHV3jl1E1ZdKkSfjoo4/w4MEDfP/990rbhg4dijt37qg8Zm28PzX9y74yBw8exFtvvYWmTZuCx+PhypUrKh23f/9+uLi4QCQSoX379jh27Ngrjzly5AjS09MxbNgwleOrLKGty8S1shhq+u+9TZs26NatG3788cdqH5udnY2RI0fCyMgIxsbGGDduHJ4+fVpl/ylTpqBVq1bQ1dWFnZ0dvvrqK0il0leea+7cuZgyZcqrN4athw4ePMj9u68JpqamGDVqFAICAmpszIYkTVqAC3ezkCYt0FgMlESRavnwww9x+fJlbNu2DXfu3MGRI0fQq1cvPHr0qNbOaWZm1igW4D99+hQZGRnw8/ODtbU1DA0Nlbbp6urC3Nxc5XEb8vuTl5eHHj164IcfflD5GFX2z1Rm3bp1GDt2rMIay9okk8m4u35rQ238vY8dOxa//vorSktLq3XcyJEjcePGDYSGhuLo0aM4d+5clQleamoqUlNTsWrVKsTGxiI4OBghISEYN25cledJTk7G0aNHMWbMmEr71Pb7/jpMTExqPPkbO3Ysdu7ciezs7Bodt77bG52M7stPYcTmSHRffgp7o5M1E0gdbEHzxqpq752CggJ28+ZNVlBQ8NrnSc3JZ+EJmSw1J/+1x6rK48ePGQB25syZV/abOHEiMzc3Z0KhkLVt25b99ddfjDHGsrKy2LBhw5i1tTXT1dVl7dq1Y7t27VI43sfHh02dOpV7bG9vz9asWcM9BsA2b97M/P39ma6uLmvRogU7fPiwwhiHDx9mLVq0YEKhkPXq1YsFBwczAOzx48dqxc0YY3/88Qdr06YN09HRYfb29mzVqlUKxxcWFrIZM2Ywa2trpqenx9zd3dnp06cZY4ydPn2aAVD4qqxt69atTCwWK4x95MgR1qVLFyYUClnTpk2Zv79/pe/P48eP2bhx45ipqSkzNDRkvXv3ZleuXOGeDwgIYB07dmTbt29n9vb2zMjIiA0dOpTl5uYyxhgbPXp0hbgSExMrfd9qQmJiIgPALl++/Mq+r9o/U5mMjAzG4/FYbGysQvvq1atZu3btmJ6eHrOxsWGTJ09mT548YYwp/zsLCAhgPj4+FdoZY9zf2+HDh1nr1q2ZQCBgiYmJLCoqivn6+rKmTZsyIyMj5u3tzS5duqQQR2U/e5XFwJji3/vw4cPZkCFDFMYsLi5mTZs2Zdu2bWOMle1LumzZMubg4MBEIhHr0KED279/v8IxRUVFTCgUspMnT77y76HczZs3GQAWHR3NtR0/fpzxeDyWkpKi8jj79u1jOjo6rKSkpNI+K1euZF26dFFoq+x9r+rf44vH/fnnn9zvirfeeoslJydzfUaPHs0GDhyocL6pU6cyHx8f7vHLv682bNjAjWdubs4+/PBDpX3nzp3L3N3dK7zGDh06sMWLF3OPN2/ezFxcXJhQKGStWrViGzZsqHCMo6Mj27JlS6XvW01+3miaXC5nR6+mMPvZRxW+ms/5u0Y/AxvE3nnkOcYY8otLq/31v4gkhWz8fxFJ1R6DqXhvgYGBAQwMDHDo0CGuntbL5HI53n77bYSHh2PHjh24efMmli9fztUmKSwshJubG/7++2/ExsZi4sSJ+OSTTxAVFVWt92vx4sUYMmQIrl27hnfeeQcjR47k/ieWmJiIjz76CP7+/rh69SomTZqEb7/9tsrxXhX3pUuXMGTIEAwbNgzXr1/HokWLsGDBAgQHB3NjfPnll4iIiMCePXtw7do1DB48GP3790d8fDy8vLxw+/ZtAMCBAweQlpZWadvL/v77b3zwwQd45513cPnyZYSFhcHd3b3S1zJ48GBkZGTg+PHjuHTpEjp37oy+ffsq/E/17t27OHToEI4ePYqjR4/i7NmzWL58OQDgp59+gqenJyZMmIC0tDSkpaVVWnn/s88+434uKvuqaa/aP1OZ8+fPQ09PD61bt1Zo5/P5WLduHW7cuIFt27bh1KlTmDVrFoCyO4HXrl0LIyMj7n2YOXMmDh48CBsbG3z33Xdce7n8/Hz88MMP2LJlC27cuAFzc3M8efIEo0ePxvnz5/Hff//B2dkZ77zzDremp6qfvcpieNnIkSPx119/KVxC++eff5Cfn48PPvgAQNm2VNu3b0dQUBBu3LiBr7/+Gh9//DHOnj3LHaOjowNXV1f8+++/qv51ICIiAsbGxujSpQvX5uvrCz6fj8jISJXHKV/Aq6VV+U3j//77r8J5yil736v69/jicUuXLsX27dsRHh6OnJycal3ufdnFixfx1Vdf4bvvvsPt27cREhICb29vpX1HjhyJqKgo3L17l2u7ceMGrl27hhEjRgAAdu7ciYULF2Lp0qW4desWli1bhgULFmDbtm0KY7m7u1fr76whys4rxuZz99B39Vl8setyhedljCEpK7/O49J4iQNSpqBEhjYL/3mtMeQMWHD4BhYcvlGt425+5wc9nVf/KGhpaSE4OBgTJkxAUFAQOnfuDB8fHwwbNgwdOnQAAJw8eRJRUVG4desWWrZsCQBo3rw5N0azZs0UPgSmTJmCf/75B/v27asyMXjZmDFjMHz4cADAsmXLsG7dOkRFRaF///7YuHEjWrVqhZUrVwIAWrVqhdjYWCxdurTS8V4V948//oi+fftiwYIFAICWLVvi5s2bWLlyJcaMGYPk5GRs3boVycnJsLa2BgDMnDkTISEh2Lp1K5YtW8ZdojMxMeGq3itre9nSpUsxbNgwhX0ZO3bsqLTv+fPnERUVhYyMDK5cx6pVq3Do0CH88ccf3CUWuVyO4OBg7tLCJ598grCwMCxduhRisRg6OjrQ09OrNKZy3333ndIP9dr0qv0zlbl//z4sLCwqXMp7+QaGJUuW4LPPPsMvv/wCHR0diMVi8Hi8Cu+DQCCAoaFhhfaSkhL88ssvCn8/ffr0UeizadMmGBsb4+zZs3j33Xdf+bNXWQwv8vPzg76+Pv7880988sknAIBdu3bh/fffh6GhIYqKirBs2TKcPHmSq4XXvHlznD9/Hhs3boSPjw83lrW1Ne7fv1/puV4mkUgqXH7W0tKCiYlJlX8nL8rKysL333//yjVe9+/fV5pEvfy+q/Lvsfy49evXw8PDAwCwbds2tG7dGlFRUdX6fVQuOTkZ+vr6ePfdd2FoaAh7e3t06tRJad+2bduiY8eO2LVrF/d7ZefOnfDw8ECLFi0AAAEBAVi9ejUGDRoEoKyY9M2bN7Fx40aMHj2aG8va2hqXL1dMLBo6xhgi7j3C7qgH+CdWgmJZ2WVaXW0+CkoUL9kKeDw4mNb9sgZKoki1fPjhhxgwYAD+/fdf/Pfffzh+/DhWrFiBLVu2YMyYMbhy5QpsbGy4D4OXyWQyLFu2DPv27UNKSgqKi4tRVFRU7bUd5UkbAOjr68PIyIjb2uD27dvo2rWrQv9X/UJ8Vdy3bt3CwIEDFdq6d++OtWvXQiaT4fr165DJZBWOLyoqeu2K9FeuXMGECRNU6nv16lU8ffq0wjkLCgoU/sfr4OCgsDbDysqKe/+qw9zcvFrrtzSloKBA6a3dJ0+eRGBgIOLi4pCbm4vS0lIUFhYiPz9frfVGOjo6Cj+bQNl2UfPnz8eZM2eQkZEBmUyG/Px8JCeXreF41c+eKrS0tDBkyBDs3LkTn3zyCfLy8nD48GHs2bMHAJCQkID8/Hz069dP4bji4uIKH/K6urrIz6+7/9Hn5uZiwIABaNOmDRYtWlRl38r+Hl9+31X996ilpaXwu8LFxQXGxsa4deuWWklUv379YG9vj+bNm6N///7o378/Pvjgg0p/lkaOHInff/8dCxYsAGMMu3fv5go65+Xl4e7duxg3bpzCv//S0lKIxWKFcer676y2ZT0twoFLD7En+gESs57fLdrBRozh7nZ4r6M1/r6WinkHYyFjDAIeD8sGtYOVWLfOY6Ukqp7Q1Rbg5nd+1TpGIi2E749nIX/hahyfB5yc7gNLseq1QHS1q7cNgEgkQr9+/dCvXz8sWLAA48ePR0BAAMaMGcNtMVCZlStX4qeffsLatWvRvn176OvrY9q0aSguLq5WDOVb8ZTj8XivtZj0VXG/ytOnTyEQCHDp0qUK2yq87iWt6sT29OlTWFlZ4cyZMxWee/EOr5p6/z777DPs2LHjlTHVpFftn6mMqakpHj9+rNCWlJSEd999F5MnT8bSpUthYmKC8+fPY9y4cSguLlYridLV1a1Q2HD06NF49OgRfvrpJ9jb20MoFMLT05P7mX/dn71yI0eOhI+PDzIyMhAaGgpdXV30798fwPO/g7///hvNmjVTOO7l/T6zs7Ph5OSk8nktLS0rJOClpaXIzs5+5UzmkydP0L9/fxgaGuLPP/+s8HP5MmV/j0DF972m/j3y+fwKyx2qqvptaGiImJgYnDlzBidOnMDChQuxaNEiREdHK73Dcvjw4Zg9ezZiYmJQUFCABw8eYOjQodxrAIDNmzdzM2XlXn5N2dnZMDMzU/l11Udyedms066oZJy4IUGJrOx9NxBqYaCrNYa726Fds+fJ49CudvBuaYakrHw4mOppJIECKImqN3g8nkqX1F7U3MwAgYPaV8jGm5vV/DqUqrRp0waHDh0CUDZD9PDhQ9y5c0fp/6zDw8MxcOBAfPzxxwDKLivduXMHbdq0qbF4WrVqVeGW9+jo6CqPeVXcrVu3rlDGITw8HC1btoRAIECnTp0gk8mQkZGBnj17vv6LeCm2sLAwbiukqnTu3BkSiQRaWlpwcHBQ+5w6OjqQyWSv7KeJy3mv2j9TmU6dOkEikeDx48fc3peXLl2CXC7H6tWruct8+/btUziusvdB1fcHKPs5+eWXX/DOO+8AAB48eICsrCzu+Vf97Kl6Li8vL9ja2mLv3r04fvw4Bg8ezCUlbdq0gVAoRHJyssKlO2ViY2Px0UcfqfTagLK/j5ycHFy6dAlubm4AgFOnTkEul1f48H9Rbm4u/Pz8IBQKceTIEZWKQHbq1Ak3b95UqZ8q/x5LS0tx8eJFbtbp9u3byMnJ4dbOmZmZVbjr88qVK1Ume1paWvD19YWvry8CAgJgbGyMU6dOcZfkXmRjYwMfHx/s3LkTBQUF6NevHzeza2FhAWtra9y7dw8jR46s8vXGxsaiV69eVfaprzKfFOGPSw+xJzoZ9x89n03raGuMEe62eLeDNfSFyj8brcS6GkueylES1cDVZTb+6NEjDB48GJ9++ik6dOgAQ0NDXLx4EStWrOAudfn4+MDb2xsffvghfvzxR7Ro0QJxcXHg8Xjo378/nJ2d8ccff+DChQto0qQJfvzxR6Snp9doEjVp0iT8+OOPmD17NsaNG4crV65wC8Ar2/7gVXHPmDEDXbt25eo4RUREYP369fjll18AlK2RGjlyJEaNGoXVq1ejU6dOyMzMRFhYGDp06IABAwao/XoCAgLQt29fODk5YdiwYSgtLcWxY8cwe/bsCn19fX3h6ekJf39/rFixAi1btkRqaiq3OF3ZehJlHBwcEBkZiaSkJBgYGMDExERpaYDXvZyXnZ2N5ORkpKamAgC30N7S0pKbxRg1ahSaNWuGwMBAAGX7Z/r4+GD16tUYMGAA9uzZg4sXL2LTpk2VnqdTp04wNTVFeHg43n33XQBAixYtUFJSgp9//hnvvfcewsPDERQUVOF9ePr0KcLCwtCxY0fo6elBT08PDg4OOHfuHIYNGwahUAhTU9NKz+3s7Iz//e9/6NKlC3Jzc/HNN98ozD696mevshiUGTFiBIKCgnDnzh2cPn2aazc0NMTMmTPx9ddfQy6Xo0ePHpBKpQgPD4eRkRG3viYpKQkpKSkVFu5XpXXr1ujfvz+3VrKkpARffvklhg0bxq1HSklJQd++fbF9+3a4u7sjNzcXb731FvLz87Fjxw7k5uYiNzcXQFniUtkmuX5+fhg/fjxkMlmVG+mq+u9RW1sbU6ZMwbp166ClpYUvv/wS3bp145KqPn36YOXKldi+fTs8PT2xY8cOxMbGVrrO6ejRo7h37x68vb3RpEkTHDt2DHK5HK1atao01pEjRyIgIADFxcVYs2aNwnOLFy/GV199BbFYjP79+6OoqAgXL17E48ePuct++fn5uHTpErfOqyGQyxnC72Zhd1QyTtxIR+mzyymGQi34d2qGYe62aGstfsUo9USN3Q9IKqirEgd1pbCwkM2ZM4d17tyZicVipqenx1q1asXmz5/P8vOf31r66NEjNnbsWNa0aVMmEolYu3bt2NGjR7nnBg4cyAwMDJi5uTmbP38+GzVqlMJtxKqUOPjzzz8VYhOLxWzr1q3c45dLHPz6668MQJXvd1VxM/a8xIG2tjazs7NjK1euVDi+uLiYLVy4kDk4ODBtbW1mZWXFPvjgA3bt2jXG2PMSES/eZq2sTVmJgwMHDjBXV1emo6PDTE1N2aBBgyp9f3Jzc9mUKVOYtbU109bWZra2tmzkyJHcrdvlJQ5etGbNGmZvb889vn37NuvWrRvT1dWt1RIHW7durXALP164jZ+xsp+H0aNHKxy3b98+1rJlS6ajo8Patm3L/v7771eea9asWWzYsGEKbT/++COzsrJiurq6zM/Pj23fvr1CKYzPPvuMNW3aVCGuiIgI1qFDByYUCiuUOHhZTEwM69KlCxOJRMzZ2Znt37+/wt/Zq372lMXw8hiMPS83YG9vz+RyucJzcrmcrV27lrVq1Yppa2szMzMz5ufnx86ePcv1WbZsGfPz81M4LiAgQOFnQ5lHjx6x4cOHMwMDA2ZkZMTGjh3LlYpg7HkJi6pKfpR/VfWzVlJSwqytrVlISAjXVtn7/qp/j+XHHThwgDVv3pwJhULm6+vL7t+/rzDOwoULmYWFBROLxezrr79mX375ZaUlDv7991/m4+PDmjRpwnR1dVmHDh3Y3r17lfYt9/jxYyYUCpmenp7Ce1Zu586d3L/9Jk2aMG9vb3bw4EHu+V27drFWrVpV+p4xVn8+b9JzC9j6U/Gsxw9hCuUJ/DecZ3ujk1leUeXlLeqaqiUOaO+8WkR759UfS5cuRVBQEB48eKDpUIiGSCQStG3bFjExMbC3t9d0OPVOcXExnJ2dsWvXLnTv3p1rHz16NHg8nkI5D03asGEDjhw5gn/+eb27mYODgzFt2jTk5OTUTGAa0q1bN3z11VdcWQRlNPl5I5cz/JuQhd2RyTh564VZJ5EWBnVqhmHudmhtVfnedJqi6t55dDmPNEq//PILunbtiqZNmyI8PBwrV67El19+qemwiAZZWlrit99+Q3JyMiVRSiQnJ2PevHkKCRRjDGfOnMH58+c1GJmiSZMmIScnB0+ePGmQW7/UpKysLAwaNIgr91KfpOcWYv/FB9gT/QAPHz/flsXNvgmGu9thQHsr6OpU76am+oiSKNIoxcfHY8mSJcjOzoadnR1mzJiBuXPnajosomH+/v6aDqHeatGiBVefqByPx6tWzai6oKWl9criuW8KU1NTrjhsfSCTM5yLz8TuyGSExWVA9mzWyUikhUGdbTDc3Q6tLBtX4kuX82oRXc4jhBCiabX9eSORFmLfxQfYG/0AKTnPZ526OpTNOr3T3gqiapbS0TS6nEcIIYSQWiGTM5y9k4FdkQ9wKi6dq1co1tXGh51tMNzdFs4WjWvWSRlKojSMJgIJIYTUppr8nEnNKcC+iw+wL/oBUqWFXLu7owmGu9vi7XYNb9bpdVASpSHlxdry8/NrrGIxIYQQ8rLyLWFeVRG+MqUyOc7czsTuqGScvp3BzToZ62njo842GOZuixbmjX/WSRlKojREIBDA2NiY2y5BT0+v0kKQhBBCSHUxxpCfn4+MjAwYGxtXWaBUmZScAuyNLpt1kuQ+n3Xq1twEw93t4NfW8o2adVKGkigNKq/IrM7Gr4QQQogqjI2NX7mPYblSmRyn4jKwOyoZZ+5kovxKYBM9bXzkZoNh7nZwquOtxeozSqI0iMfjwcrKCubm5lVuakkIIYSoQ1tbW6UZqAfZ+WVrnS4+QHpuEdfu2bwphnvYwa+tBYRab/askzKURNUDAoGg2tOshBBCyOsokckRdqts1ulc/PNZJxN9HQx2s8HQrrZ1vqF9Q0NJFCGEEPIGeZCdjz3Rydh38SEynzyfdereoimGu9uhXxuadVIVJVGEEEJII1cik+PkzXTsikrG+YQsbtbJ1EAHH7nZYlhXWziY6ms2yAaIkihCCCGkkUp+lI/d0cnYf/Ehsp4+n3Xq6WyK4e528G1tAR0tvgYjbNgoiSKEEEIakeJSOUJvpmNPdDL+jc/i2k0NhBjSxQbDutrBrqmeBiNsPCiJIoQQQhqBpKw87I5Oxh8XH+JRXjEAgMcDejqbYYS7Lfq2toC2gGadahIlUYQQQkgDVVQqw4kbZbNO4QmPuHYzQyGGdrHF0K62sDWhWafaQkkUIYQQ0sDcy3yKPdEP8Melh8h+YdbJp6UZhrvboY+LOc061QFKogghhJAGoKhUhpBYCfZEPUDEveezThZGZbNOQ7rawqYJzTrVJUqiCCGEkHrsbuZT7I5MxoGYh3icX7a7BY8H9G5ljuHudujdygxaNOukEZREEUIIIRqWJi1AYlYeHE31YSXWRWFJ2azT7qhkRCZmc/0sjUQY2rVs1qmZsa4GIyYAJVGEEEKIRu2NTsbcg9chZwCfB3g5mSI2VYqcZ7NOfB7Qx6Vs1smnJc061SeURBFCCCEakiYt4BIoAJAz4HxCWW0na7EIQ7vaYUhXG1iJadapPqIkihBCCNGQxKw8LoF60Sy/lpjk0wICPq/ugyIqqxdzghs2bICDgwNEIhE8PDwQFRVVZf/9+/fDxcUFIpEI7du3x7FjxxSeZ4xh4cKFsLKygq6uLnx9fREfH889n5SUhHHjxsHR0RG6urpwcnJCQEAAiouLFcb5559/0K1bNxgaGsLMzAwffvghkpKSaux1E0IIebNpKUmSBDwePuhsQwlUA6DxJGrv3r2YPn06AgICEBMTg44dO8LPzw8ZGRlK+1+4cAHDhw/HuHHjcPnyZfj7+8Pf3x+xsbFcnxUrVmDdunUICgpCZGQk9PX14efnh8LCQgBAXFwc5HI5Nm7ciBs3bmDNmjUICgrCvHnzuDESExMxcOBA9OnTB1euXME///yDrKwsDBo0qHbfEEIIIW+E1JwCfL33qkKbgMfDskHt6PJdA8FjjCmZSKw7Hh4e6Nq1K9avXw8AkMvlsLW1xZQpUzBnzpwK/YcOHYq8vDwcPXqUa+vWrRtcXV0RFBQExhisra0xY8YMzJw5EwAglUphYWGB4OBgDBs2TGkcK1euxK+//op79+4BAP744w8MHz4cRUVF4PPLcs2//voLAwcORFFREbS1tV/52nJzcyEWiyGVSmFkZFS9N4YQQkijlfW0CEM2RuBeZtkdeeuGu+JpoQwOpnqUQNUDqn5+a3Qmqri4GJcuXYKvry/Xxufz4evri4iICKXHREREKPQHAD8/P65/YmIiJBKJQh+xWAwPD49KxwTKEi0TExPusZubG/h8PrZu3QqZTAapVIr//e9/8PX1rTSBKioqQm5ursIXIYQQ8iJpfgk++S0K9zLzYC0WYcd4D7RvZgxPp6aUQDUwGk2isrKyIJPJYGFhodBuYWEBiUSi9BiJRFJl//I/qzNmQkICfv75Z0yaNIlrc3R0xIkTJzBv3jwIhUIYGxvj4cOH2LdvX6WvJzAwEGKxmPuytbWttC8hhJA3T15RKcYGR+FWWi5MDYTYOaEb1XtqwDS+JkrTUlJS0L9/fwwePBgTJkzg2iUSCSZMmIDRo0cjOjoaZ8+ehY6ODj766CNUdgV07ty5kEql3NeDBw/q6mUQQgip5wpLZJj4v4uISc6BkUgL/xvnDkdTfU2HRV6DRkscmJqaQiAQID09XaE9PT0dlpaWSo+xtLSssn/5n+np6bCyslLo4+rqqnBcamoqevfuDS8vL2zatEnhuQ0bNkAsFmPFihVc244dO2Bra4vIyEh069atQmxCoRBCofAVr5oQQsibpkQmx5e7LiM84RH0dATY9qk7WlvRWtmGTqMzUTo6OnBzc0NYWBjXJpfLERYWBk9PT6XHeHp6KvQHgNDQUK6/o6MjLC0tFfrk5uYiMjJSYcyUlBT06tULbm5u2Lp1K7d4vFx+fn6FNoFAwMVICCGEqEIuZ5i5/ypO3kqHjhYfW0Z3QSe7JpoOi9QAjV/Omz59OjZv3oxt27bh1q1bmDx5MvLy8jB27FgAwKhRozB37lyu/9SpUxESEoLVq1cjLi4OixYtwsWLF/Hll18CAHg8HqZNm4YlS5bgyJEjuH79OkaNGgVra2v4+/sDeJ5A2dnZYdWqVcjMzIREIlFYMzVgwABER0fju+++Q3x8PGJiYjB27FjY29ujU6dOdfcGEUIIabAYY5h/OBaHr6RCi8/DryM7w8vJVNNhkRqi8YrlQ4cORWZmJhYuXAiJRAJXV1eEhIRwC8OTk5MVZoS8vLywa9cuzJ8/H/PmzYOzszMOHTqEdu3acX1mzZqFvLw8TJw4ETk5OejRowdCQkIgEokAlM1cJSQkICEhATY2NgrxlK936tOnD3bt2oUVK1ZgxYoV0NPTg6enJ0JCQqCrS4sACSGEVI0xhsDjcdgVmQweD1gz1BV9W1u8+kDSYGi8TlRjRnWiCCHkzfVzWDxWh94BAPzwYXsM7Wqn4YiIqhpEnShCCCGkMfr9fCKXQC14tw0lUI0UJVGEEEJIDdoX/QDfHb0JAJjm64xxPRw1HBGpLZREEUIIITXk72tpmHPwGgBgfA9HTO3rrOGISG2iJIoQQgipAafjMjBt72XIGTCsqy2+HdAaPB5P02GRWkRJFCGEEPKa/rv3CJ/tuIQSGcN7Ha2x9IP2lEC9ASiJIoQQQl7DlQc5GBccjaJSOXxbm+PHIR0h4FMC9SagJIoQQghRU5wkF6N/j0JesQxeTk2xfkRnaAvoo/VNQX/ThBBCiBoSs/Lw8ZYoSAtK4GprjM2jukCkLdB0WKQOURJFCCGEVFNqTgE+3hKJrKdFcLE0RPDYrtAXanwTEFLHKIkihBBCqiHzSRE+3hKJlJwCOJrq43/jPGCsp6PpsIgGUBJFCCGEqEiaX4JRv0fhXlYemhnrYsd4D5gZCjUdFtEQSqIIIYQQFeQVlWJMcBRupeXC1ECIHeM90MyYNqR/k1ESRQghhLxCYYkME7ZfxOXkHIh1tbFjvDscTfU1HRbRMEqiCCGEkCqUyOT4clcMLtx9BH0dAbZ96g4XSyNNh0XqAUqiCCGEkErI5Awz9l3FyVsZEGrxsWV0V7jaGms6LFJPUBJFCCGEKMEYw/xD13Hkaiq0+Dz8+nFneDo11XRYpB6hJIoQQgh5CWMMy47dwu6oB+DzgLXDXNHHxULTYZF6hpIoQggh5CU/n0rA5n8TAQDLB3XAux2sNRwRqY8oiSKEEEJe8Nv5RPwYegcAsODdNhjS1VbDEZH6ipIoQggh5Jm90cn4/uhNAMD0fi0xroejhiMi9RklUYQQQgiAv66mYs7B6wCACT0dMaVPCw1HROo7SqIIIYS88U7FpePrvVfAGDDc3Rbz3mkNHo+n6bBIPUdJFCGEkDdaxN1HmLwjBqVyhvc7WmOJf3tKoIhKKIkihBDyxrryIAfjt0WjqFQO39bmWD2kIwR8SqCIaiiJIoQQ8kaKk+Ri9O9RyCuWwcupKdaP6AxtAX0sEtXRTwshhJA3TmJWHj7eEgVpQQk62Rlj86guEGkLNB0WaWAoiSKEEPJGSckpwMdbIpH1tAitrYwQPMYd+kItTYdFGiBKogghhLwxMp8U4eMtkUjJKUBzU338b5w7xHramg6LNFCUehNCCGn00qQFiH2Yi+Uht5CYlYdmxrrYMd4DpgZCTYdGGjBKogghhDRqe6OTMffgdchZ2WMDoRZ2jveAtbGuZgMjDR5dziOEENJopUkLFBIoAMgvLoVQmz7+yOujnyJCCCGNVmJWnkICBQByBiRl5WsmINKoUBJFCCGk0XI01cfLtTMFPB4cTPU0ExBpVCiJIoQQ0mhZiXUx753W3GM+D1g2qB2sxLQeirw+WlhOCCGkUWtrLQYAmBsKcfjL7pRAkRpTL2aiNmzYAAcHB4hEInh4eCAqKqrK/vv374eLiwtEIhHat2+PY8eOKTzPGMPChQthZWUFXV1d+Pr6Ij4+nns+KSkJ48aNg6OjI3R1deHk5ISAgAAUFxdXGGfVqlVo2bIlhEIhmjVrhqVLl9bcCyeEEFLrEjKeAADaNRNTAkVqlMaTqL1792L69OkICAhATEwMOnbsCD8/P2RkZCjtf+HCBQwfPhzjxo3D5cuX4e/vD39/f8TGxnJ9VqxYgXXr1iEoKAiRkZHQ19eHn58fCgsLAQBxcXGQy+XYuHEjbty4gTVr1iAoKAjz5s1TONfUqVOxZcsWrFq1CnFxcThy5Ajc3d1r780ghBBS4xIyngIAnM0NNBwJaXSYms6dO8dGjhzJunXrxh4+fMgYY2z79u3s33//rdY47u7u7IsvvuAey2QyZm1tzQIDA5X2HzJkCBswYIBCm4eHB5s0aRJjjDG5XM4sLS3ZypUruedzcnKYUChku3fvrjSOFStWMEdHR+7xzZs3mZaWFouLi6vW63mRVCplAJhUKlV7DEIIIa9n+KYIZj/7KNsXnazpUEgDoernt1ozUQcOHICfnx90dXVx+fJlFBUVAQCkUimWLVum8jjFxcW4dOkSfH19uTY+nw9fX19EREQoPSYiIkKhPwD4+flx/RMTEyGRSBT6iMVieHh4VDpmeewmJibc47/++gvNmzfH0aNH4ejoCAcHB4wfPx7Z2dmVjlFUVITc3FyFL0IIIZoV/2wmqgXNRJEaplYStWTJEgQFBWHz5s3Q1n6+51D37t0RExOj8jhZWVmQyWSwsLBQaLewsIBEIlF6jEQiqbJ/+Z/VGTMhIQE///wzJk2axLXdu3cP9+/fx/79+7F9+3YEBwfj0qVL+Oijjyp9PYGBgRCLxdyXra1tpX0JIYTUPml+CTKflP1Hn5IoUtPUSqJu374Nb2/vCu1isRg5OTmvG1OdSklJQf/+/TF48GBMmDCBa5fL5SgqKsL27dvRs2dP9OrVC7/99htOnz6N27dvKx1r7ty5kEql3NeDBw/q6mUQQghRIiGzbFG5pZEIhiLaaJjULLWSKEtLSyQkJFRoP3/+PJo3b67yOKamphAIBEhPT1doT09Ph6WlZaXnrqp/+Z+qjJmamorevXvDy8sLmzZtUnjOysoKWlpaaNmyJdfWunVZrZHk5GSlsQmFQhgZGSl8EUII0RxuUbkFzUKRmqdWEjVhwgRMnToVkZGR4PF4SE1Nxc6dOzFz5kxMnjxZ5XF0dHTg5uaGsLAwrk0ulyMsLAyenp5Kj/H09FToDwChoaFcf0dHR1haWir0yc3NRWRkpMKYKSkp6NWrF9zc3LB161bw+YpvRffu3VFaWoq7d+9ybXfu3AEA2Nvbq/waCSGEaE58Oq2HIrVInVXrcrmcLVmyhOnr6zMej8d4PB4TiURs/vz51R5rz549TCgUsuDgYHbz5k02ceJEZmxszCQSCWOMsU8++YTNmTOH6x8eHs60tLTYqlWr2K1bt1hAQADT1tZm169f5/osX76cGRsbs8OHD7Nr166xgQMHMkdHR1ZQUMAYY+zhw4esRYsWrG/fvuzhw4csLS2N+yonk8lY586dmbe3N4uJiWEXL15kHh4erF+/fiq/Nro7jxBCNGvUb5HMfvZRtuO/JE2HQhoQVT+/1apYzuPx8O233+Kbb75BQkICnj59ijZt2sDAoPqZ/tChQ5GZmYmFCxdCIpHA1dUVISEh3MLw5ORkhVkiLy8v7Nq1C/Pnz8e8efPg7OyMQ4cOoV27dlyfWbNmIS8vDxMnTkROTg569OiBkJAQiEQiAGUzVwkJCUhISICNjY1CPIyV7VTJ5/Px119/YcqUKfD29oa+vj7efvttrF69utqvkRBCiGY8rxFlqOFISGPEY+VZQzXs2LEDgwYNgp4ebeBYldzcXIjFYkilUlofRQghdSyvqBRtA/4BAFxe0A9N9HU0HBFpKFT9/FZrTdTXX38Nc3NzjBgxAseOHYNMJlM7UEIIIaQ23MvMAwA01dehBIrUCrWSqLS0NOzZswc8Hg9DhgyBlZUVvvjiC1y4cKGm4yOEEELUEv9szzxaVE5qi1pJlJaWFt59913s3LkTGRkZWLNmDZKSktC7d284OTnVdIyEEEJItVGlclLb1FpY/iI9PT34+fnh8ePHuH//Pm7dulUTcRFCCCGvhTYeJrVNrZkoAMjPz8fOnTvxzjvvoFmzZli7di0++OAD3LhxoybjI4QQQtTyvNAm3ZlHaodaM1HDhg3D0aNHoaenhyFDhmDBggWVFsckhBBC6lpRqQz3H5UtLKfLeaS2qJVECQQC7Nu3D35+fhAIBDUdEyGEEPJaErPyIGeAoUgL5oZCTYdDGim1kqidO3fWdByEEEJIjSnf7sXZ3AA8Hk/D0ZDGSuUkat26dZg4cSJEIhHWrVtXZd+vvvrqtQMjhBBC1JVAd+aROqByErVmzRqMHDkSIpEIa9asqbQfj8ejJIoQQohG0XYvpC6onEQlJiYq/Z4QQgipb2gmitQFtUscvEgmk+HKlSt4/PhxTQxHCCGEqK1UJse9LEqiSO1TK4maNm0afvvtNwBlCZS3tzc6d+4MW1tbnDlzpibjI4QQQqrlfnY+SmQMutoCNDPW1XQ4pBFTK4n6448/0LFjRwDAX3/9haSkJMTFxeHrr7/Gt99+W6MBEkIIIdVRfinPyVwffD7dmUdqj1pJVFZWFiwtLQEAx44dw+DBg9GyZUt8+umnuH79eo0GSAghhFQHLSondUWtJMrCwgI3b96ETCZDSEgI+vXrB6BsKxgqvkkIIUSTaFE5qStqFdscO3YshgwZAisrK/B4PPj6+gIAIiMj4eLiUqMBEkIIIdURn/EEACVRpPaplUQtWrQI7dq1w4MHDzB48GAIhWUl9QUCAebMmVOjARJCCCGqkssZ7maU7ZnnTEkUqWVqJVEA8NFHH1VoGz169GsFQwghhLyOlJwCFJTIoCPgw85ET9PhkEauWtu+qIoqlhNCCNGE8vVQjqb60BLUSClEQipVrW1fVEHbvhBCCNEUblG5BV3KI7VPrW1fCCGEkPqIW1RuRkkUqX0010kIIaTR4GpE0UwUqQNqLSz/9NNPq3z+999/VysYQgghRF2MMcRTjShSh9RKol7eaLikpASxsbHIyclBnz59aiQwQgghpDoynhThSWEp+LyyheWE1Da1kqg///yzQptcLsfkyZPh5OT02kERQggh1VV+Kc+hqT6EWrR7Bql9NbYmis/nY/r06SrfxUcIIYTUpPj0skXlTnQpj9SRGl1YfvfuXZSWltbkkIQQQohKEjLLNx6mJIrUDbUu502fPl3hMWMMaWlp+Pvvv6lqOSGEEI2IT6c780jdUiuJunz5ssJjPp8PMzMzrF69+pV37hFCCCG1gSu0aWao4UjIm0KtJOr06dM1HQchhBCituy8YjzKKwYAOJnTnXmkbqi9ATEAZGRk4Pbt2wCAVq1awdzcvEaCIoQQQqqjfBaqmbEu9HRe66ONEJWptbA8NzcXn3zyCaytreHj4wMfHx80a9YMH3/8MaRSaU3HSAghhFSJKpUTTVAriZowYQIiIyPx999/IycnBzk5OTh69CguXryISZMm1XSMhBBCSJXK98yjO/NIXVJrzvPo0aP4559/0KNHD67Nz88PmzdvRv/+/WssOEIIIUQVCbTdC9EAtWaimjZtCrFYXKFdLBajSZMm1R5vw4YNcHBwgEgkgoeHB6Kioqrsv3//fri4uEAkEqF9+/Y4duyYwvOMMSxcuBBWVlbQ1dWFr68v4uPjueeTkpIwbtw4ODo6QldXF05OTggICEBxcbHS8yUkJMDQ0BDGxsbVfm2EEEJq3/Mkiu7MI3VHrSRq/vz5mD59OiQSCdcmkUjwzTffYMGCBdUaa+/evZg+fToCAgIQExODjh07ws/PDxkZGUr7X7hwAcOHD8e4ceNw+fJl+Pv7w9/fH7GxsVyfFStWYN26dQgKCkJkZCT09fXh5+eHwsJCAEBcXBzkcjk2btyIGzduYM2aNQgKCsK8efMqnK+kpATDhw9Hz549q/W6CCGE1I0nhSVIk5b9fqeZKFKXeIwxVt2DOnXqhISEBBQVFcHOzg4AkJycDKFQCGdnZ4W+MTExVY7l4eGBrl27Yv369QDK9uCztbXFlClTMGfOnAr9hw4diry8PBw9epRr69atG1xdXREUFATGGKytrTFjxgzMnDkTACCVSmFhYYHg4GAMGzZMaRwrV67Er7/+inv37im0z549G6mpqejbty+mTZuGnJycqt+cF+Tm5kIsFkMqlcLIyEjl4wghhKjuyoMc+G8Ih7GuNo5P6wkrsa6mQyINnKqf32qtifL391c3LgXFxcW4dOkS5s6dy7Xx+Xz4+voiIiJC6TEREREVKqb7+fnh0KFDAIDExERIJBL4+vpyz4vFYnh4eCAiIqLSJEoqlcLExESh7dSpU9i/fz+uXLmCgwcPvvL1FBUVoaioiHucm5v7ymMIIYS8nh3/3QcA5BSUoPvyUwgc1B5Du9ppOCryJlAriQoICKiRk2dlZUEmk8HCwkKh3cLCAnFxcUqPkUgkSvuXX1os/7OqPi9LSEjAzz//jFWrVnFtjx49wpgxY7Bjxw6VZ5ECAwOxePFilfoSQgh5PQXFMqw9eRt/XHrItckZMO9gLLxbmtGMFKl1am9AnJOTgy1btmDu3LnIzs4GUHbpLiUlpcaCqwspKSno378/Bg8ejAkTJnDtEyZMwIgRI+Dt7a3yWHPnzoVUKuW+Hjx4UBshE0LIG61UJseeqGT0WnUaG88lVnhexhiSsvI1EBl506g1E3Xt2jX4+vpCLBYjKSkJEyZMgImJCQ4ePIjk5GRs375dpXFMTU0hEAiQnp6u0J6eng5LS0ulx1haWlbZv/zP9PR0WFlZKfRxdXVVOC41NRW9e/eGl5cXNm3apPDcqVOncOTIEW52ijEGuVwOLS0tbNq0SekegUKhEEKhUIVXTgghpLoYYwi7lYEfQuIQ/+xuPEuxCOnSQry4uFfA48HBVE8zQZI3ilozUdOnT8eYMWMQHx8PkUjEtb/zzjs4d+6cyuPo6OjAzc0NYWFhXJtcLkdYWBg8PT2VHuPp6anQHwBCQ0O5/o6OjrC0tFTok5ubi8jISIUxU1JS0KtXL7i5uWHr1q3g8xXfioiICFy5coX7+u6772BoaIgrV67ggw8+UPk1EkIIeX2Xkx9j6Mb/MH77RcRnPIWxnjbmD2iNs9/0wvIP20PA4wEoS6CWDWpHl/JInVBrJio6OhobN26s0N6sWbNK1x1VZvr06Rg9ejS6dOkCd3d3rF27Fnl5eRg7diwAYNSoUWjWrBkCAwMBAFOnToWPjw9Wr16NAQMGYM+ePbh48SI3k8Tj8TBt2jQsWbIEzs7OcHR0xIIFC2Btbc0tiC9PoOzt7bFq1SpkZmZy8ZTPZLVu3VohzosXL4LP56Ndu3bVen2EEELUl5iVh5X/xOHY9bLPFqEWH5/2cMRnPk4Q62oDAIZ2tYN3SzMkZeXDwVSPEihSZ9RKooRCodI7z+7cuQMzM7NqjTV06FBkZmZi4cKFkEgkcHV1RUhICLcwPDk5WWGWyMvLC7t27cL8+fMxb948ODs749ChQwrJzaxZs5CXl4eJEyciJycHPXr0QEhICDdrFhoaioSEBCQkJMDGxkYhHjUqPhBCCKlhWU+LsC4sHrsik1EqZ+DxgI862+Drfi1hbVwxSbIS61LyROqcWnWixo8fj0ePHmHfvn0wMTHBtWvXIBAI4O/vD29vb6xdu7YWQm14qE4UIYRUT15RKbb8m4hN5+4ir1gGAOjdygyz33aBiyX9HiV1Q9XPb7WSKKlUio8++ggXL17EkydPYG1tDYlEAk9PTxw7dgz6+vqvFXxjQUkUIYSoplQmx96LD7D2ZDwyn5TV2+tgI8act13g5WSq4ejIm6ZWi22KxWKEhoYiPDwcV69exdOnT9G5c2eFApeEEELIqzDGcOJmOlaExOFuZh4AwNZEF7P8XDCgvRX4fJ6GIySkctVOokpKSqCrq4srV66ge/fu6N69e23ERQghpJG7dP8xAo/dwsX7jwEATfS08VVfZ4z0sIeOltplDAmpM9VOorS1tWFnZweZTFYb8RBCCGnk7mY+xcqQ2wi5UXbHnUibj3E9HDHJxwlGIm0NR0eI6tS6nPftt99i3rx5+N///ldhvzlCCCFEmYwnhfjpZDz2RD+ATM7A5wGD3Wzxdb+WsBSLXj0AIfWMWknU+vXrkZCQAGtra9jb21dYSB4TE1MjwRFCCGn48opKsencPWz+9x7yn91x59vaHLP6u6ClhaGGoyNEfWolUeVFKwkhhJDKlMjk2BP9AD+djEfW07I77jraGmPe2y7waN5Uw9ER8vrUKnGgqt27d+P9999/Y0seUIkDQsibiDGGf25IsCLkNu5lld1x59BUD9/4ueCd9pbg8eiOO1K/1WqJA1VNmjQJHh4eaN68eW2ehhBCSD1xMSkby47dQkxyDgCgqb4OvurrjOHudnTHHWl0ajWJoi1UCCHkzZCQ8RQ/hMQh9GY6AEBXW4DxPR0x0bs5DOmOO9JI1WoSRQghpHHLyC3EmpPx2Hfx+R13Q7va4WtfZ5gb0R13pHGjJIoQQki1PS0qxaazd7H530QUlJTdcdevjQVm92+FFuZ0xx15M1ASRQghRGUlMjl2RyXjp5PxeJRXDADoZGeMee+0RlcHqhtI3iyURBFCCHklxhiOx0qwIiQOSY/yAQCOpvqY5dcK/dvRHXfkzVSrSZS9vT20tWlBISGENGSR9x4h8HgcrjzIAQCYGuhgqm9LDOtqC20B3XFH3lxqJVEPHjwAj8eDjY0NACAqKgq7du1CmzZtMHHiRK5fbGxszURJCCGkzsWnP8EPIXE4eSsDAKCnI8CEns0xwbs5DIR0IYMQtf4VjBgxAhMnTsQnn3wCiUSCfv36oW3btti5cyckEgkWLlxY03ESQgipI+m5hVgTegf7Lj6AnAECPg/Dutpiqq8zzA3pjjtCyqmVRMXGxsLd3R0AsG/fPrRr1w7h4eE4ceIEPvvsM0qiCCGkAXpSWIKNZ+9hy/l7KCyRAwD82lpgVn8XOJkZaDg6QuoftZKokpISCIVCAMDJkyfx/vvvAwBcXFyQlpZWc9ERQgipdcWlcuyKvI91pxKQ/eyOOzf7Jpj7tgu60B13hFRKrSSqbdu2CAoKwoABAxAaGorvv/8eAJCamoqmTWlTSUIIaQgYY/j7ehpW/nMb95/dcdfcTB+z+7vgrTYWdMcdIa+gVhL1ww8/4IMPPsDKlSsxevRodOzYEQBw5MgR7jIfIYSQ+ivi7iMsP34LVx9KAQBmhkJM83XG0C620KI77ghRCY+pucGdTCZDbm4umjRpwrUlJSVBT08P5ubmNRZgQ6bqLtCEEFJXbkvK7rg7FVd2x52+jgATvZ0wvqcj9OmOO0IAqP75rda/mIKCAjDGuATq/v37+PPPP9G6dWv4+fmpFzEhhJBakyYtwJrQO/jj0kPIGaDF52GEhx2m9HGGmaFQ0+ER0iCplUQNHDgQgwYNwmeffYacnBx4eHhAW1sbWVlZ+PHHHzF58uSajpMQQogacgtL8OuZu/j9fCKKSsvuuHunvSW+8XOBo6m+hqMjpGFT68J3TEwMevbsCQD4448/YGFhgfv372P79u1Yt25djQZICCGk+opKZfjtfCJ8VpzGr2fuoqhUDncHExz83Au/jHSjBIqQGqDWTFR+fj4MDct26T5x4gQGDRoEPp+Pbt264f79+zUaICGEENXJ5Qx/XUvFqhO38SC7AADQwtwAs/u7wLe1Od1xR0gNUiuJatGiBQ4dOoQPPvgA//zzD77++msAQEZGBi2gJoSQOpQmLUBiVh4cTfWRmJmHwONxuJ5SdseduaEQX/dricFuNnTHHSG1QK0kauHChRgxYgS+/vpr9OnTB56engDKZqU6depUowESQghRbm90MuYevA75S/dYGwi18JlPc3zawxF6OnTHHSG1Re0SBxKJBGlpaejYsSP4/LL/4URFRcHIyAguLi41GmRDRSUOCCG1JU1agO7LT1VIoAa72WDO2y5oakB33BGirlotcQAAlpaWsLS0xMOHDwEANjY2VGiTEELqAGMMu6OSKyRQADCosw0lUITUEbUuksvlcnz33XcQi8Wwt7eHvb09jI2N8f3330Mul9d0jIQQQp5JySnAuG0XsS4socJzAh4PDqZ6GoiKkDeTWjNR3377LX777TcsX74c3bt3BwCcP38eixYtQmFhIZYuXVqjQRJCyJtOJmcIvpCE1SduI79YBh0BH94tTXEqLgNyVpZALRvUDlZiXU2HSsgbQ601UdbW1ggKCsL777+v0H748GF8/vnnSElJqbEAGzJaE0UIqQk3UqWYe/A6rj3b587dwQTLBrVDC3NDpEkLkJSVDwdTPUqgCKkhtbomKjs7W+nicRcXF2RnZ6szJCGEkJcUFMuw9uQdbDmfCJmcwVCkhXnvtMbQLrbg88vqPVmJdSl5IkRD1FoT1bFjR6xfv75C+/r169GxY8fXDooQQuqLNGkBLtzNQpq0oE7Pe/ZOJt5aexYbz92DTM4woIMVwqb7YLi7HZdAEUI0S60kasWKFfj999/Rpk0bjBs3DuPGjUObNm0QHByMlStXVnu8DRs2wMHBASKRCB4eHoiKiqqy//79++Hi4gKRSIT27dvj2LFjCs8zxrBw4UJYWVlBV1cXvr6+iI+P555PSkrCuHHj4OjoCF1dXTg5OSEgIADFxcVcnzNnzmDgwIGwsrKCvr4+XF1dsXPnzmq/NkJIw7U3Ohndl5/CiM2R6L78FPZGJ9faucqTtRupUkzbcxmjf4/Cg+wCWItF+G10F2wY0RnmRqJaOz8hpPrUupzn4+ODO3fuYMOGDYiLiwMADBo0CJ9//jmsra2rNdbevXsxffp0BAUFwcPDA2vXroWfnx9u374Nc3PzCv0vXLiA4cOHIzAwEO+++y527doFf39/xMTEoF27dgDKkrx169Zh27ZtcHR0xIIFC+Dn54ebN29CJBIhLi4OcrkcGzduRIsWLRAbG4sJEyYgLy8Pq1at4s7ToUMHzJ49GxYWFjh69ChGjRoFsViMd999V523jRDSgKRJCxQKWcoZMPvAdYTezICZoQ70dLSgryOAnlAL+sJn3+toQV8oePZYC3o6z74XCqAj4Fe65Yqyopl8HjDGyxEz3moJfSEVzCSkPlK72GZN8fDwQNeuXbnLg3K5HLa2tpgyZQrmzJlTof/QoUORl5eHo0ePcm3dunWDq6srgoKCwBiDtbU1ZsyYgZkzZwIApFIpLCwsEBwcjGHDhimNY+XKlfj1119x7969SmMdMGAALCws8Pvvv6v02mhhOSEN14W7WRixObLGxtPi815Iqp4nXQI+cD7hUYX+v43ugr6tLWrs/IQQ1dX4wvJr166pfPIOHTqo1K+4uBiXLl3C3LlzuTY+nw9fX19EREQoPSYiIgLTp09XaPPz88OhQ4cAAImJiZBIJPD19eWeF4vF8PDwQERERKVJlFQqhYmJSZXxSqVStG7dutLni4qKUFRUxD3Ozc2tcjxCSP3laKoPPg8VClpqC3gY7m4HPR0t5BeX4mlRKfKLZMgrLkV+sQx5RaVl3xfJ8LSoFEWlZbXzSuUMuYWlyC0sVen8tF0LIfWfyv9KXV1dwePx8KqJKx6PB5lMptKYWVlZkMlksLBQ/N+WhYUFd5nwZRKJRGl/iUTCPV/eVlmflyUkJODnn3/mLuUps2/fPkRHR2Pjxo2V9gkMDMTixYsrfZ4Q0nBYiXUROKg95h2MhYwx8ACAB5TIGA5fScW3A1pjsJtNpZfoypXK5MgvkXFJVX5xKfKKZFwClpZTiB9C4vDib1YqmklIw6ByEpWYmFibcWhMSkoK+vfvj8GDB2PChAlK+5w+fRpjx47F5s2b0bZt20rHmjt3rsIsWW5uLmxtbWs8ZkJI3Rja1Q7eLc24OkzZecWYfeAaYlNyMeuPazhyJRXLPmgPu6aVJzxaAj6MBHwYibQr7dNEX5tL1qhoJiENh8pJlL29fbUHHzBgALZs2QIrKyulz5uamkIgECA9PV2hPT09HZaWlkqPsbS0rLJ/+Z/p6ekK501PT4erq6vCcampqejduze8vLywadMmpec7e/Ys3nvvPaxZswajRo2q/MUCEAqFEAppzypCGpMX6zBZiXVx6PPu+O18In4MvYPzCVnwW3sOM95qif7tLJGcnQ9HU/1qJ0AvJ2uUQBHSMKhV4kBV586dQ0FB5bVVdHR04ObmhrCwMK5NLpcjLCwMnp6eSo/x9PRU6A8AoaGhXH9HR0dYWloq9MnNzUVkZKTCmCkpKejVqxfc3NywdetW8PkV34ozZ85gwIAB+OGHHzBx4kTVXjQhpFHTEvAxyccJ/0zzhmfzpigokWHJ37fQ44fTr1UKwUqsC0+nppRAEdKA1GoSpYrp06dj8+bN2LZtG27duoXJkycjLy8PY8eOBQCMGjVKYeH51KlTERISgtWrVyMuLg6LFi3CxYsX8eWXXwIoW5M1bdo0LFmyBEeOHMH169cxatQoWFtbw9/fH8DzBMrOzg6rVq1CZmYmJBKJwpqp06dPY8CAAfjqq6/w4Ycfcs9TRXZCCAA4mOpj1wQPzO2vuHuDnAHzDsbWeXFOQkjd0/jtH0OHDkVmZiYWLlwIiUQCV1dXhISEcAvDk5OTFWaJvLy8sGvXLsyfPx/z5s2Ds7MzDh06xNWIAoBZs2YhLy8PEydORE5ODnr06IGQkBCIRGWF6kJDQ5GQkICEhATY2NgoxFO+cH7btm3Iz89HYGAgAgMDued9fHxw5syZ2no7CCENCI/HQ3tbcYV2GWNIysqnWSVCGrlarRNlaGiIq1evonnz5rV1inqN6kQR0vilSQvgFXiqwt115+f0piSKkAZK1c9vjV/OI4SQhsxQpA0dree/SunuOkLeHBq/nEcIIQ3ZgUsPUVQqh10TXSz/sAMczap/dx4hpGGq1SRq3rx5r6wCTgghDZVczhB8IQkAMMG7ObxamGo2IEJInVIriTpy5IjSdh6PB5FIhBYtWsDR0VHhrjpCCGlsztzJQGJWHgxFWhjU2ebVBxBCGhW1kih/f3+lW8CUt/F4PPTo0QOHDh1CkyZNaiRQQgipb34/nwQAGO5uB30hrY4g5E2j1sLy0NBQdO3aFaGhoZBKpZBKpQgNDYWHhweOHj2Kc+fO4dGjR5g5c2ZNx0sIIfXCnfQnOJ+QBT4PGOVZ/R0dCCENn1r/dZo6dSo2bdoELy8vrq1v374QiUSYOHEibty4gbVr1+LTTz+tsUAJIaQ+2Rpetp+oX1tL2DShzYIJeROpNRN19+5dpXUTjIyMcO/ePQCAs7MzsrKyXi86Qgiph7LzinEwJgUAMLa7o4ajIYRoilpJlJubG7755htkZmZybZmZmZg1axa6du0KAIiPj4etrW3NREkIIfXI7qhkFJXK0a6ZEbo60LpPQt5Ual3O++233zBw4EDY2NhwidKDBw/QvHlzHD58GADw9OlTzJ8/v+YiJYSQeqBEJsf/Iu4DAD7t7ggej6fhiAghmqJWEtWqVSvcvHkTJ06cwJ07d7i2fv36cfvclW/2SwghjcnxWAkkuYUwNRBiQAcrTYdDCNEgte/J5fP56N+/P/r371+T8RBCSL32+/myBeWfdLOHUEug4WgIIZqkdhIVFhaGsLAwZGRkQC6XKzz3+++/v3ZghBBS38QkP8aVBznQEfAxwsNO0+EQQjRMrSRq8eLF+O6779ClSxdYWVnRmgBCyBtha3gSAOB9V2uYGQo1GwwhROPUSqKCgoIQHByMTz75pKbjIYQ0cmnSAiRm5cHRtGFt1JsmLcCx62kAgLHdHTQbDCGkXlAriSouLlYotEkIIarYG52MuQevQ84APg8IHNQeQ7s2jMti/4u4D5mcwcPRBG2txZoOhxBSD6hVJ2r8+PHYtWtXTcdCCGnE0qQFmHOgLIECADkD5h2MRZq0QLOBqaCgWIZdUckAqLgmIeQ5tWaiCgsLsWnTJpw8eRIdOnSAtra2wvM//vhjjQRHCGk8wuOzwF5qkzGGpKz8en9Z79CVFOTkl8CmiS76tbHQdDiEkHpCrSTq2rVrcHV1BQDExsYqPEeLzAkhL2OMYeYf1yq0C3g8OJjW733nGGPcPnljvBwg4NPvOEJIGbWSqNOnT9d0HISQRuyrPVe473k8gLGyBGrZoHb1fhYqPOER7qQ/hb6OAEO60lZWhJDn1K4TRQghqriT/gR/XU3lHl+Y0wdJWflwMNWr9wkUAPz+bBZqcBdbGIm0X9GbEPImUTmJGjRoEIKDg2FkZIRBgwZV2ffgwYOvHRghpOFjjOGtNee4x9cXvQVDkXaDSJ4A4F7mU5yKywCPB4z2ctB0OISQekblJEosFnPrncRiur2XEPJqX+yK4b4PHNQehg1sJmfbhSQAQJ9W5nA01ddsMISQekflJGrr1q1KvyeEEGVupeXi2HUJ93i4e8OoB1VOWlCC/ZceAqCyBoQQ5dSqE0UIIVWRyRne/ulf7vG1RW9pMBr17L/4APnFMrS0MED3Fk01HQ4hpB5SeSaqU6dOKpcviImJeXUnQkij9fnOS9z3yz5o3+AWZMvkDMHPLuV92t2RSrcQQpRSOYny9/fnvi8sLMQvv/yCNm3awNPTEwDw33//4caNG/j8889rPEhCSMMRmyLFPzfSuccjPBrWZTwACL2ZjoePC9BETxv+nZppOhxCSD2lchIVEBDAfT9+/Hh89dVX+P777yv0efDgQc1FRwhpUEpkcrz783nu8dWFDe8yHvC8rMEIDzuItAUajoYQUl+ptSZq//79GDVqVIX2jz/+GAcOHHjtoAghDdPkHc8v4y39oB3Eeg3rMh5QNpMWlZgNLT4Pn3Rz0HQ4hJB6TK0kSldXF+Hh4RXaw8PDIRKJXjsoQkjDczn5MU7eyuAej/Sw12A06tsangQAeKe9FSzF9PuMEFI5tSqWT5s2DZMnT0ZMTAzc3d0BAJGRkfj999+xYMGCGg2QEFL/FZXK8MEvF7jHMQv6aTAa9WU+KeKqq4/t7qDZYAgh9Z5aSdScOXPQvHlz/PTTT9ixYwcAoHXr1ti6dSuGDBlSowESQuq/idufX8b7fmBbmOjraDAa9e2MvI9imRyd7IzRya6JpsMhhNRzau+dN2TIEEqYCCGIvPcIZ+9kAgD4PODjbg3zMl5RqQw7/rsPgIprEkJUQ8U2CSFqyy8uxdBN/3GP/5vXt8HWVDp6NQ1ZT4thaSTC2+0sNR0OIaQBUHkmqkmTJir/cszOzlY7IEJIw/HiZbxF77WBuWHDXIjNGOPKGozysoe2gP5/SQh5NZV/U6xduxZr1qzBmjVrMH/+fACAn58fFi1ahEWLFsHPzw8A1FpYvmHDBjg4OEAkEsHDwwNRUVFV9t+/fz9cXFwgEonQvn17HDt2TOF5xhgWLlwIKysr6OrqwtfXF/Hx8dzzSUlJGDduHBwdHaGrqwsnJycEBASguLhYYZxr166hZ8+eEIlEsLW1xYoVK6r92ghprP6Nz8T5hCwAgBafh9FeDq89Zpq0ABfuZiFNWvDaY1VHVGI2bqTmQqTNx/CuDa84KCFEQ5gaBg0axH7++ecK7T///DMbOHBgtcbas2cP09HRYb///ju7ceMGmzBhAjM2Nmbp6elK+4eHhzOBQMBWrFjBbt68yebPn8+0tbXZ9evXuT7Lly9nYrGYHTp0iF29epW9//77zNHRkRUUFDDGGDt+/DgbM2YM++eff9jdu3fZ4cOHmbm5OZsxYwY3hlQqZRYWFmzkyJEsNjaW7d69m+nq6rKNGzeq/NqkUikDwKRSabXeE0LqO2lBMbOffZT7Ss3Jf+0x90TdZ45zysZznHOU7Ym6XwORqmbS9ovMfvZRNufAtTo7JyGk/lL185vHGGPVTbwMDAxw5coVtGjRQqE9ISEBrq6uePr0qcpjeXh4oGvXrli/fj0AQC6Xw9bWFlOmTMGcOXMq9B86dCjy8vJw9OhRrq1bt25wdXVFUFAQGGOwtrbGjBkzMHPmTACAVCqFhYUFgoODMWzYMKVxrFy5Er/++ivu3bsHAPj111/x7bffQiKRQEen7E6jOXPm4NChQ4iLi1PpteXm5kIsFkMqlcLIyEjl94SQ+m7IxghEJZZdtp8/oDXG92z+WuOlSQvQffkpyF/4bcQH8OcXXuhoW7t3yT3IzofPytOQM+DE195oaWFYq+cjhNR/qn5+q3Xhv2nTpjh8+HCF9sOHD6NpU9V3Oy8uLsalS5fg6+v7PCA+H76+voiIiFB6TEREhEJ/oOyyYnn/xMRESCQShT5isRgeHh6VjgmUJVomJiYK5/H29uYSqPLz3L59G48fP1Y6RlFREXJzcxW+CGlsTt5M5xIoHQEf43q8/p1siVl5CgkUAMgB+G+4gN1Rya89flW2RyRBzoCezqaUQBFCqkWtEgeLFy/G+PHjcebMGXh4eAAoK7YZEhKCzZs3qzxOVlYWZDIZLCwsFNotLCwqne2RSCRK+0skEu758rbK+rwsISEBP//8M1atWqVwHkdHxQ+H8jElEgmaNKn4v+PAwEAsXrxY6TkIaQwe5xVj/PaL3OOwGT41cjeeo6k++DxUSKQYgLkHr+NpYQlGezlCR6tmF3znFZViT3TZfp9UXJMQUl1q/UYaM2YMwsPDYWRkhIMHD+LgwYMwMjLC+fPnMWbMmBoOsXalpKSgf//+GDx4MCZMmPBaY82dOxdSqZT7os2YSWMzbls09/3ct11ga6L3WuMxxhCdlI0lf9+qst/SY3HwWXkaW/69h6dFpa91zhcdiHmIJ4WlcDTVR6+W5jU2LiHkzaB2sU0PDw/s3LnztU5uamoKgUCA9PR0hfb09HRYWiqv02JpaVll//I/09PTYWVlpdDH1dVV4bjU1FT07t0bXl5e2LRpk0rnefEcLxMKhRAKhUqfI6Sh++tqKmKScwAAOlp8THiNdVCFJTIcuZKK4AtJuJn2/LJ3G0sj3JLk4sUJKR4AE30dpEkLseTvW1gXFo9Rng4Y7eUAM0P1/73J5YzbJ29sdwfw+Q2zvhUhRHPUnhu/e/cu5s+fjxEjRiAjo2zT0ePHj+PGjRsqj6GjowM3NzeEhYVxbXK5HGFhYfD09FR6jKenp0J/AAgNDeX6Ozo6wtLSUqFPbm4uIiMjFcZMSUlBr1694Obmhq1bt4LPV3wrPD09ce7cOZSUlCicp1WrVkov5RHSmGU8KcSU3Ze5xyFTe6qVdDx8nI/A47fQLTAMsw5cw820XAi1+BjaxRZ/f9UDx6b1xPIP20Pw7BKhgMfD8g/b48LcPvjhw/ZobqaP3MJSrD+dgB4/nMK3f17H/Ud5ar2mM3cykJiVB0ORFj7sbKPWGISQN5w6t/6dOXOG6erqMl9fX6ajo8Pu3r3LGGMsMDCQffjhh9Uaa8+ePUwoFLLg4GB28+ZNNnHiRGZsbMwkEgljjLFPPvmEzZkzh+sfHh7OtLS02KpVq9itW7dYQECA0hIHxsbG7PDhw+zatWts4MCBCiUOHj58yFq0aMH69u3LHj58yNLS0rivcjk5OczCwoJ98sknLDY2lu3Zs4fp6elRiQPyxpHL5ezddf9y5Qx+DrtT7ePD4zPZhG3RXAkD+9lHmVdgGAs6k8CynxZVOCY1J59dSMiqUDpBJpOz49fT2MD157lxHOccZV/svMSuP8ypVlwfb/mP2c8+ypYcvVGt4wghjZ+qn99qb0C8ZMkSTJ8+HYaGz+9m6dOnD1eqQFVDhw5FZmYmFi5cCIlEAldXV4SEhHCLuJOTkxVmiby8vLBr1y7Mnz8f8+bNg7OzMw4dOoR27dpxfWbNmoW8vDxMnDgROTk56NGjB0JCQiASlVVTDg0NRUJCAhISEmBjo/g/UPas4oNYLMaJEyfwxRdfwM3NDaampli4cCEmTpxYvTeLkAZu/6WHuJ4iBQCItPn4zMdJpePyikpx8HIKtl9IQnzG87In3Vs0xWhPB/RtbQFBJbNZVmJdWIl1K7Tz+Tz0b2cJv7YWiEzMRtDZuzhzOxNHr6Xh6LU09HQ2xWc+TvByalrlgvc76U/wb3wW+DxglKeDSq+HEEJepnadqOvXr8PR0RGGhoa4evUqmjdvjqSkJLi4uKCwsLA2Ym1wqE4UaehScsrqN5X7Z5o3WllWXQYgKSsP2yPuY/+lB3hSWLYIXE9HgEGdm2G0pwOca7iMwM3UXGw6dxd/XUuD7Nntfe2biTHJpznebmelNFGbe/A6dkclo39bSwR94laj8RBCGj5VP7/VmokyNjZGWlpahRIAly9fRrNmzdQZkhBSz8jlDOOCn9+NN7Wvc6UJlFzOcDY+E9suJOHM7Uyu3aGpHkZ5OuCjLjYwEmnXSpxtrI2wdlgnzHirFX47n4g90cm4niLFl7suw77pbUzo2RwfudlApC0AUFam4WDMQwBU1oAQ8nrUSqKGDRuG2bNnY//+/eDxeJDL5QgPD8fMmTMxatSomo6REKIBOyLvI07yBEDZZbwv+7So0Ce3sAR/XHyI//13H4lZzxd492plhtFeDvBxNquzu95sTfSw6P22+KqvM7ZdSMK2iCTcf5SP+YdisfbkHYzt7oiPu9lj0793UVQqR0sLA7g7mrx6YEIIqYRal/OKi4vxxRdfIDg4GDKZDFpaWpDJZBgxYgSCg4MhEAhqI9YGhy7nkYYqMSsPvVed4R4fndID7ZqJucfx6U+wPeI+DsQ8RH6xDABgKNTC4C62GOVpDwdT/boOuYL84lLsiXqA384nIiWnbENjHQEPxbKyX3k8AMs/bI+htOEwIeQlqn5+q5VElUtOTkZsbCyePn2KTp06wdnZWd2hGiVKokhDJJMz+K09h4Rni8E/83HCnLddIJMzhN1Kx7aIJIQnPOL6O5sbYJSXAwZ1agZ9odql52pNiUyOv66mYv2pBNzLUiyHIODxcH5Ob6WL2Akhb65aXRNVzs7ODra2tgBQI1s/EEI0b8u/97gESqTFx9juDgg6exf/i7jPzejweYBvawuM8XKA5yvuhNM0bQEfgzrbwNJIhBFbIhWekzGGpKx8SqIIIWpRO4n67bffsGbNGsTHxwMAnJ2dMW3aNIwfP77GgiOE1K3z8VkIPP5838rCUjm6Lz+F0md3vRnraWNYVzt83M0ONk1eb8uXuuZoVnF/PgGPBwfThvU6CCH1h1pJ1MKFC/Hjjz9iypQpXBXwiIgIfP3110hOTsZ3331Xo0ESQmrfrsj7mPdnbIX2UjmDs7kBJvRsjvddrbm73BoaK7EuAge1x7yDsZAxBgGPh2WD2tEsFCFEbWqtiTIzM8O6deswfPhwhfbdu3djypQpyMrKqrEAGzJaE0UaijRpAbwCT6GyXwa7J3jA08m0TmOqLWnSAiRl5cPBVI8SKEKIUrW6JqqkpARdunSp0O7m5obS0prbYZ0QUjfuZuRVmkDxADzKK0aatKBRJB2VVUMnhJDqUmsD4k8++QS//vprhfZNmzZh5MiRrx0UIaRu3ZbkVvocA/DlrsvovvwU9kYn111QhBBSz6k8EzV9+nTuex6Phy1btuDEiRPo1q0bACAyMhLJyclUbJOQBuj7v28pPObzgMFdbLA3+iHXJmfAvIOx8G5pRjM5hBCCaiRRly9fVnjs5la239Tdu3cBAKampjA1NcWNGzdqMDxCSG377Xwi9/2377igXTNjOJjqITErTyGJAqgkACGEvEjlJOr06dO1GQchRAMKS2T4/uhN7vEEbyeF518uCcDngUoCEELIM2qtiSKENA6zD1zjvt81wUPhufKSAIIXCmm2MDegWShCCHlGrbvzCgsL8fPPP+P06dPIyMiAXC5XeD4mJqZGgiOEqC5NWoDErDw4muqrlOhcfZCDw1dSucdeSkoYDO1qB++WZvjv7iN888dV3El/itO3M9C7lXmNxk4IIQ2RWknUuHHjcOLECXz00Udwd3ev11s+EPIm2BudjLkHr0POyi65BQ6qemPd4lI5vtz9/D87m0dVLFlSzkqsiw862+BmWi42/5uIpX/fQo8WptAW0EQ2IeTNplYSdfToURw7dgzdu3ev6XgIIVVQNtuUJi3gEiigbA3T3IPXq7yLbsPpBDzILtsHz0ikhb4ur55Z+rKPMw7EpCAh4yl2RSZjtJdDjbwmQghpqNT6r2SzZs1gaGhY07EQQqqwNzoZ3ZefwojNkQo1mxKz8hQWfwNlidSnwdEIvZkO+UtP3krLxc+n4rnHM95qBT7/1bPJYl1tTO/XEgCw5uQdSPNLXvMVEUJIw6ZWErV69WrMnj0b9+/fr+l4CCFKKJttmncwFmnSAjialm2s+7JbaU8wYftF9P3xLLZHJCG/uBSlMjm++eMqN46RSAsfudmoHMewrrZoaWGAnPwS/BQW/+oDCCGkEVMrierSpQsKCwvRvHlzGBoawsTEROGLEFKzbqc9qTDb9GLNphfvons5n0rMysPCwzfgGXgKLb49jtiU59XJh7vbQV+o+lV9LQEfC95tAwDYHpGEu5lP1Xo9hBDSGKi1Jmr48OFISUnBsmXLYGFhQQvLCall+y89qND2Ys2m8rvokrLyYd9UF9si7mPj2XsAyi7DiXW1kZydX2EMddY19XQ2Q18Xc4TFZSDw2C1sGd212mMQQkhjoFYSdeHCBURERKBjx441HQ8h5CX7Lj7A39clZTNMPIA9m5FyMjOAqYGQ6/fixrpz324NMwMhlvx9C9KCEvRwNlWaRE3bewXjeziib2sLCFRYF1Vu3oDWOHsnEydvZeDf+Ez0dDZ7nZdICCENklqX81xcXFBQUFDTsRDyxkuTFuDC3SykScv+fcVJcrHwcCwAYMZbLXFhTh8s828HkTYf8RlPMefAdTDGlI41vmdz/DikI7T4PPx9LU1pn6jEbEz83yX0WX0G2y4kIa+oVKU4ncwM8ImnPQBgydFbKJXJX3EEIYQ0PjxW2W/gKpw4cQKLFy/G0qVL0b59e2hrays8b2RkVGMBNmS5ubkQi8WQSqX0npBXernWU8B7bbEtIgn3MvPg3dIMwWO6cnfRnY7LwPjtFyGTM3zVpwWmv9Wq0nG3XUhCwJGKe1rOe6c1HucXY1dkMqQFZXfaGYm0MNzDDmO8HF5ZsDMnvxi9Vp1BTn4Jln7QDiM97F/j1RNCSP2h6ue3WkkUn/98AuvF9VCMMfB4PMhksuoO2ShREkVUlSYtQPflpxQWj/MAMACWRiL8/VUPNH3h0h0A7IlKxpyD1wEAywe1xzD3isU15XKGEVv+w3/3sis8J+DxcH5Ob4h1tXHg0kP8dj4RSY/KLvlp8Xl4p70Vxvd0RAcb40rjLk/QTPR1cOabXjASaVfalxBCGgpVP7/VWhNFmxETUrOU1XpiKJuRWj+iU4UECgCGudshNacA604l4NtDsbAQiypsx7IrKhn/3cuGjoCP4pcuuZXf3efp1BSfeDpgpIc9wuIy8Nv5e/jvXjaOXE3Fkaup6OrQBON6NEe/NhXXTY3wsHt2l14e1p9KwLx3WtfI+0EIIQ2BWmuifHx8wOfzsXnzZsyZMwctWrSAj48PkpOTIRAIajpGQho9exM9pe2f93JCF4fKy4Z83a8lPuxsA5mc4YudMbj+UMo9l5JTgMBjtwAAX/R2qlBLSsDjcXf3AQCfz0O/NhbYM9ETR6f0wAedmkGLz0N00mN8tuMSeq86g63hiXj6wropbQEf85+VPNganoikrLxqv3ZCCGmo1EqiDhw4AD8/P+jq6uLy5csoKioCAEilUixbtqxGAyTkTXAjNbdCW2srQ8yoYq0TUHY5ffmH7dHT2RT5xTKMDY7Gg+x8MMYw9+B15BXL0MW+Cab0cVaoJSXg8bBsULtK1z21aybGmqGuOD+7Dz7v5cSVSFj81014BoYh8NgtpOaULX7v3cocPi3NUCJjCDx+6zXfCUIIaTjUWhPVqVMnfP311xg1ahQMDQ1x9epVNG/eHJcvX8bbb78NiURSG7E2OLQmiqjqo18v4OL9xwptAe+1wdjujiod/6SwBEM2/odbablobqaP4V3tsPTYLeho8XF8ak84mRkAKFt7lZSVDwdTvVcuHH9RfnEpDsSk4PfziUh8NtskeLZualwPR+jrCND/p38hkzPsmuABLydTlccmhJD6plYXluvp6eHmzZtwcHBQSKLu3buHNm3aoLCw8LWCbywoiSKquHT/MT789UKF9vKF36omO+m5hfhgQzhSpc///c152wWf+TjVWKxyOcPp2xnY8m8iIu494tq72DeBtKAE8RlP4WxugID32sLJXL9aiRohhNQXqn5+q3U5z9LSEgkJCRXaz58/j+bNm6szJCFvrE3n7iptL1/4rSoLIxGCP3VXaPtUxZksVfH5PPRtbYHdE7vh7696YFDnZtAW8HDx/mPEZ5RtAROf8RQf/6a4STIhhDRGaiVREyZMwNSpUxEZGQkej4fU1FTs3LkTM2fOxOTJk2s6RkIarXuZT3HiZrrS517c1kVVcZInCo9/CIlTO7ZXaWstxo9DytZNfdHbCUYixZt9X9wkmRBCGiO1ShzMmTMHcrkcffv2RX5+Pry9vSEUCjFz5kxMmTKlpmMkpNHa/G8iXrygzueBK3Xg36lZtS6HPXpahEXPimo6menjbmYefjufCCuxCON71t4MsYWRCN/4uaCrvQnGBEcrPPfiJsmEENLYqDUTxePx8O233yI7OxuxsbH477//kJmZie+//76m4yOk0cp8UoQDMQ+5xz4tzRA+pw/eaWcJAGiip1Ot8QKO3EB2XjFcLA1xfKo35r7tAgBYeuwWjl0v2/bl5W1lalIrK8NXllEghJDGRK2ZqHI6Ojpo06ZNTcVCyBtle0QSikvLCmAK+DwseLc1rMS66NnSDMdiJdwao1dJkxZgX/RDHL2WBgGfh5UfdYSOFh8TvZsjNacA2yLuY9reK7j2MAebzt3jtpUJHNQeQ7tWrHKuLiuxLgIHtce8g7GQMfbKMgqEENLQqTUTVdM2bNgABwcHiEQieHh4ICoqqsr++/fvh4uLC0QiEdq3b49jx44pPM8Yw8KFC2FlZQVdXV34+voiPj5eoc/SpUvh5eUFPT09GBsbKz1PdHQ0+vbtC2NjYzRp0gR+fn64evXqa71WQoCykgHB4Unc40+62aOFuSEAwNm8rBzBXRWSqL3Ryei+/BTWnLwDAOhsZww5Y/j7Who2nbuHkmfXBotL5Qg6e4+7VFhb65WGdrXD+Tm9sXtCN5yf07tGkzRCCKlvNJ5E7d27F9OnT0dAQABiYmLQsWNH+Pn5ISMjQ2n/CxcuYPjw4Rg3bhwuX74Mf39/+Pv7IzY2luuzYsUKrFu3DkFBQYiMjIS+vj78/PwUSi8UFxdj8ODBlS6Ef/r0Kfr37w87OztERkbi/PnzMDQ0hJ+fH0pKSmr2TSBvnH3RD/DkWeVvQ5EWpvk6c8+1eJZEpeQUIO+F6uAvS5MWcBsWl4tOeoyBG8Lxxa4YBB6Pw67Iyu+Oq+7df6qyEuvC06kpzUARQho9tepE1SQPDw907doV69evBwDI5XLY2tpiypQpmDNnToX+Q4cORV5eHo4ePcq1devWDa6urggKCgJjDNbW1pgxYwZmzpwJoKySuoWFBYKDgzFs2DCF8YKDgzFt2jTk5OQotF+8eBFdu3ZFcnIybG1tAQDXr19Hhw4dEB8fjxYtWrzytVGdKKJMqUyOTt+H4klhWYK0+P22GO3loNCny5JQZD0txpEvu1e6AfCFu1kYsTmyQnsTPW00NzOATRPdZ196KC6VIeDITYV+PACHvvBCR9smNfGyCCGk0ajVOlE1pbi4GJcuXYKvry/Xxufz4evri4iICKXHREREKPQHAD8/P65/YmIiJBKJQh+xWAwPD49Kx1SmVatWaNq0KX777TcUFxejoKAAv/32G1q3bg0HBwelxxQVFSE3N1fhi5CXHYuVcAmUrYkuRnpUvORVPhsVn175JT1HU/0KC7n5PODY1J44MNkLPw3rhG/8XDDc3Q6jvRwxy09xCxkG4INfLlAtJ0IIUZNGk6isrCzIZDJYWFgotFtYWFS6dYxEIqmyf/mf1RlTGUNDQ5w5cwY7duyArq4uDAwMEBISguPHj0NLS/l6/MDAQIjFYu6rfAaLkHKMMcw9cI17vNS/PbQEFf8ZlidRCZmVJ1HlC7lf3A8vcFD7Si+jfd67BVZ81EGhjWo5EUKI+l7r7rzGrKCgAOPGjUP37t2xe/duyGQyrFq1CgMGDEB0dDR0dSt+UM2dOxfTp0/nHufm5lIiRRRE3H2EvGIZAKCTnTG8W5op7ef8bJF5VTNRQNlCbu+WZirvh2fTpOLzVMuJEELUo9EkytTUFAKBAOnpihWb09PTYWlpqfQYS0vLKvuX/5meng4rKyuFPq6urirHtmvXLiQlJSEiIgJ8Pp9ra9KkCQ4fPlxhbRUACIVCCIVClc9B3jwjf3u+hmnV4I6V9iufibpbxUxUOSuxrsoJUPklwBcXo1MtJ0IIUY9GL+fp6OjAzc0NYWFhXJtcLkdYWBg8PT2VHuPp6anQHwBCQ0O5/o6OjrC0tFTok5ubi8jIyErHVCY/Px98Ph883vNFJ+WP5XK5yuOQ+qU2i02+SmyKlKtO/lYbCziZGVTat7zMwf1HeSgqldVYDMouAVItJ0IIUY/GL+dNnz4do0ePRpcuXeDu7o61a9ciLy8PY8eOBQCMGjUKzZo1Q2BgIABg6tSp8PHxwerVqzFgwADs2bMHFy9exKZNmwCUVVOfNm0alixZAmdnZzg6OmLBggWwtraGv78/d97k5GRkZ2cjOTkZMpkMV65cAQC0aNECBgYG6NevH7755ht88cUXmDJlCuRyOZYvXw4tLS307t27Tt8jUjP2RidzJQFqo9hkVdKkBXj35/Pc45VVzEIBgJmhEIYiLTwpLEViVh5cLGvu7s7qXgIkhBCinMaTqKFDhyIzMxMLFy6ERCKBq6srQkJCuIXhycnJ3OU0APDy8sKuXbswf/58zJs3D87Ozjh06BDatWvH9Zk1axby8vIwceJE5OTkoEePHggJCYFIJOL6LFy4ENu2beMed+rUCQBw+vRp9OrVCy4uLvjrr7+wePFieHp6gs/no1OnTggJCVG4TEgahpdrKpUvqPZuaVbrScTe6GTMOXide9zVvgnEutpVHsPj8eBsboCY5BwkZDyt0SQKqN4lQEIIIcppvE5UY0Z1ouqPymoq7Z7QDZ5OTWvtvGnSAnRffkphDRKfB4TP6fPKJGbWH1ex7+JDDOrUDN/0b0VJDyGE1JEGUSeKkLriaKpfoa0uFlQnZuUpJFBA2SzYqyqFM8aQ9bQYAHDwcgq6Lz9F9ZwIIaSe0fjlPELqwsuzOHW1oFrVu+Hkcobb6U8QlZiNqMRsXLj7CI/zi58/X4eXHwkhhKiGkijyRnhxD7rhXW3xla9znSQj5XfDzTsYCxljXPJmaiDE5eTHXNIUnZSN3MLK98kDqJ4TIYTUN5REkTqRJi1AYlYeHE31NZIErAm9w32/eGA76GjV3ZXsoV3t4O7QFGFx6ZBIC3HkaioWHbmJghLF0gV6OgK42TeBu4MJnMz08eXuy1TPiRBC6jFKokit02RpgXJbzidy39dFApVbWIJLSY8RlVQ203TtYQ5KZIqLo4z1tNHVwQQejibo6mCCttZGClvABBaVVpjBolkoQgipPyiJIrVKk6UFyhW+MOMzvoejSseoMnP2Yh9tAR/Ridlc0nQrLbfCgnILIyHcHZvC3dEE7g4mcDY3AP/lHYRfQPWcCCGkfqMkitQqZXen1fXanp/C4rnvv/J1fmV/VWbO9kQlY+6f11FVgRD7pnpwdzCBu6MJPBybwtZEV6ECviqonhMhhNRflESRWqXs7jQ+D3W2tmdvdDJ+PXOXe3z8elqVlxJVmTlLkxYoTaCam+qjewvTspkmRxNYGIlACCGk8aI6UaRWPd+r7Xmbno4WdLUFtX7u8oToRfMOxla5b96Wf+9VOnNWLjErT+kM1NIP2uN7/3Z4r6M1JVCEEPIGoCSK1LqhXe1wfk4ffNnbCQDwtKgUi/+6WevnrepSojKpOfn47XxShfaXZ87KZ9dexKvD2TVCCCH1AyVRpE5YiXXxdb9WXOXwPy+nIOxWeq2e09FUHy+vQKqsTABjDEuO3lI6jkNTfTx8XIDyHZKez649H13AA8ITsqqc5SKEENK4UBJF6oyAz8NkHyfu8bw/r0OaX1Jr57MS66KlhQH3mAdUWiZgw+kEHIuVKB3nXlYeBgdF4J1157EnKhkFxTJ4tzTDT8Nd8fNwV9ib6KJUDszcf422ZyGEkDcIbUBci2gD4oqKS+XotfI0UqWFAICP3GywanDHWjnX06JSdP4uFMUyOQBAqMXDpQVvwUCoeD/F/yKSsODwDZXH1dUWoLBEhsr+4Qh4PJyf05vuqiOEkAaKNiAm9ZKOFh+TXpiN+uPSQ5y+nVEr5zp7OxPFMjkcmurB0VQfRaUMoTcVZ5sOX0nBwiNlCVT5pUZ9HQFerkRgbijEJJ/msDIWoaCKBAqoet0VIYSQxoOSKFLnhna1hamBkHs87+B15BbW/GW9I1dSAACeTk0x0NUaAHDocir3/Km4dMzYdxWMAaM97bkZqvxiWYW77zKeFOHUrQwsGND6leel7VkIIeTNQEkUqXMibQHG93xeOTxNWohlfytf1K2uXZH38c/NsoXre6IfcNupnE/IQtbTIkQlZmPyjhiUyhn8Xa0xoIM1rqdIocXnKZ1laqKnjfiMp1j5z22ld+aV/0Oi7VkIIeTNQUkU0YiPu9lDrKvNPd4T/QD/xmfWyNhp0gJ8eyiWe8wYsObEHbS2NIRMzvDD8TiMC45GUakcfV3MsXJwRwRfKNtbr/TlmggoS4w2fuIGCyMhErPyYWYo5BIpAY+H5YPaI3xuH+ye0A3n5/Su830BCSGEaAYlUUQjDIRaGNvdQaFtzoHreFpU+tpjKyuGKWMMbazKFgfuv/QQT4pK4e5ogg0jOyM9txDHriu/M698ZsndsSl2T+gGc0Mh0nOL4NBUHxs/7swlTVZiXXg6NaUZKEIIeYNQEkU0ZoyXA/R1nlcuT8kpQOCx17+s52iqX2FhOA/AwWdrpABAW8DDEv92EGkL8O7P5yuM0cxYhB3j3BVmlpqbGWDPxLJE6l5WHn4MjYeOgP4JEULIm4o+AYjGGOvp4GNPe4W2nZHJuJCQ9VrjWol14eFootDGAIXZqRIZg9+ac3h//XnkvFCrqjz5mv12a/RwNqsws/RiInU7/QlGbonEo6dFrxUvIYSQhomSKKJR43s0h1Cr7Mew+bMSA7MPXkPeKy7rpUkLcOGu8grhadICRN7LfuW5GYBrD6XcY3dHEzAGtGtmhHfbW1V6XHMzA+x+lkjFSSiRIoSQNxUV26xFVGxTNYuO3EDwhSR0sBHj0dNipOQUYLSnPRYPbKe0/9bwRHz3100wlO1rN7u/C9rbiOFoqg8rsS5O3JBg4v8uvVZMPVqYwtFUH46m+mhupo/mpgZo1kQXghduzbub+RTDNv2HzCdFcDLTx6z+LuhgI6Z1UYQQ0sCp+vlNSVQtqm9JVJq0AIlZeVyyUV/OlZpTAJ+Vp1EiY5jeryV+DL0DANg7sRvsmupx4zzOK8HGcwk4fCVN6Th8HuDhaIKY5McoKq34Y83jocKC8+rQEfBh/6xwZ3MzAzQ31QcDw5yD17lx+TwgcFB7ukOPEEIaMEqi6oH6lETtjU7G3IPXIWe1/0Ff3XOlSQsw98B1nLmTiT4u5rAwEmJ31AOY6OsgJ78YSqoOVNtIDzvESXJx6X5Ohee8W5rh3/hMMAZs+9QdBkIB7mXmITErj/sz8VEeikvlKp2LzwPC5/ShGSlCCGmgVP381qr0GdJopEkLuKQGAOQMmHcwFt4tKy6crutzvZhwAcCpuAzsmdgNYbcykPGk5tYZ7Yws2xRYT0eA2W+7QKTFx+wD1yHg85CRWwjGgAHtreDT0gwA4GavuDBdJmdIzSnAvaw8JGY+LUuwsvJwKzUXWXnFCn3lDIh9KKUkihBCGjlKot4AiVl5FWZzyvd3q+kP+uqc6+WEq9zGc3fR2d4YIbHpNRqbk5k+Nn7ihhbmhpj9xzUAQBM9HcRJnkDA52HGWy0rPVbA58HWRA+2JnpcolX+GrovP1XhNUzacQmX5vdDE32dGn0NhBBC6g+6O+8N4GiqX2GrEj4PVe7vVtXdb9U9l6CSc11MylZ6qe50XCb+uVGzCRQAHJjshRbmhnj0tAh/PqsZlfXsrrphXW3R3Myg2mNaiXUROKg9BC8VppIzoNP3oXj80iwVIYSQxoOSqDfA8w/6521yhkpnofZGJ6P78lMYsTkS3Zefwt7o5Gqf68VE6r2O1hXOtTc6GVP3XKl0nNpYqfdvfFn9qZ2RyQrrm3S1BZja11ntcYd2tcP5Ob2xe0I3/DOtJ4z1nm9nQ4kUIYQ0XpREvSHKPuj74L2Oz+sfXU5+XKGfsjVNcw5cx530J9U6V/icPtzjQ1dSqzxHOR4AI1HtXWFee/IOikpl+N9/9xXaP+3hAHMj0WuNXb7tSytLI/w3ty+am+lzz3X6PhQ5+ZRIEUJIY0NJ1BvESqyLH4e4co8/+OVChT7K1jQxAAM3nMemc3dRWCJT+VwB77XhHt9MzeW+v5dR8RwAYG4kRP92lq8cW0fAxxe9nXDzOz8kLR+Az3yaqxTT3cw89F19FpkvLFg31tPGJB8nlY5XlUhbgNCvfRSqprt+RzNShBDS2FAS9YbRFvAx0uN5uYHoJMXK3srWNAFAQbEcy47FodfKM9gTlYxS2atv9x/t6cB9/+GvZQlbYYkMW8LvKe2fnluEfRcfvnLcf772xjd+LtDTKZu1mvN2a4xwt33lcQDw8LHiGq8B7a1eWR1dHQI+D3smdsP7Ha25tk7fhyI2RarWWjNCCCH1DyVRb6A5b7tw3w8OisCLpcK4NU2VHCvJLcScg9fx1tpzOHY9DVWVGePzeejpbAoAKCiR4V7mUwzf/B9Ox2VCwCu7fFddfB6QW1BSoX1KX2elyd+r7IxMrva6L1XxeDysG94JE3o6cm3v/nxerbVmhBBC6h9Kot5AhiJthRmS8gXX5YZ2tcO6EZ2UHvtJN3uY6OvgXmYePt8Zg4EbwhFexYbBa4a6ct/3WX0Wl5NzAAC6Olp4Of1a+VEHnJnZq8rkSs6AaXsv4+ydDIXZnJfvkhPweFj8flt0b9G0itGejznvYGytzQ59O6ANPu+leMmwts9JCCGk9lES9Yaa905r7vtRv0dVmFFys29SIZnh84DPezvh7De9MLWvM/R1BLj2UIqRWyLx8ZZIXHuYU+E8pgZCped/quQSWlGpHEM2RlRIrl6WmJWP0b9HV5jNefEuufNzemO0lwN+H9MVHs1NqhitTHktq9rS49mMXF2ekxBCSO2iJOoNZSkWoVer50Uj/7khUXjeSqyLER6KW7UYibShL9SCoUgbX/dribOzemNsdwdoC3g4n5CF99eH44udMbib+ZQ7Zlek6pes5h+KrVaVcjkD5h68XmFGytOpKVdSQaglwOrBHV85loDHq7Ju1utSXj+rds9JCCGkdtWLJGrDhg1wcHCASCSCh4cHoqKiquy/f/9+uLi4QCQSoX379jh27JjC84wxLFy4EFZWVtDV1YWvry/i4+MV+ixduhReXl7Q09ODsbFxpecKDg5Ghw4dIBKJYG5uji+++ELt11nfzO7/fG3UtL1XIHvplrkuDk0UHucUlCDg8A3usamBEAHvtcWpGb0wqHMz8HjA39fT0O/Hs/jw1wtwmPM35v15vcJ5K/uh0+ID/dtaVOs1yBkwc99VXHmQU2kfmyZ6CuuSXibg8bBsULta3aZF2eXG2j4nIYSQ2qXxJGrv3r2YPn06AgICEBMTg44dO8LPzw8ZGRlK+1+4cAHDhw/HuHHjcPnyZfj7+8Pf3x+xsbFcnxUrVmDdunUICgpCZGQk9PX14efnh8LCQq5PcXExBg8ejMmTJ1ca248//ohvv/0Wc+bMwY0bN3Dy5En4+fnV3IvXsNZWRnC1NQYAFJbIcehyisLzyjbc/fNyCrZFJCncYWZroocfh7giZKo3TA10IGfApfuKNah0Xqj0Wdl9fTI5EPJSpXKeCovFw+8+gv+GcHgGhuHwlRSlcX/aw7Fi1XYAG0Z0wvk5vWttM+YXvXy5sS7OSQghpPbwWFW3V9UBDw8PdO3aFevXrwcAyOVy2NraYsqUKZgzZ06F/kOHDkVeXh6OHj3KtXXr1g2urq4ICgoCYwzW1taYMWMGZs6cCQCQSqWwsLBAcHAwhg0bpjBecHAwpk2bhpycHIX2x48fo1mzZvjrr7/Qt29ftV6bqrtAa9L5+Cx8/FskAMBQqIVLC/pBR6sst94ekYSFL8w8vYzPAwIHtceQLraITMzG+lPxOJ/wqEbi4gH4eXgnWBkL8eGv/1X7+DFeDviyTwuFNVl7o5Mx72AsZIxxM0GUyBBCCHmZqp/fGt2AuLi4GJcuXcLcuXO5Nj6fD19fX0RERCg9JiIiAtOnT1do8/Pzw6FDhwAAiYmJkEgk8PX15Z4Xi8Xw8PBAREREhSSqMqGhoZDL5UhJSUHr1q3x5MkTeHl5YfXq1bC1VV6TqKioCEVFz9f05ObmKu1Xn3Rv0RSOpvpIzMrDk6JSBB67hYk+zWEl1kVRSdmMTnMzfdzLzIMWH3hxkkfOgDkHr2NreBLiJKpXNFcFA9DUQIiiUuU5/oIBrfFOBytoC/g4fj0NW8OTcC8rj3s++EISgi8kgccDdk/oBvumZZsHH/zcE/nFcjiY6tGlNEIIIa9Fo5fzsrKyIJPJYGGhuA7GwsICEolE6TESiaTK/uV/VmdMZe7duwe5XI5ly5Zh7dq1+OOPP5CdnY1+/fqhuFh55enAwECIxWLuq7Jkqz7h8Xj4qm8L7vHWC0nwWn4KG8/dRfGzgppd7U1g20QXSq6SgTEg7v/t3XlcVOX+B/DPzMAMg8MwKchgIJCK4E4qi3hFcuGat9BuNzNL9Hqz1G6U11SUQCu3tNTM233dFrFya3HrpriQmAKSKSiyg2wpS4iyqWzz/f3hjxMjg4wjMAN+36/XvGzOec5znvM8BN/XOd/zPEWVkJmJMXVYL4PmatKlcdHilhKynxxiD3trOWwUMrzk44yfFo1F7NIn8FZA/2bte/6/Z+Cz5s5agFP/HYv8smoOoBhjjD0wo+dEmSqNRoO6ujp89NFHCAgIgLe3N3bt2oXMzEycOHFC5zEhISEoLy8XPgUFBR3c6pYVlt9qcabs4U7aCeREwJpDaYiIyQEAJBRcR8H1lucz+sdoF8QufQIbn/fAe1MG6ZXHdC8iEbD6mcGwt5bfV0J2L5UcC/z7InftZET9y0/r7cNGPD8TY4yxtmLUx3k2NjaQSCQoLtZOJi4uLoZarXsNNbVafc/yjf8WFxfD3t5eq8ywYcP0blvjsQMG/LH+m62tLWxsbJCfr/u1fZlMBplM97xIxrTnbL6w4G9jHlPTXKC7l0Jp9HvVnTtuGcVVOvcDgJ1SBodH5DiSXIyjKUWITv/doDaKRHeCNxGAVVMGYYzrHwHQtJG9McbVFrmlN/V+DNfHVoGI2Z6IzSrFC5/Fa+1rnJ+J70Yxxhh7EEa9EyWVSjF8+HBERUUJ2zQaDaKiouDj46PzGB8fH63ywJ38pcbyLi4uUKvVWmUqKioQHx/fYp26+Pr6AgDS09OFbWVlZSgtLYWTk5Pe9Rjbles3sfT7JGHBX113Ylxsuhl896i4ogYrfkjBsn1J9wygxrra4i9D7PGafx94Ojef/LLx9QYCsGzfpWYTad49/5O+XGx5fibGGGPtw+iP8xYuXIhPP/0U27dvR2pqKubNm4fq6mrMnj0bADBz5kytxPPg4GBERkbigw8+QFpaGlasWIFff/0Vr732GoA7OT5vvPEG3nvvPRw8eBBJSUmYOXMmevXqhSlTpgj15OfnIzExEfn5+WhoaEBiYiISExNRVXXnrourqysCAwMRHByM2NhYXLp0CUFBQXBzc4O/v3/HddADSC+qxOyIs81mAL97pmx7a7nWenrtITrjd/zvYiE+PpGNX+5a9FiXtnrsxvMzMcYYay9GfZwH3Jmy4Pfff0dYWBiKioowbNgwREZGConh+fn5EIv/iPVGjRqFnTt3IjQ0FMuWLUO/fv2wf/9+DBo0SCizePFiVFdXY+7cubhx4wZGjx6NyMhIWFhYCGXCwsKwfft24buHx5214k6cOIGxY8cCAL788ku8+eabmDx5MsRiMfz8/BAZGQlzc/P27JIHdrO2HpujMvH5qRzUa5q/3abrTswrY/pAoyGsi0xvVv5+7fiHFx6z7YbSylqUVtegtLIGpVW1uFZVg9KqO/99+h7r7TVqq8duhjwOZIwxxlpj9HmiujJjzBMVlVqMsAPJuHKj5Ts4IZPc8IpfH537vjqTi7f3654bapaPE05lliK7yVQCLRGLgH3zR6G6tgEuNt2aBS4llbcxeu0J1DZoYGkuxs265q/+SUQinF7qz0EPY4yxDtUp5olibaew/BZWHkxB5P+vgfeoSo7nPR3xwdGMZmWHOKharMemm+7EeG+X7th+Jg/6htwaAgK3xgJonsxeWH4LOaXVcLWzwqWr5UIAFTDADsdTS7Qmw+QAijHGmKniIKqTq2/QICI2FxuPZaC6tgFmYhHm/MkFweP6ofxWHTYey8DdT/SkZrqzyE9nliJ4d6LOfWdydOcxiUXAWNeeSC4sR3GF7sWDNQSEfJ+EbjIzXLlxC+sOpzVrEwAcTy3hyTAZY4x1GhxEdUKNd3Ju1jbgw6MZSCm8MzP6cKdHsGrqILip79x6tJSaYc0zg4WlThq99d1F7J03CipLqVDf0ZRirDmUeufxmlSMm7UtrXD3h+VPumOMqw0szCW4Vl2Lg4lXERGbq7OsBsBrOxPuWV8DEW7WauDTp4cevcAYY4wZF+dEtaP2yIlqOudTI2u5OUImueG5EY4Q65gyvLD8FnJLb6KbTIJXvzqHq+W34enSHV/N8cT+hCtYujdJ78d07YlzoBhjjJkCzonqggrLbzULoACgn103VNXU43z+dQzsZQ25VKK1v3HmbwDYNtsTz34Si19yyjD5o1PIKmk9SbwjcA4UY4yxzoaDqE4kp7RaZy7Rr7k38GvuDQCARCyCm9oKQx1VGPb/nz62CpRU3kZOaTVsFTL0tVMgIf9GmwdQ9tYWcLWzQn+1FfrbWUEpN0PetWqsOpSm806XRCTC4kn9MeRRFedAMcYY63Q4iOpEGhfjbRpIiUXA3DGPIaukGokFN1BaVYPkqxVIvlqBnfF3ZvyWmYlRo2v1YAP9e8bj6N3dEo6PWMLasvU5s6wszIW8LIlIhMV/7o8hDhw4McYY69w4iOpEGmffbhqQrH5mkDB1ABGhsPw2LhTcQKLwua53ACUC4NffFgED1eilksNSKkFqYQXCDmjPG1V5uw6DHrXWu9082SVjjLGuiBPL21F7TbbZmCjeWkCy52w+ln6f1GzZl5Z07yaFWmmBksrbuFZd22KyOSeAM8YY68o4sbwLa5oo3pLGJPT7iZDLqmtRVl0rfJeIRVBamOH6zTqtcm21HAtjjDHWmXEQ1UW1lISuj91zvdG3pwLdLaUorrwN37U/adWla+09xhhj7GEjbr0I64wak9AN8eGxDCgtzCEWi4Q8rKZ18VQEjDHGGAdRnVph+S3EZpeisLz5YsNC8NNKHdYWZni8t0pr2y85ZXjmkxg0pstNG9kbBxb4ArjzNmBjIjtjjDH2MOPHeZ1U05nL717gtykSAY2JUXZKGX54bTSKKm5jf8JV/HDxKn6vrMH5/BvNjrt0pQJ9lx9G2rt/hrlEDDtrizv1teM1McYYY50Jv53Xjtrz7by785SakptLYG9tgculzSfTnDPaGX/qZ4uhDioo5eaIzS7F/oSrOJJchKqaep31bZo2DDIzMebtOA8AiAt5gh/nMcYY67L0/fvNQVQ7aq8gKja7FC98Gt9m9d2ve935Yowxxjo7ff9+c05UJ9RS0riFWccMp4aAZXsv6czFYowxxh4WnBPVCbU2c3mjPWfzhTJiERAyyR1/crVBcUUNUgsrEJ1egjOXywxqA88VxRhj7GHHj/PaUXs9zmukz8zl+s5u3qi+QYNzedfx/KdnWpyxHOBZyxljjHVd/DjvIWBvLYdPnx73DGT0KdOUmUQMr8d6YO1dc0M1JQbPFcUYY4zx4zymU+OiwdticvDpzzkg3Fmg+OU/PYbZo505gGKMMfbQ48d57ai9H+d1lPt9JMgYY4x1ZrwAMWsz+ix4zBhjjD1sOCeKMcYYY8wAHEQxxhhjjBmAgyjGGGOMMQNwEMUYY4wxZgAOohhjjDHGDMBBFGOMMcaYATiIYowxxhgzAAdRjDHGGGMG4CCKMcYYY8wAHEQxxhhjjBmAgyjGGGOMMQPw2nntqHFt54qKCiO3hDHGGGP6avy73fh3vCUcRLWjyspKAICjo6ORW8IYY4yx+1VZWQlra+sW94uotTCLGUyj0eDq1auwsrKCSCQydnNMTkVFBRwdHVFQUAClUmns5jyUeAyMi/vf+HgMjM8Ux4CIUFlZiV69ekEsbjnzie9EtSOxWAwHBwdjN8PkKZVKk/kf52HFY2Bc3P/Gx2NgfKY2Bve6A9WIE8sZY4wxxgzAQRRjjDHGmAE4iGJGI5PJEB4eDplMZuymPLR4DIyL+9/4eAyMrzOPASeWM8YYY4wZgO9EMcYYY4wZgIMoxhhjjDEDcBDFGGOMMWYADqIYY4wxxgzAQRTT29atW+Hs7AwLCwt4eXnhl19+uWf5b7/9Fm5ubrCwsMDgwYNx6NAhrf1EhLCwMNjb20Mul2P8+PHIzMzUKlNWVoYZM2ZAqVRCpVJhzpw5qKqq0ipz5MgReHt7w8rKCra2tvjrX/+K3NzcNrlmU2KM/l+1ahVGjRoFS0tLqFQqnefJz8/H5MmTYWlpiZ49e+Ktt95CfX39A12rqTLFMbhw4QKmT58OR0dHyOVyuLu7Y/PmzQ98rabKFMegqWvXrsHBwQEikQg3btww5BJNnimPQUREBIYMGQILCwv07NkTCxYsMPg69UKM6WH37t0klUrpiy++oOTkZHr55ZdJpVJRcXGxzvIxMTEkkUjo/fffp5SUFAoNDSVzc3NKSkoSyqxdu5asra1p//79dOHCBXr66afJxcWFbt26JZT585//TEOHDqUzZ87QqVOnqG/fvjR9+nRh/+XLl0kmk1FISAhlZWXRuXPnaMyYMeTh4dF+nWEExur/sLAw+vDDD2nhwoVkbW3d7Dz19fU0aNAgGj9+PCUkJNChQ4fIxsaGQkJC2rwPjM1Ux+Dzzz+n119/naKjoyk7O5u++uorksvltGXLljbvA2Mz1TFoKjAwkCZNmkQA6Pr1621x2SbFlMfggw8+oF69etGOHTsoKyuLLly4QAcOHGjT678bB1FML56enrRgwQLhe0NDA/Xq1YvWrFmjs/xzzz1HkydP1trm5eVFr7zyChERaTQaUqvVtH79emH/jRs3SCaT0a5du4iIKCUlhQDQ2bNnhTKHDx8mkUhEV65cISKib7/9lszMzKihoUEoc/DgQRKJRFRbW/uAV206jNH/TW3btk3nL65Dhw6RWCymoqIiYdsnn3xCSqWSampq7usaTZ2pjoEu8+fPJ39/f73KdiamPgb//ve/yc/Pj6KiorpsEGWqY1BWVkZyuZyOHz9uyGUZjB/nsVbV1tbi3LlzGD9+vLBNLBZj/PjxiIuL03lMXFycVnkACAgIEMrn5OSgqKhIq4y1tTW8vLyEMnFxcVCpVBgxYoRQZvz48RCLxYiPjwcADB8+HGKxGNu2bUNDQwPKy8vx1VdfYfz48TA3N2+bDjAyY/W/PuLi4jB48GDY2dlpnaeiogLJycl612PqTHkMdCkvL0f37t0fqA5TY+pjkJKSgnfeeQdffvnlPRes7cxMeQyOHTsGjUaDK1euwN3dHQ4ODnjuuedQUFBwP5d437rmSLM2VVpaioaGBq0/lABgZ2eHoqIinccUFRXds3zjv62V6dmzp9Z+MzMzdO/eXSjj4uKCo0ePYtmyZZDJZFCpVPjtt9/wzTffGHi1psdY/a+Pls7T9BxdgSmPwd1iY2OxZ88ezJ071+A6TJEpj0FNTQ2mT5+O9evXo3fv3nof19mY8hhcvnwZGo0Gq1evxqZNm/Ddd9+hrKwMEyZMQG1trd713C8OolinVlRUhJdffhlBQUE4e/YsTp48CalUimeffRbEk/Gzh8ylS5cQGBiI8PBwTJw40djNeWiEhITA3d0dL774orGb8tDSaDSoq6vDRx99hICAAHh7e2PXrl3IzMzEiRMn2u28HESxVtnY2EAikaC4uFhre3FxMdRqtc5j1Gr1Pcs3/ttamZKSEq399fX1KCsrE8ps3boV1tbWeP/99+Hh4YExY8bg66+/RlRUlPDIr7MzVv/ro6XzND1HV2DKY9AoJSUF48aNw9y5cxEaGnrfx5s6Ux6Dn376Cd9++y3MzMxgZmaGcePGCW0ODw/Xux5TZ8pjYG9vDwAYMGCAsM3W1hY2NjbIz8/Xu577xUEUa5VUKsXw4cMRFRUlbNNoNIiKioKPj4/OY3x8fLTKA3eeWTeWd3FxgVqt1ipTUVGB+Ph4oYyPjw9u3LiBc+fOCWV++uknaDQaeHl5AQBu3rzZLP9AIpEIbewKjNX/+vDx8UFSUpJWsHvs2DEolUqtX2adnSmPAQAkJyfD398fQUFBWLVq1X0d21mY8hh8//33uHDhAhITE5GYmIjPPvsMAHDq1Kn2f8W+A5nyGPj6+gIA0tPThW1lZWUoLS2Fk5OT3vXctw5NY2ed1u7du0kmk1FERASlpKTQ3LlzSaVSCW9lvfTSS7R06VKhfExMDJmZmdGGDRsoNTWVwsPDdb7WqlKp6MCBA3Tx4kUKDAzUOcWBh4cHxcfH0+nTp6lfv35aUxxERUWRSCSilStXUkZGBp07d44CAgLIycmJbt682QE90zGM1f95eXmUkJBAK1euJIVCQQkJCZSQkECVlZVE9McUBxMnTqTExESKjIwkW1vbLjvFgSmOQVJSEtna2tKLL75IhYWFwqekpKSDeqbjmOoY3O3EiRNd9u08Ux6DwMBAGjhwIMXExFBSUhL95S9/oQEDBrTrm9ocRDG9bdmyhXr37k1SqZQ8PT3pzJkzwj4/Pz8KCgrSKv/NN9+Qq6srSaVSGjhwIP34449a+zUaDb399ttkZ2dHMpmMxo0bR+np6Vplrl27RtOnTyeFQkFKpZJmz57d7BfXrl27yMPDg7p160a2trb09NNPU2pqattevAkwRv8HBQURgGafEydOCGVyc3Np0qRJJJfLycbGhv71r39RXV1dm1+/KTDFMQgPD9e538nJqT26wOhMcQzu1pWDKCLTHYPy8nL6+9//TiqVirp3705Tp06l/Pz8Nr/+pkREnH3LGGOMMXa/OCeKMcYYY8wAHEQxxhhjjBmAgyjGGGOMMQNwEMUYY4wxZgAOohhjjDHGDMBBFGOMMcaYATiIYowxxhgzAAdRjLF7Gjt2LN544402rzciIgIqleqeZVasWIFhw4YJ32fNmoUpU6a0eVtakpubC5FIhMTExA47Z1ejzzgz1llxEMUY6zQ2b96MiIgIYzfDJDk7O2PTpk1a2zo6gNHVhmnTpiEjI6PD2sBYRzIzdgMYY0xf1tbWxm5Cl1RbWwupVNoudcvlcsjl8napmzFj4ztRjHUhGo0Ga9asgYuLC+RyOYYOHYrvvvsOABAdHQ2RSIQjR47Aw8MDcrkcTzzxBEpKSnD48GG4u7tDqVTihRdewM2bN7Xqra+vx2uvvQZra2vY2Njg7bffRtMVo2pqarBo0SI8+uij6NatG7y8vBAdHa1VR0REBHr37g1LS0tMnToV165da9b+tWvXws7ODlZWVpgzZw5u376ttf/ux3ljx47F66+/jsWLF6N79+5Qq9VYsWKF1jFpaWkYPXo0LCwsMGDAABw/fhwikQj79++//w4GcPLkSXh6ekImk8He3h5Lly5FfX29sP+7777D4MGDIZfL0aNHD4wfPx7V1dUA7oyBp6cnunXrBpVKBV9fX+Tl5bV6zuzsbAQGBsLOzg4KhQIjR47E8ePHtfohLy8Pb775JkQiEUQiEaKjozF79myUl5cL2xr7xtnZGe+++y5mzpwJpVKJuXPnAgCWLFkCV1dXWFpa4rHHHsPbb7+Nuro6rbb88MMPGDlyJCwsLGBjY4OpU6e22AZA+25YRkYGRCIR0tLStOrcuHEj+vTpI3y/dOkSJk2aBIVCATs7O7z00ksoLS3VZ3gY61jtujIfY6xDvffee+Tm5kaRkZGUnZ1N27ZtI5lMRtHR0cKiqN7e3nT69Gk6f/489e3bl/z8/GjixIl0/vx5+vnnn6lHjx60du1aoU4/Pz9SKBQUHBxMaWlp9PXXX5OlpSX997//Fcr84x//oFGjRtHPP/9MWVlZtH79epLJZJSRkUFERGfOnCGxWEzr1q2j9PR02rx5M6lUKrK2thbq2LNnD8lkMvrss88oLS2Nli9fTlZWVjR06FChTFBQEAUGBmq1TalU0ooVKygjI4O2b99OIpGIjh49SkRE9fX11L9/f5owYQIlJibSqVOnyNPTkwDQvn37Wu3PnJwcAkAJCQlERPTbb7+RpaUlzZ8/n1JTU2nfvn1kY2ND4eHhRER09epVMjMzow8//JBycnLo4sWLtHXrVqqsrKS6ujqytramRYsWUVZWFqWkpFBERATl5eW12o7ExET6z3/+Q0lJSZSRkUGhoaFkYWEhHHvt2jVycHCgd955hwoLC6mwsJBqampo06ZNpFQqhW2Ni3c7OTmRUqmkDRs2UFZWFmVlZRER0bvvvksxMTGUk5NDBw8eJDs7O1q3bp3Qjv/9738kkUgoLCyMUlJSKDExkVavXt1iG4iItm3bpjXOI0aMoNDQUK3rGz58uLDt+vXrZGtrSyEhIZSamkrnz5+nCRMmkL+/f6v9xFhH4yCKsS7i9u3bZGlpSbGxsVrb58yZQ9OnTxeCqOPHjwv71qxZQwAoOztb2PbKK69QQECA8N3Pz4/c3d1Jo9EI25YsWULu7u5ERJSXl0cSiYSuXLmidd5x48ZRSEgIERFNnz6dnnzySa3906ZN0/rj6uPjQ/Pnz9cq4+Xl1WoQNXr0aK1jRo4cSUuWLCEiosOHD5OZmZnwB52I6NixYwYHUcuWLaP+/ftr9cXWrVtJoVBQQ0MDnTt3jgBQbm5us7quXbtGACg6OrrV8+pj4MCBtGXLFuG7k5MTbdy4UavM3QFM07JTpkxp9Rzr16+n4cOHC999fHxoxowZLZbXpw0bN26kPn36CN/T09MJAKWmphLRnUBu4sSJWnUUFBQQAEpPT2+1zYx1JH6cx1gXkZWVhZs3b2LChAlQKBTC58svv0R2drZQbsiQIcJ/29nZCY9umm4rKSnRqtvb21t4PAMAPj4+yMzMRENDA5KSktDQ0ABXV1et8548eVI4b2pqKry8vLTq9PHx0fquTxldml4PANjb2wvtT09Ph6OjI9RqtbDf09Oz1TpbkpqaCh8fH62+8PX1RVVVFX777TcMHToU48aNw+DBg/G3v/0Nn376Ka5fvw4A6N69O2bNmoWAgAA89dRT2Lx5MwoLC/U6b1VVFRYtWgR3d3eoVCooFAqkpqYiPz/f4GsZMWJEs2179uyBr68v1Go1FAoFQkNDtc6RmJiIcePGGXxOAHj++eeRm5uLM2fOAAB27NiBxx9/HG5ubgCACxcu4MSJE1o/S437mv4cM2YKOLGcsS6iqqoKAPDjjz/i0Ucf1donk8mEP0Dm5ubCdpFIpPW9cZtGo7mv80okEpw7dw4SiURrn0KhuK9rMMSDtr8tSSQSHDt2DLGxsTh69Ci2bNmC5cuXIz4+Hi4uLti2bRtef/11REZGYs+ePQgNDcWxY8fg7e19z3oXLVqEY8eOYcOGDejbty/kcjmeffZZ1NbWGtzWbt26aX2Pi4vDjBkzsHLlSgQEBMDa2hq7d+/GBx98IJRpiwRxtVqNJ554Ajt37oS3tzd27tyJefPmCfurqqrw1FNPYd26dc2Otbe3f+DzM9aW+E4UY13EgAEDIJPJkJ+fj759+2p9HB0dH6ju+Ph4re9nzpxBv379IJFI4OHhgYaGBpSUlDQ7b+MdIHd3d511NKVPmfvVv39/FBQUoLi4WNh29uxZg+tzd3dHXFycVlJ9TEwMrKys4ODgAOBOEOfr64uVK1ciISEBUqkU+/btE8p7eHggJCQEsbGxGDRoEHbu3NnqeWNiYjBr1ixMnToVgwcPhlqtRm5urlYZqVSKhoaGVre1JDY2Fk5OTli+fDlGjBiBfv36NUt6HzJkCKKiolqsQ9/zzZgxA3v27EFcXBwuX76M559/Xtj3+OOPIzk5Gc7Ozs1+nu4O/BgzNg6iGOsirKyssGjRIrz55pvYvn07srOzcf78eWzZsgXbt29/oLrz8/OxcOFCpKenY9euXdiyZQuCg4MBAK6urpgxYwZmzpyJvXv3IicnB7/88gvWrFmDH3/8EQCEuy8bNmxAZmYmPv74Y0RGRmqdIzg4GF988QW2bduGjIwMhIeHIzk5+YHaPWHCBPTp0wdBQUG4ePEiYmJiEBoaCgBaj+T0NX/+fBQUFOCf//wn0tLScODAAYSHh2PhwoUQi8WIj4/H6tWr8euvvyI/Px979+7F77//Dnd3d+Tk5CAkJARxcXHIy8vD0aNHkZmZCXd391bP269fP+zduxeJiYm4cOECXnjhhWZ325ydnfHzzz/jypUrwptszs7OqKqqQlRUFEpLS5u9dXn3OfLz87F7925kZ2fjo48+0gr+ACA8PBy7du1CeHg4UlNTkZSUpHXHSFcbdHnmmWdQWVmJefPmwd/fH7169RL2LViwAGVlZZg+fTrOnj2L7OxsHDlyBLNnz9Y7IGSswxg7KYsx1nY0Gg1t2rSJ+vfvT+bm5mRra0sBAQF08uRJIbH8+vXrQnldicfh4eFaydx+fn40f/58evXVV0mpVNIjjzxCy5Yt00qurq2tpbCwMHJ2diZzc3Oyt7enqVOn0sWLF4Uyn3/+OTk4OJBcLqennnqKNmzY0Ozcq1atIhsbG1IoFBQUFESLFy9uNbE8ODhYq47AwEAKCgoSvqemppKvry9JpVJyc3OjH374gQBQZGRkq/15d2I5EVF0dDSNHDmSpFIpqdVqWrJkCdXV1RERUUpKCgUEBJCtrS3JZDJydXUVkr+LiopoypQpZG9vT1KplJycnCgsLIwaGhr0aoe/vz/J5XJydHSkjz/+uNm1x8XF0ZAhQ0gmk1HTX+2vvvoq9ejRgwAIbxHqSgAnInrrrbeoR48epFAoaNq0abRx48ZmY/T999/TsGHDSCqVko2NDT3zzDP3bENLye3PPfccAaAvvvii2b6MjAyaOnUqqVQqksvl5ObmRm+88YbWzxxjpkBE1OS+NGOMdXExMTEYPXo0srKytOYmYoyx+8VBFGOsS9u3bx8UCgX69euHrKwsBAcH45FHHsHp06eN3TTGWCfHOVGMsS6tsrISCxYsgJubG2bNmoWRI0fiwIEDAIDVq1drvUrf9DNp0qQOa+PAgQNbbMeOHTs6rB2MsfvDd6IYYw+tsrIylJWV6dwnl8ubTRXRXvLy8potr9KocRkcxpjp4SCKMcYYY8wA/DiPMcYYY8wAHEQxxhhjjBmAgyjGGGOMMQNwEMUYY4wxZgAOohhjjDHGDMBBFGOMMcaYATiIYowxxhgzAAdRjDHGGGMG+D84jikC7/LicAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label = f'Scaling coefficient = {1.0} (attractive), {0.2} (repulsive)'\n",
    "plt.plot(loss_history['embedding_loss_attractive'],loss_history['embedding_loss_repulsive'],\n",
    "         marker='o', markersize=3, label=label)\n",
    "\n",
    "plt.xlabel('embedding_loss_attractive')\n",
    "plt.ylabel('embedding_loss_repulsive')\n",
    "plt.title('Loss Functions')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2508e02b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
