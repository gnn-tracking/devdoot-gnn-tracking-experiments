{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from gnn_tracking.models.graph_construction import GraphConstructionFCNN\n",
    "from tcn_trainer_mdmm import TCNTrainer\n",
    "\n",
    "from gnn_tracking.metrics.losses import GraphConstructionHingeEmbeddingLoss\n",
    "from gnn_tracking.utils.loading import get_loaders, TrackingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = Path(\"D:\\Devdoot\\Princeton RSE\\dataset\\graph constructed new\")\n",
    "assert graph_dir.is_dir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    \"train\": TrackingDataset(graph_dir, stop=810),\n",
    "    \"val\": TrackingDataset(graph_dir, start=810, stop=900),\n",
    "}\n",
    "loaders = get_loaders(datasets, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_dicts_to_dict_of_lists(list_of_dicts):\n",
    "    dict_of_lists = {}\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key, value in dictionary.items():\n",
    "            dict_of_lists.setdefault(key, []).append(value)\n",
    "    return dict_of_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_mdmm(epsilon,\n",
    "                     damping,\n",
    "                     main_weight,\n",
    "                     constraint_weight,\n",
    "                     loaders,\n",
    "                     main_loss=\"attractive\",\n",
    "                     constraint_loss=\"repulsive\",\n",
    "                     num_epochs=50\n",
    "                    ):\n",
    "    \n",
    "    main_loss_functions = {\n",
    "        \"embedding_loss\": (GraphConstructionHingeEmbeddingLoss(), {main_loss: main_weight}),\n",
    "    }\n",
    "    constraint_loss_functions = {\n",
    "        \"embedding_loss\": (GraphConstructionHingeEmbeddingLoss(), {constraint_loss: (constraint_weight, epsilon, damping)}),\n",
    "    }\n",
    "\n",
    "    model = GraphConstructionFCNN(\n",
    "        in_dim = 14,\n",
    "        hidden_dim = 64,\n",
    "        out_dim = 10,\n",
    "        depth = 4,\n",
    "        beta = 0.4\n",
    "    )\n",
    "\n",
    "    trainer = TCNTrainer(\n",
    "        model=model,\n",
    "        loaders=loaders,\n",
    "        main_loss_functions=main_loss_functions,\n",
    "        constraint_loss_functions=constraint_loss_functions,\n",
    "        lr=0.005,\n",
    "    )\n",
    "\n",
    "    loss_history = trainer.train(epochs=num_epochs)\n",
    "    return list_of_dicts_to_dict_of_lists(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss_history(constraints, path, main_loss=\"attractive\", constraint_loss=\"repulsive\"):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for constraint in constraints:\n",
    "        print(f'Training for scaling coefficients = {constraint}')\n",
    "        loss_history = train_model_mdmm(epsilon=constraint[0],\n",
    "                                damping=constraint[1],\n",
    "                                main_weight=constraint[2],\n",
    "                                constraint_weight=constraint[3],\n",
    "                                loaders=loaders,\n",
    "                                main_loss=main_loss,\n",
    "                                constraint_loss=constraint_loss,\n",
    "                                num_epochs=10)\n",
    "        model_dict = {'loss_history':loss_history,\n",
    "                      'epsilon':constraint[0],\n",
    "                      'damping':constraint[1],\n",
    "                      'weight':constraint[2]}\n",
    "        \n",
    "        file_path = os.path.join(path, f'{constraint[0]}_{constraint[1]}_{constraint[2]}.pkl')\n",
    "\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            pickle.dump(model_dict,f)\n",
    "            f.close()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_1 = [(0.0009653154573041118, 10.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 10.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 10.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 10.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 10.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_2 = [(0.0009653154573041118, 5.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 5.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 5.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 5.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 5.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_3 = [(0.0009653154573041118, 1.0, 1.0, 1.0),\n",
    "               (0.0004771597001106582, 1.0, 1.0, 1.0),\n",
    "               (0.00028893887271952276, 1.0, 1.0, 1.0),\n",
    "               (0.00016839923191582784, 1.0, 1.0, 1.0),\n",
    "               (0.00010062895363475553, 1.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loss_history(constraints_1, \"loss_histories/damping_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loss_history(constraints_2, \"loss_histories/damping_05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loss_history(constraints_3, \"loss_histories/damping_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints_4 = [(0.0009300140692725962, 1.0, 1.0, 1.0),\n",
    "               (0.0008121351376578304, 1.0, 1.0, 1.0),\n",
    "               (0.0006466396967880428, 1.0, 1.0, 1.0),\n",
    "               (0.0005346179523010864, 1.0, 1.0, 1.0),\n",
    "               (0.000420255135826732, 1.0, 1.0, 1.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_loss_history(constraints_4, \"loss_histories/damping_01_attractive\",\n",
    "                  main_loss=\"repulsive\", constraint_loss=\"attractive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
